{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 주택 가격 예측 : 회귀 문제\n",
        "개별적인 레이블 대신에 연속적인 값을 예측하는 회귀(regression) 문제. 예를 들어 기상 데이터가 주어졌을 때 내일 기온을 예측하거나, 소프트웨어 명세가 주어졌을 때 소프트웨어 프로젝트가 완료될 시간을 예측하는 것을 회귀 문제라 한다.<br><br>\n",
        "이 절에서는 1970년 중반 보스턴 외곽의 데이터가 주어졌을 때 주택 가격의 중간 값을 예측해보려한다"
      ],
      "metadata": {
        "id": "p22mS_Jkihzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = (boston_housing.load_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cByAlV26ile6",
        "outputId": "f6d3fe5d-e597-4c5c-9d0b-716152a63d6c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터는 404개\n",
        "# 테스트 데이터는 102개\n",
        "# 13개의 특성\n",
        "print(train_x.shape)\n",
        "print(test_x.shape)\n",
        "\n",
        "#\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_y.shape)\n",
        "\n",
        "# 예시 살펴보기\n",
        "print(train_y[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST7rGQubimQZ",
        "outputId": "a4051a21-ac70-4f39-880b-a094c074e6b1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n",
            "(102, 13)\n",
            "(404, 13)\n",
            "(404,)\n",
            "(102,)\n",
            "[15.2 42.3 50.  21.1 17.7 18.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 전처리(준비)\n",
        "- 정규화(normalization) : 0~1 사이의 값으로 변경해주는 것 <br>\n",
        "정규화의 목적은 데이터셋의 numerical value 범위의 차이를 왜곡하지 않고 공통 척도로 변경하는 것이다. 기계학습에서 모든 데이터셋이 정규화 될 필요는 없고, 피처의 범위가 다른 경우에만 필요<br>\n",
        "- 표준화(standardizaion) : 평균 0, 분산 1이 되도록 변경해주는 것<br>표준정규분포의 속성을 갖도록 피처가 재조정되는 것\n",
        "<br><br>\n",
        "상이한 스케일을 가진 값을 신경망에 주입하면, 모델이 이런 다양한 데이터에 자동으로 맞추려고 할 수도 있지만 이는 학습을 더 어렵게 만드는 등의 문제를 일으킬 수 있다. 따라서 이런 데이터를 다룰 때 대표적인 방법은 특성별로 정규화를 하는 것인데, 입력 데이터에 있는 각 특성(입력 데이터 행렬의 열)에 대해 특성의 평균을 빼고 표준편차로 나눈다.\n",
        "<br><br>\n",
        "이를 통패 특성의 중앙이 0근처에 맞춰지고 표준편차는 1이된다"
      ],
      "metadata": {
        "id": "zwIdgr2dipe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 컬럼별로 평균 구하기\n",
        "mean = train_x.mean(axis=0)\n",
        "# 컬럼별로 분산 구하기\n",
        "std = train_x.std(axis = 0)\n",
        "# train_x에서 계산해주기\n",
        "train_x = (train_x - mean) / std\n",
        "\n",
        "train_x[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NOcRzubipyh",
        "outputId": "725ccf9a-7fa2-47f8-a148-4fb16656e927"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
              "        -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
              "         1.14850044,  0.44807713,  0.8252202 ],\n",
              "       [-0.40342651,  2.99178419, -1.33391162, -0.25683275, -1.21518188,\n",
              "         1.89434613, -1.91036058,  1.24758524, -0.85646254, -0.34843254,\n",
              "        -1.71818909,  0.43190599, -1.32920239],\n",
              "       [ 0.1249402 , -0.48361547,  1.0283258 , -0.25683275,  0.62864202,\n",
              "        -1.82968811,  1.11048828, -1.18743907,  1.67588577,  1.5652875 ,\n",
              "         0.78447637,  0.22061726, -1.30850006]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test 데이터 정규화시 평균과 분산은\n",
        "test 데이터 셋도 동일하게 변경을 해줘야 한다.<br>\n",
        "이때 평균과 분산은 test로 새로 계산해줘야 하는가?<br>\n",
        "-> train 데이터에서 계산한 값으로 해주어야 한다.<br>"
      ],
      "metadata": {
        "id": "lpb_5g46irmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = (test_x - mean) / std"
      ],
      "metadata": {
        "id": "CZTDUGOKiqm5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zhh32e_itDp",
        "outputId": "1602760f-84d0-41f5-82b7-886d90235f6f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 구성\n",
        "아래 모델의 마지막 층은 하나의 유닛을 가지고 있고 활성화 함수가 없다(선형층). 이것은 전형적인 스칼라 회귀(하나의 연속적인 값을 예측하는 회귀)를 위한 구성이다.<br><br>\n",
        "손실함수로 mse 를 사용하여 컴파일 해주었는데. 이 함수는 평균 제곱 오차(mean squared error) 의 약어로 예측과 타깃 사이 거리의 제곱을 뜻한다. 회귀 문제에서 널리 사용되는 손실함수<br><br>"
      ],
      "metadata": {
        "id": "P0T4kuw-iuca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "def build_model() :\n",
        "  model = keras.Sequential([\n",
        "      Dense(64, activation='relu'),\n",
        "      Dense(64, activation='relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics='mae')\n",
        "  return model"
      ],
      "metadata": {
        "id": "baoJDoNKit05"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "history = model.fit(train_x, train_y, epochs=100, verbose=1, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qc-6YFUiwNp",
        "outputId": "5c8a8d4f-a9ce-4452-c360-d513554407b4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "26/26 [==============================] - 1s 2ms/step - loss: 544.0865 - mae: 21.5359\n",
            "Epoch 2/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 399.6449 - mae: 18.0375\n",
            "Epoch 3/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 225.9632 - mae: 12.9145\n",
            "Epoch 4/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 95.8845 - mae: 7.6595\n",
            "Epoch 5/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 46.7959 - mae: 5.1500\n",
            "Epoch 6/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 31.7159 - mae: 4.0879\n",
            "Epoch 7/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 24.4143 - mae: 3.5516\n",
            "Epoch 8/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 21.4110 - mae: 3.2922\n",
            "Epoch 9/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 19.3956 - mae: 3.0410\n",
            "Epoch 10/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 17.8790 - mae: 2.9782\n",
            "Epoch 11/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 16.2370 - mae: 2.8315\n",
            "Epoch 12/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 15.0446 - mae: 2.7165\n",
            "Epoch 13/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 14.3463 - mae: 2.6461\n",
            "Epoch 14/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 13.1497 - mae: 2.5361\n",
            "Epoch 15/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 12.8905 - mae: 2.5541\n",
            "Epoch 16/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 12.4534 - mae: 2.4793\n",
            "Epoch 17/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 11.8849 - mae: 2.4645\n",
            "Epoch 18/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 11.4530 - mae: 2.3934\n",
            "Epoch 19/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 11.1469 - mae: 2.3730\n",
            "Epoch 20/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 10.8325 - mae: 2.3561\n",
            "Epoch 21/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 10.5802 - mae: 2.2991\n",
            "Epoch 22/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 10.4912 - mae: 2.3009\n",
            "Epoch 23/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 10.3492 - mae: 2.2886\n",
            "Epoch 24/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 10.1694 - mae: 2.2637\n",
            "Epoch 25/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 9.8221 - mae: 2.2524\n",
            "Epoch 26/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 9.9087 - mae: 2.2691\n",
            "Epoch 27/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 9.7443 - mae: 2.2434\n",
            "Epoch 28/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 9.4934 - mae: 2.2055\n",
            "Epoch 29/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 9.5057 - mae: 2.1595\n",
            "Epoch 30/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 9.1684 - mae: 2.1451\n",
            "Epoch 31/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 9.4538 - mae: 2.2071\n",
            "Epoch 32/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.9933 - mae: 2.1564\n",
            "Epoch 33/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.8978 - mae: 2.1502\n",
            "Epoch 34/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 9.1031 - mae: 2.1371\n",
            "Epoch 35/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 8.7200 - mae: 2.1442\n",
            "Epoch 36/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.7312 - mae: 2.1100\n",
            "Epoch 37/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.7266 - mae: 2.1256\n",
            "Epoch 38/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.6948 - mae: 2.1007\n",
            "Epoch 39/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.5538 - mae: 2.0676\n",
            "Epoch 40/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.5211 - mae: 2.0815\n",
            "Epoch 41/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.2670 - mae: 2.0452\n",
            "Epoch 42/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.2987 - mae: 2.0762\n",
            "Epoch 43/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.1966 - mae: 2.0437\n",
            "Epoch 44/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.0544 - mae: 1.9882\n",
            "Epoch 45/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.0711 - mae: 2.0117\n",
            "Epoch 46/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.8803 - mae: 1.9988\n",
            "Epoch 47/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.0567 - mae: 2.0244\n",
            "Epoch 48/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 8.0083 - mae: 2.0026\n",
            "Epoch 49/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.6945 - mae: 1.9648\n",
            "Epoch 50/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.6865 - mae: 1.9830\n",
            "Epoch 51/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.6100 - mae: 1.9769\n",
            "Epoch 52/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.3805 - mae: 1.9665\n",
            "Epoch 53/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.4693 - mae: 1.9486\n",
            "Epoch 54/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.4734 - mae: 1.9337\n",
            "Epoch 55/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.4879 - mae: 1.9393\n",
            "Epoch 56/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.3979 - mae: 1.9120\n",
            "Epoch 57/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.5459 - mae: 1.9423\n",
            "Epoch 58/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.3203 - mae: 1.8837\n",
            "Epoch 59/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.2891 - mae: 1.9221\n",
            "Epoch 60/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.0634 - mae: 1.8821\n",
            "Epoch 61/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.0749 - mae: 1.8978\n",
            "Epoch 62/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.9669 - mae: 1.9054\n",
            "Epoch 63/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 7.0518 - mae: 1.9020\n",
            "Epoch 64/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.8766 - mae: 1.8480\n",
            "Epoch 65/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.7694 - mae: 1.8870\n",
            "Epoch 66/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.6828 - mae: 1.8426\n",
            "Epoch 67/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.9445 - mae: 1.8940\n",
            "Epoch 68/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.7599 - mae: 1.8415\n",
            "Epoch 69/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.4946 - mae: 1.8462\n",
            "Epoch 70/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 6.7104 - mae: 1.8341\n",
            "Epoch 71/100\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 6.5650 - mae: 1.8168\n",
            "Epoch 72/100\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 6.5492 - mae: 1.8190\n",
            "Epoch 73/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 6.4674 - mae: 1.7922\n",
            "Epoch 74/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.5645 - mae: 1.8203\n",
            "Epoch 75/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.2584 - mae: 1.7978\n",
            "Epoch 76/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.4432 - mae: 1.7632\n",
            "Epoch 77/100\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 6.3266 - mae: 1.8035\n",
            "Epoch 78/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 6.2783 - mae: 1.7619\n",
            "Epoch 79/100\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 6.1804 - mae: 1.7464\n",
            "Epoch 80/100\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 6.2346 - mae: 1.7677\n",
            "Epoch 81/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 6.2149 - mae: 1.7567\n",
            "Epoch 82/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.0117 - mae: 1.7406\n",
            "Epoch 83/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 6.1337 - mae: 1.7396\n",
            "Epoch 84/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.8945 - mae: 1.7006\n",
            "Epoch 85/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.9507 - mae: 1.7412\n",
            "Epoch 86/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.9962 - mae: 1.7215\n",
            "Epoch 87/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.8523 - mae: 1.6962\n",
            "Epoch 88/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.8086 - mae: 1.6892\n",
            "Epoch 89/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.8492 - mae: 1.6746\n",
            "Epoch 90/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.7780 - mae: 1.7115\n",
            "Epoch 91/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.8064 - mae: 1.7122\n",
            "Epoch 92/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.7249 - mae: 1.6606\n",
            "Epoch 93/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.4958 - mae: 1.6330\n",
            "Epoch 94/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.4627 - mae: 1.6418\n",
            "Epoch 95/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.6891 - mae: 1.6860\n",
            "Epoch 96/100\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 5.5976 - mae: 1.6557\n",
            "Epoch 97/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.4476 - mae: 1.6410\n",
            "Epoch 98/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.2888 - mae: 1.6102\n",
            "Epoch 99/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.5079 - mae: 1.6358\n",
            "Epoch 100/100\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 5.3429 - mae: 1.6364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_mae = history.history['mae']\n",
        "plt.plot(history_mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "VZS4zMsPiw-h",
        "outputId": "57ef26e1-8899-40bd-84ac-f74fee373049"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdb9b3a1550>]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhvCrI2-iyYh",
        "outputId": "6e796ac5-60ff-4ba6-a6a0-7581ea984116"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 76 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdb9b3784c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_x, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb23gUANizH4",
        "outputId": "c74106b4-1ff5-442f-d9a8-5a53262516a2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 17.8302 - mae: 2.6221\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17.83021354675293, 2.622060775756836]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Fold\n",
        "매개변수들을 조정하면서 모델을 평가하기 위해 데이터를 train 과 validation으로 나눠준다면, 지금 데이터의 경우 수가 많지 않기 때문에 검증 세트도 매우 작아진다. 이렇게 되면 val set와 train set로 어떤 데이터가 선택되었는지에따라 validation 점수가 크게 달라질 수 있는 것이다. 이렇게 되면 모델을 신뢰있게 평가할 수 없게된다\n",
        "<br><br>\n",
        "이런 상황에서 가장 좋은 방법은 K-fold cross validation을 사용하는 것인데. 데이터를 K개로 나누고, K개의 모델을 각각 만들어 K-1개의 분할에서는 훈련을 나머지 분할에서는 평가(validation)을 하는 방법이다."
      ],
      "metadata": {
        "id": "rGO50UXHi1VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 4\n",
        "num_val_samples = len(train_x) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "\n",
        "for i in range(k) :\n",
        "  print(f'{i}번째 폴드 처리중')\n",
        "  # 분할 하나를 validation 용으로 빼놓기\n",
        "  val_data = train_x[i*num_val_samples : (i+1)*num_val_samples]\n",
        "  val_targets = train_y[i*num_val_samples : (i+1)*num_val_samples]\n",
        "\n",
        "  # 다른 분할 전체를 train 용으로 데이터 준비\n",
        "  par_train_data = np.concatenate([\n",
        "      train_x[:i*num_val_samples],\n",
        "      train_x[(i+1)*num_val_samples:]\n",
        "  ], axis = 0)\n",
        "  par_train_targets = np.concatenate([\n",
        "      train_y[:i*num_val_samples],\n",
        "      train_y[(i+1)*num_val_samples:]\n",
        "  ], axis = 0)\n",
        "\n",
        "  # 모델 구성\n",
        "  model = build_model()\n",
        "\n",
        "  # fit 해주기\n",
        "  history = model.fit(\n",
        "      par_train_data, par_train_targets,\n",
        "      epochs = num_epochs, batch_size=16, verbose=0  )\n",
        "\n",
        "  # 각 폴드의 validation 값 저장하기\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "\n",
        "  # 저장해주기\n",
        "  all_scores.append(val_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaBnhs9jizxR",
        "outputId": "3b5f197a-a3d7-41a0-8ec2-bd066e0fbab1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0번째 폴드 처리중\n",
            "1번째 폴드 처리중\n",
            "2번째 폴드 처리중\n",
            "3번째 폴드 처리중\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IniRBMB5i2ux",
        "outputId": "0685a70a-acc4-4d7e-b176-dfe046b528b6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.023070812225342, 9.185294151306152, 12.76341724395752, 11.389884948730469]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 4\n",
        "num_val_samples = len(train_x) // k\n",
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "\n",
        "for i in range(k) :\n",
        "  print(f'{i}번째 폴드 처리중')\n",
        "  # 분할 하나를 validation 용으로 빼놓기\n",
        "  val_data = train_x[i*num_val_samples : (i+1)*num_val_samples]\n",
        "  val_targets = train_y[i*num_val_samples : (i+1)*num_val_samples]\n",
        "\n",
        "  # 다른 분할 전체를 train 용으로 데이터 준비\n",
        "  par_train_data = np.concatenate([\n",
        "      train_x[:i*num_val_samples],\n",
        "      train_x[(i+1)*num_val_samples:]\n",
        "  ], axis = 0)\n",
        "  par_train_targets = np.concatenate([\n",
        "      train_y[:i*num_val_samples],\n",
        "      train_y[(i+1)*num_val_samples:]\n",
        "  ], axis = 0)\n",
        "\n",
        "  # 모델 구성\n",
        "  model_1 = build_model()\n",
        "\n",
        "  # fit 해주기\n",
        "  history = model_1.fit(\n",
        "      par_train_data, par_train_targets,\n",
        "      validation_data=(val_data, val_targets),\n",
        "      epochs = num_epochs, batch_size=16, verbose=0  )\n",
        "\n",
        "  # 각 폴드의 validation 값 저장하기\n",
        "  mae_history = history.history['val_mae']\n",
        "\n",
        "  # 저장해주기\n",
        "  all_mae_histories.append(mae_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz0aN3Hoi3uR",
        "outputId": "8d2e24a9-be5e-4879-d644-c1ac2568f3f9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0번째 폴드 처리중\n",
            "1번째 폴드 처리중\n",
            "2번째 폴드 처리중\n",
            "3번째 폴드 처리중\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vaildation 점수 평균 기록하기\n",
        "avg_mae_history = [ np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs) ]"
      ],
      "metadata": {
        "id": "_5Hx46Jgi4cy"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_mae_history[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay2Upa7oi5Tx",
        "outputId": "d2639c81-841f-41e9-b62e-b70d9df144ae"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[19.48509979248047, 16.096861600875854, 11.831308603286743, 7.925925016403198]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vaildation 점수 그래프\n",
        "# 처음 몇 번의 epochs동안 mae가 너무 높기때문에 가시성을 위해 제거\n",
        "plt.plot(range(1, len(avg_mae_history[10:])+1), avg_mae_history[10:])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('validation MAE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "6OT2LtHYi6gB",
        "outputId": "60b0b3cb-32a1-4186-e8a3-0e873a1d7647"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'validation MAE')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "책에서는 약 120 ~ 140번째 epochs 이후에 검증 MAE가 줄어들기를 멈추고, 과대적합이 시작되었다고 보고 있다. 따라서 epochs를 130으로 잡고 모델을 훈련시킨 뒤 test 해보자"
      ],
      "metadata": {
        "id": "-4dePdiCi8ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = build_model()\n",
        "model_2.fit(train_x, train_y, epochs=130, batch_size=16, verbose=0)\n",
        "model_2.evaluate(test_x, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqkBRJBri7bx",
        "outputId": "8ac92118-8be0-4a26-838a-b18a374dd056"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 17.6082 - mae: 2.8086\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17.60822868347168, 2.808640480041504]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 조기 종료 시키기(EarlyStopping)\n",
        "과적합을 방지하는 방법 중 하나로 사용하는 콜백 함수이다. 적절한 시점에 학습을 종료시킬 수 있도록 옵션을 주어 사용하는 콜백함수\n",
        "```python\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping()\n",
        "model.fit(X_train, Y_train, epoch = 1000, callbacks = [early_stopping])\n",
        "```\n",
        "위와 같이 설정하면 에폭을 1000으로 지정했더라도 콜백함수에서 설정한 조건을 만족하면 학습을 조기종료시킨다. 조건 설정을 위한 파라미터는 아래와 같다\n",
        "```python\n",
        "EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 0, mode = 'auto')\n",
        "```\n",
        "- `monitor` : 학습 조기종료를 위해 관찰하는 항목. val_loss나 val_accuracy가 주로 사용된다. (default = val_loss)\n",
        "- `min_delta` : 개선되고 있다고 판단하기 위한 최소 변화량. 만약 변화량이 min_delta보다 적은 경우에는 개선이 없다고 판단 (default = 0)\n",
        "- `patience` : 개선이 안된다고 바로 종료시키지 않고, 개선을 위해 몇 번의 에폭을 기다릴지 설정한다. (default = 0)\n",
        "- `mode` : 관찰항목에 대해 개선이 없다고 판단하기 위한 기준을 설정. monitor에서 설정한 항목이 val_loss이면 값이 감소되지 않을 때 종료하여야 하므로 min을 설정하고, val_accuracy의 경우에는 max를 설정해야 한다. (default = auto)<br>\n",
        "  - auto : monitor에 설정된 이름에 따라 자동으로 지덩\n",
        "  - min : 관찰값이 감소하는 것을 멈출 때 학습을 종료하겠다.\n",
        "  - max : 관찰값이 증가하는 것을 멈출 때, 학습을 종료"
      ],
      "metadata": {
        "id": "9DODev7si_OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 20, mode = 'auto')\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_x) // k\n",
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "\n",
        "for i in range(k) :\n",
        "  print(f'{i}번째 폴드 처리중')\n",
        "  # 분할 하나를 validation 용으로 빼놓기\n",
        "  val_data = train_x[i*num_val_samples : (i+1)*num_val_samples]\n",
        "  val_targets = train_y[i*num_val_samples : (i+1)*num_val_samples]\n",
        "\n",
        "  # 다른 분할 전체를 train 용으로 데이터 준비\n",
        "  par_train_data = np.concatenate([\n",
        "      train_x[:i*num_val_samples],\n",
        "      train_x[(i+1)*num_val_samples:]\n",
        "  ], axis = 0)\n",
        "  par_train_targets = np.concatenate([\n",
        "      train_y[:i*num_val_samples],\n",
        "      train_y[(i+1)*num_val_samples:]\n",
        "  ], axis = 0)\n",
        "\n",
        "  # 모델 구성\n",
        "  model = build_model()\n",
        "\n",
        "  # fit 해주기\n",
        "  history = model.fit(\n",
        "      par_train_data, par_train_targets,\n",
        "      validation_data=(val_data, val_targets),\n",
        "      epochs = num_epochs, batch_size=16, verbose=1,\n",
        "      callbacks = [early_stopping])\n",
        "\n",
        "  # 각 폴드의 validation 값 저장하기\n",
        "  mae_history = history.history['val_mae']\n",
        "\n",
        "  # 저장해주기\n",
        "  all_mae_histories.append(mae_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-auMuX1i9qx",
        "outputId": "8574c4a6-5b4b-4872-9f6e-e630d157111c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0번째 폴드 처리중\n",
            "Epoch 1/500\n",
            "19/19 [==============================] - 1s 11ms/step - loss: 544.0913 - mae: 21.5044 - val_loss: 469.6920 - val_mae: 19.4342\n",
            "Epoch 2/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 428.0374 - mae: 18.7335 - val_loss: 335.1812 - val_mae: 16.0393\n",
            "Epoch 3/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 290.8565 - mae: 14.9892 - val_loss: 196.6243 - val_mae: 11.8786\n",
            "Epoch 4/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 164.2943 - mae: 10.6648 - val_loss: 94.4267 - val_mae: 7.6779\n",
            "Epoch 5/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 82.8760 - mae: 7.2402 - val_loss: 47.5909 - val_mae: 5.1174\n",
            "Epoch 6/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 46.7261 - mae: 5.2827 - val_loss: 33.1936 - val_mae: 3.9953\n",
            "Epoch 7/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 33.2748 - mae: 4.3421 - val_loss: 26.4162 - val_mae: 3.3796\n",
            "Epoch 8/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 26.6302 - mae: 3.7994 - val_loss: 22.6836 - val_mae: 3.0035\n",
            "Epoch 9/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 23.1386 - mae: 3.4480 - val_loss: 20.8308 - val_mae: 2.9568\n",
            "Epoch 10/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 21.0975 - mae: 3.2931 - val_loss: 19.3594 - val_mae: 2.8977\n",
            "Epoch 11/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 19.5067 - mae: 3.1178 - val_loss: 18.5939 - val_mae: 2.8438\n",
            "Epoch 12/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 18.3376 - mae: 3.0207 - val_loss: 16.8067 - val_mae: 2.7219\n",
            "Epoch 13/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 17.0996 - mae: 2.8973 - val_loss: 15.6371 - val_mae: 2.6819\n",
            "Epoch 14/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 16.1221 - mae: 2.8052 - val_loss: 14.9487 - val_mae: 2.6594\n",
            "Epoch 15/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 15.1623 - mae: 2.7422 - val_loss: 14.1310 - val_mae: 2.5621\n",
            "Epoch 16/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 14.3819 - mae: 2.6360 - val_loss: 13.9104 - val_mae: 2.5985\n",
            "Epoch 17/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 13.9470 - mae: 2.6430 - val_loss: 12.7169 - val_mae: 2.4856\n",
            "Epoch 18/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 13.3574 - mae: 2.5504 - val_loss: 12.2276 - val_mae: 2.3875\n",
            "Epoch 19/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 12.7135 - mae: 2.4847 - val_loss: 11.4645 - val_mae: 2.3046\n",
            "Epoch 20/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 12.5745 - mae: 2.4547 - val_loss: 11.2411 - val_mae: 2.3412\n",
            "Epoch 21/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 11.9258 - mae: 2.4203 - val_loss: 11.2014 - val_mae: 2.3583\n",
            "Epoch 22/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 11.9398 - mae: 2.4218 - val_loss: 10.8277 - val_mae: 2.2538\n",
            "Epoch 23/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 11.4044 - mae: 2.3731 - val_loss: 10.5177 - val_mae: 2.2373\n",
            "Epoch 24/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 11.3010 - mae: 2.2975 - val_loss: 10.0569 - val_mae: 2.1965\n",
            "Epoch 25/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 11.1029 - mae: 2.3608 - val_loss: 10.3506 - val_mae: 2.1999\n",
            "Epoch 26/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.9592 - mae: 2.2898 - val_loss: 10.2941 - val_mae: 2.2562\n",
            "Epoch 27/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.8243 - mae: 2.2737 - val_loss: 10.4689 - val_mae: 2.2911\n",
            "Epoch 28/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.7198 - mae: 2.2661 - val_loss: 9.4950 - val_mae: 2.0919\n",
            "Epoch 29/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.4172 - mae: 2.2534 - val_loss: 10.1598 - val_mae: 2.2237\n",
            "Epoch 30/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.3407 - mae: 2.2119 - val_loss: 9.3895 - val_mae: 2.0946\n",
            "Epoch 31/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.0599 - mae: 2.2536 - val_loss: 9.4110 - val_mae: 2.0278\n",
            "Epoch 32/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.1368 - mae: 2.2121 - val_loss: 9.2326 - val_mae: 2.1111\n",
            "Epoch 33/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.1556 - mae: 2.2241 - val_loss: 8.9783 - val_mae: 2.0235\n",
            "Epoch 34/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.9178 - mae: 2.1866 - val_loss: 9.0100 - val_mae: 2.0982\n",
            "Epoch 35/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.8608 - mae: 2.1851 - val_loss: 8.7856 - val_mae: 2.0537\n",
            "Epoch 36/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.5136 - mae: 2.1391 - val_loss: 9.2113 - val_mae: 2.1526\n",
            "Epoch 37/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.5690 - mae: 2.1518 - val_loss: 8.6557 - val_mae: 1.9925\n",
            "Epoch 38/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.2777 - mae: 2.1462 - val_loss: 8.8744 - val_mae: 2.0731\n",
            "Epoch 39/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.5574 - mae: 2.1421 - val_loss: 8.4448 - val_mae: 1.9507\n",
            "Epoch 40/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.1803 - mae: 2.1015 - val_loss: 8.5284 - val_mae: 2.0412\n",
            "Epoch 41/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.9641 - mae: 2.1065 - val_loss: 8.5497 - val_mae: 1.9667\n",
            "Epoch 42/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.1862 - mae: 2.0756 - val_loss: 8.4173 - val_mae: 1.9608\n",
            "Epoch 43/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.9499 - mae: 2.0201 - val_loss: 8.3785 - val_mae: 1.9445\n",
            "Epoch 44/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.9595 - mae: 2.0399 - val_loss: 8.3325 - val_mae: 2.0193\n",
            "Epoch 45/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.9044 - mae: 2.0789 - val_loss: 8.1458 - val_mae: 1.9451\n",
            "Epoch 46/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 8.4607 - mae: 2.0208 - val_loss: 8.4437 - val_mae: 1.9515\n",
            "Epoch 47/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.8049 - mae: 2.0698 - val_loss: 8.3192 - val_mae: 1.9806\n",
            "Epoch 48/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.5895 - mae: 1.9826 - val_loss: 8.2386 - val_mae: 1.9958\n",
            "Epoch 49/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.6896 - mae: 2.0469 - val_loss: 7.9669 - val_mae: 1.9172\n",
            "Epoch 50/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.6428 - mae: 2.0608 - val_loss: 8.0151 - val_mae: 1.9623\n",
            "Epoch 51/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.4591 - mae: 2.0277 - val_loss: 8.2126 - val_mae: 1.9676\n",
            "Epoch 52/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.5694 - mae: 1.9997 - val_loss: 7.9194 - val_mae: 1.9335\n",
            "Epoch 53/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 8.4190 - mae: 2.0120 - val_loss: 8.3686 - val_mae: 1.9852\n",
            "Epoch 54/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.4166 - mae: 1.9801 - val_loss: 7.9105 - val_mae: 1.9913\n",
            "Epoch 55/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.1389 - mae: 1.9596 - val_loss: 7.9598 - val_mae: 1.9354\n",
            "Epoch 56/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.1923 - mae: 1.9936 - val_loss: 7.9822 - val_mae: 1.9944\n",
            "Epoch 57/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.1015 - mae: 1.9500 - val_loss: 7.6776 - val_mae: 1.8840\n",
            "Epoch 58/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 7.9916 - mae: 1.9416 - val_loss: 7.8270 - val_mae: 2.0135\n",
            "Epoch 59/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.8914 - mae: 1.9513 - val_loss: 7.9499 - val_mae: 1.9887\n",
            "Epoch 60/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.9160 - mae: 1.9596 - val_loss: 7.7342 - val_mae: 1.9780\n",
            "Epoch 61/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.9357 - mae: 1.9253 - val_loss: 7.9356 - val_mae: 1.9813\n",
            "Epoch 62/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.7707 - mae: 1.9279 - val_loss: 7.6713 - val_mae: 1.9463\n",
            "Epoch 63/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.7374 - mae: 1.9265 - val_loss: 8.0907 - val_mae: 1.9423\n",
            "Epoch 64/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.7163 - mae: 1.8903 - val_loss: 7.6196 - val_mae: 1.9118\n",
            "Epoch 65/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.8141 - mae: 1.8930 - val_loss: 7.6175 - val_mae: 1.9483\n",
            "Epoch 66/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.6417 - mae: 1.8723 - val_loss: 7.8305 - val_mae: 2.0360\n",
            "Epoch 67/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.3373 - mae: 1.8710 - val_loss: 7.8125 - val_mae: 1.9004\n",
            "Epoch 68/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.5427 - mae: 1.8650 - val_loss: 7.9771 - val_mae: 2.0515\n",
            "Epoch 69/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.3150 - mae: 1.8691 - val_loss: 7.7422 - val_mae: 2.0374\n",
            "Epoch 70/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.6054 - mae: 1.9247 - val_loss: 7.5480 - val_mae: 1.9335\n",
            "Epoch 71/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.2898 - mae: 1.8788 - val_loss: 8.3433 - val_mae: 1.9611\n",
            "Epoch 72/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.3574 - mae: 1.8831 - val_loss: 7.6565 - val_mae: 1.9642\n",
            "Epoch 73/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.0697 - mae: 1.8201 - val_loss: 7.7035 - val_mae: 1.9885\n",
            "Epoch 74/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.3372 - mae: 1.8375 - val_loss: 7.5695 - val_mae: 2.0262\n",
            "Epoch 75/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1071 - mae: 1.8515 - val_loss: 8.2584 - val_mae: 2.0918\n",
            "Epoch 76/500\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 7.0127 - mae: 1.8068 - val_loss: 7.3635 - val_mae: 1.9643\n",
            "Epoch 77/500\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 7.1222 - mae: 1.8001 - val_loss: 7.3840 - val_mae: 1.9572\n",
            "Epoch 78/500\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 6.6687 - mae: 1.7915 - val_loss: 8.1545 - val_mae: 1.9357\n",
            "Epoch 79/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1588 - mae: 1.8135 - val_loss: 8.4130 - val_mae: 2.2224\n",
            "Epoch 80/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1937 - mae: 1.8645 - val_loss: 7.5439 - val_mae: 1.9807\n",
            "Epoch 81/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.6955 - mae: 1.8134 - val_loss: 7.6171 - val_mae: 1.9635\n",
            "Epoch 82/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.0783 - mae: 1.8147 - val_loss: 7.4123 - val_mae: 1.8969\n",
            "Epoch 83/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.9253 - mae: 1.8013 - val_loss: 7.2748 - val_mae: 1.9961\n",
            "Epoch 84/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.7209 - mae: 1.7752 - val_loss: 7.6323 - val_mae: 2.0587\n",
            "Epoch 85/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.8016 - mae: 1.8070 - val_loss: 7.4406 - val_mae: 2.0012\n",
            "Epoch 86/500\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 6.6624 - mae: 1.7798 - val_loss: 7.8390 - val_mae: 2.0732\n",
            "Epoch 87/500\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 6.7371 - mae: 1.7705 - val_loss: 7.8596 - val_mae: 2.1754\n",
            "Epoch 88/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.6877 - mae: 1.7918 - val_loss: 7.1949 - val_mae: 1.9730\n",
            "Epoch 89/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.4803 - mae: 1.7819 - val_loss: 7.8693 - val_mae: 2.1608\n",
            "Epoch 90/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.4146 - mae: 1.7869 - val_loss: 7.7522 - val_mae: 1.9415\n",
            "Epoch 91/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.4442 - mae: 1.7611 - val_loss: 7.0723 - val_mae: 1.9804\n",
            "Epoch 92/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.4671 - mae: 1.7329 - val_loss: 6.9994 - val_mae: 1.9401\n",
            "Epoch 93/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.5013 - mae: 1.7665 - val_loss: 7.1714 - val_mae: 1.9093\n",
            "Epoch 94/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.3342 - mae: 1.7522 - val_loss: 7.2725 - val_mae: 1.8776\n",
            "Epoch 95/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.4892 - mae: 1.7765 - val_loss: 7.4058 - val_mae: 2.0515\n",
            "Epoch 96/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.3996 - mae: 1.7217 - val_loss: 7.1582 - val_mae: 1.9095\n",
            "Epoch 97/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.2041 - mae: 1.6995 - val_loss: 7.2435 - val_mae: 2.0496\n",
            "Epoch 98/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9506 - mae: 1.6663 - val_loss: 8.6313 - val_mae: 2.2875\n",
            "Epoch 99/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.3046 - mae: 1.7498 - val_loss: 7.0276 - val_mae: 1.9156\n",
            "Epoch 100/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.2980 - mae: 1.7228 - val_loss: 7.3610 - val_mae: 2.0477\n",
            "Epoch 101/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.3119 - mae: 1.7213 - val_loss: 7.2989 - val_mae: 2.0575\n",
            "Epoch 102/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.1425 - mae: 1.7381 - val_loss: 7.0997 - val_mae: 1.9248\n",
            "Epoch 103/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9955 - mae: 1.6774 - val_loss: 6.9262 - val_mae: 1.9100\n",
            "Epoch 104/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.9589 - mae: 1.6476 - val_loss: 7.2052 - val_mae: 2.0696\n",
            "Epoch 105/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.0137 - mae: 1.6967 - val_loss: 7.1294 - val_mae: 2.0671\n",
            "Epoch 106/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2924 - mae: 1.7126 - val_loss: 7.0749 - val_mae: 1.9692\n",
            "Epoch 107/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.7243 - mae: 1.6361 - val_loss: 6.8794 - val_mae: 1.9254\n",
            "Epoch 108/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.0579 - mae: 1.7039 - val_loss: 7.2118 - val_mae: 2.0094\n",
            "Epoch 109/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.7970 - mae: 1.6648 - val_loss: 6.8535 - val_mae: 1.9657\n",
            "Epoch 110/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.0096 - mae: 1.7006 - val_loss: 7.0104 - val_mae: 1.9850\n",
            "Epoch 111/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.8424 - mae: 1.6563 - val_loss: 7.0689 - val_mae: 1.9889\n",
            "Epoch 112/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.8905 - mae: 1.6901 - val_loss: 7.0372 - val_mae: 1.9577\n",
            "Epoch 113/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.8948 - mae: 1.6763 - val_loss: 6.9954 - val_mae: 1.9489\n",
            "Epoch 114/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.7922 - mae: 1.6643 - val_loss: 7.3206 - val_mae: 2.0671\n",
            "Epoch 115/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.6942 - mae: 1.6721 - val_loss: 7.9962 - val_mae: 2.0957\n",
            "Epoch 116/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.7991 - mae: 1.6596 - val_loss: 6.9606 - val_mae: 1.9995\n",
            "Epoch 117/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.6576 - mae: 1.6485 - val_loss: 7.0342 - val_mae: 1.9905\n",
            "Epoch 118/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5790 - mae: 1.6400 - val_loss: 7.1791 - val_mae: 2.0789\n",
            "Epoch 119/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5919 - mae: 1.6030 - val_loss: 7.2973 - val_mae: 2.0031\n",
            "Epoch 120/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4663 - mae: 1.6136 - val_loss: 7.1911 - val_mae: 2.0539\n",
            "Epoch 121/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5928 - mae: 1.6745 - val_loss: 6.9005 - val_mae: 1.9530\n",
            "Epoch 122/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.5020 - mae: 1.6161 - val_loss: 7.1078 - val_mae: 2.0768\n",
            "Epoch 123/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5450 - mae: 1.6406 - val_loss: 7.0402 - val_mae: 2.0157\n",
            "Epoch 124/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4571 - mae: 1.5930 - val_loss: 6.8888 - val_mae: 1.9953\n",
            "Epoch 125/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3251 - mae: 1.6191 - val_loss: 7.0644 - val_mae: 2.0252\n",
            "Epoch 126/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.4884 - mae: 1.6144 - val_loss: 7.0678 - val_mae: 1.9788\n",
            "Epoch 127/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4474 - mae: 1.6107 - val_loss: 7.2409 - val_mae: 1.9846\n",
            "Epoch 128/500\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 5.3796 - mae: 1.6035 - val_loss: 7.0387 - val_mae: 1.9928\n",
            "Epoch 129/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 5.3481 - mae: 1.6012 - val_loss: 6.9404 - val_mae: 2.0367\n",
            "1번째 폴드 처리중\n",
            "Epoch 1/500\n",
            "19/19 [==============================] - 1s 10ms/step - loss: 523.7674 - mae: 20.9875 - val_loss: 422.6526 - val_mae: 18.8992\n",
            "Epoch 2/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 397.8425 - mae: 17.9513 - val_loss: 297.9787 - val_mae: 15.6104\n",
            "Epoch 3/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 261.6661 - mae: 14.0552 - val_loss: 167.5715 - val_mae: 11.2442\n",
            "Epoch 4/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 140.9363 - mae: 9.5834 - val_loss: 80.2110 - val_mae: 7.2835\n",
            "Epoch 5/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 75.7120 - mae: 6.5330 - val_loss: 48.6070 - val_mae: 5.4337\n",
            "Epoch 6/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 48.4074 - mae: 5.1430 - val_loss: 35.4879 - val_mae: 4.6088\n",
            "Epoch 7/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 35.7477 - mae: 4.2543 - val_loss: 28.8283 - val_mae: 4.1106\n",
            "Epoch 8/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 29.0440 - mae: 3.8119 - val_loss: 25.6977 - val_mae: 3.8954\n",
            "Epoch 9/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 25.1897 - mae: 3.4788 - val_loss: 22.7059 - val_mae: 3.5221\n",
            "Epoch 10/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 22.9226 - mae: 3.3081 - val_loss: 21.0677 - val_mae: 3.3871\n",
            "Epoch 11/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 20.8207 - mae: 3.0792 - val_loss: 20.9077 - val_mae: 3.4845\n",
            "Epoch 12/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 19.2164 - mae: 2.9954 - val_loss: 18.8009 - val_mae: 3.1678\n",
            "Epoch 13/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 17.9507 - mae: 2.8863 - val_loss: 18.0247 - val_mae: 3.1010\n",
            "Epoch 14/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 16.7549 - mae: 2.7674 - val_loss: 17.5477 - val_mae: 3.0762\n",
            "Epoch 15/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 15.8476 - mae: 2.7020 - val_loss: 16.6641 - val_mae: 3.0097\n",
            "Epoch 16/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 14.6449 - mae: 2.5666 - val_loss: 18.2033 - val_mae: 3.2172\n",
            "Epoch 17/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 14.1003 - mae: 2.6092 - val_loss: 15.3651 - val_mae: 2.8546\n",
            "Epoch 18/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 13.3958 - mae: 2.4809 - val_loss: 15.7050 - val_mae: 2.9332\n",
            "Epoch 19/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 12.8736 - mae: 2.4438 - val_loss: 14.5662 - val_mae: 2.8515\n",
            "Epoch 20/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 12.2300 - mae: 2.3960 - val_loss: 15.0883 - val_mae: 2.9721\n",
            "Epoch 21/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 12.1384 - mae: 2.4099 - val_loss: 13.4369 - val_mae: 2.7826\n",
            "Epoch 22/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 11.8381 - mae: 2.3501 - val_loss: 13.5605 - val_mae: 2.7429\n",
            "Epoch 23/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 11.1567 - mae: 2.3245 - val_loss: 12.6282 - val_mae: 2.7148\n",
            "Epoch 24/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 11.3432 - mae: 2.3068 - val_loss: 13.1452 - val_mae: 2.7491\n",
            "Epoch 25/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.6657 - mae: 2.2652 - val_loss: 12.5562 - val_mae: 2.7448\n",
            "Epoch 26/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.4773 - mae: 2.2210 - val_loss: 12.4835 - val_mae: 2.6302\n",
            "Epoch 27/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.3889 - mae: 2.1920 - val_loss: 11.5393 - val_mae: 2.5744\n",
            "Epoch 28/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.3057 - mae: 2.1543 - val_loss: 12.3732 - val_mae: 2.6671\n",
            "Epoch 29/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.9883 - mae: 2.1731 - val_loss: 12.2513 - val_mae: 2.6645\n",
            "Epoch 30/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.0554 - mae: 2.1835 - val_loss: 12.6744 - val_mae: 2.6859\n",
            "Epoch 31/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.6014 - mae: 2.1233 - val_loss: 11.6134 - val_mae: 2.6058\n",
            "Epoch 32/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.7047 - mae: 2.1333 - val_loss: 12.0903 - val_mae: 2.6640\n",
            "Epoch 33/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.4706 - mae: 2.1064 - val_loss: 12.2240 - val_mae: 2.6872\n",
            "Epoch 34/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.4950 - mae: 2.1064 - val_loss: 11.3256 - val_mae: 2.5730\n",
            "Epoch 35/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.1798 - mae: 2.0652 - val_loss: 11.8346 - val_mae: 2.6313\n",
            "Epoch 36/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.1278 - mae: 2.0627 - val_loss: 12.2367 - val_mae: 2.7337\n",
            "Epoch 37/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.1343 - mae: 2.0644 - val_loss: 11.3657 - val_mae: 2.5913\n",
            "Epoch 38/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.9050 - mae: 2.0498 - val_loss: 11.2381 - val_mae: 2.5940\n",
            "Epoch 39/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.7756 - mae: 2.0093 - val_loss: 12.2370 - val_mae: 2.7442\n",
            "Epoch 40/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.6697 - mae: 2.0310 - val_loss: 11.2612 - val_mae: 2.5863\n",
            "Epoch 41/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.9784 - mae: 2.0196 - val_loss: 11.3978 - val_mae: 2.6021\n",
            "Epoch 42/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.5822 - mae: 2.0239 - val_loss: 10.8998 - val_mae: 2.5326\n",
            "Epoch 43/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.4862 - mae: 2.0220 - val_loss: 10.8428 - val_mae: 2.5280\n",
            "Epoch 44/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.5382 - mae: 1.9987 - val_loss: 11.6484 - val_mae: 2.6233\n",
            "Epoch 45/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.4150 - mae: 2.0203 - val_loss: 10.6489 - val_mae: 2.5011\n",
            "Epoch 46/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.3507 - mae: 1.9806 - val_loss: 10.7079 - val_mae: 2.5204\n",
            "Epoch 47/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.1635 - mae: 1.9957 - val_loss: 10.9664 - val_mae: 2.5606\n",
            "Epoch 48/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.0778 - mae: 1.9538 - val_loss: 10.8140 - val_mae: 2.5466\n",
            "Epoch 49/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.1074 - mae: 1.9635 - val_loss: 11.0178 - val_mae: 2.5756\n",
            "Epoch 50/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.0980 - mae: 1.9567 - val_loss: 10.9820 - val_mae: 2.5579\n",
            "Epoch 51/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.8385 - mae: 1.9367 - val_loss: 12.2070 - val_mae: 2.7634\n",
            "Epoch 52/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.8602 - mae: 1.9372 - val_loss: 11.2926 - val_mae: 2.5990\n",
            "Epoch 53/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.0174 - mae: 1.9340 - val_loss: 10.8363 - val_mae: 2.5359\n",
            "Epoch 54/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.7983 - mae: 1.9004 - val_loss: 11.9052 - val_mae: 2.6891\n",
            "Epoch 55/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.6888 - mae: 1.9578 - val_loss: 10.8807 - val_mae: 2.5529\n",
            "Epoch 56/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.7350 - mae: 1.9230 - val_loss: 11.0801 - val_mae: 2.6110\n",
            "Epoch 57/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.6737 - mae: 1.9225 - val_loss: 10.3259 - val_mae: 2.4747\n",
            "Epoch 58/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.4706 - mae: 1.9002 - val_loss: 10.1706 - val_mae: 2.4479\n",
            "Epoch 59/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.5186 - mae: 1.9075 - val_loss: 11.9259 - val_mae: 2.7130\n",
            "Epoch 60/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.4215 - mae: 1.9246 - val_loss: 11.7358 - val_mae: 2.6899\n",
            "Epoch 61/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 7.4364 - mae: 1.8687 - val_loss: 9.9509 - val_mae: 2.4177\n",
            "Epoch 62/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.3299 - mae: 1.8724 - val_loss: 10.7289 - val_mae: 2.5416\n",
            "Epoch 63/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.1586 - mae: 1.8628 - val_loss: 10.7069 - val_mae: 2.5461\n",
            "Epoch 64/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.2996 - mae: 1.8815 - val_loss: 10.7421 - val_mae: 2.5656\n",
            "Epoch 65/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.0286 - mae: 1.8256 - val_loss: 10.7846 - val_mae: 2.5822\n",
            "Epoch 66/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.0422 - mae: 1.8498 - val_loss: 12.1185 - val_mae: 2.7567\n",
            "Epoch 67/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1049 - mae: 1.8604 - val_loss: 10.2302 - val_mae: 2.4811\n",
            "Epoch 68/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.9770 - mae: 1.8385 - val_loss: 10.5564 - val_mae: 2.5164\n",
            "Epoch 69/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.9486 - mae: 1.8236 - val_loss: 11.0617 - val_mae: 2.6003\n",
            "Epoch 70/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.8721 - mae: 1.8617 - val_loss: 9.5686 - val_mae: 2.3693\n",
            "Epoch 71/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1180 - mae: 1.8460 - val_loss: 10.0845 - val_mae: 2.4453\n",
            "Epoch 72/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.7617 - mae: 1.7872 - val_loss: 9.7836 - val_mae: 2.3967\n",
            "Epoch 73/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.7923 - mae: 1.7978 - val_loss: 9.8418 - val_mae: 2.4227\n",
            "Epoch 74/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.8385 - mae: 1.8012 - val_loss: 10.2984 - val_mae: 2.4830\n",
            "Epoch 75/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.5530 - mae: 1.7843 - val_loss: 10.2879 - val_mae: 2.4678\n",
            "Epoch 76/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.4975 - mae: 1.7573 - val_loss: 10.6119 - val_mae: 2.5305\n",
            "Epoch 77/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.9379 - mae: 1.8329 - val_loss: 10.2006 - val_mae: 2.4613\n",
            "Epoch 78/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.5048 - mae: 1.7910 - val_loss: 10.3821 - val_mae: 2.4892\n",
            "Epoch 79/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.4468 - mae: 1.7768 - val_loss: 10.2017 - val_mae: 2.4586\n",
            "Epoch 80/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.5059 - mae: 1.7705 - val_loss: 10.3028 - val_mae: 2.4764\n",
            "Epoch 81/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.4647 - mae: 1.7523 - val_loss: 10.5672 - val_mae: 2.5263\n",
            "Epoch 82/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2054 - mae: 1.7347 - val_loss: 11.0690 - val_mae: 2.5665\n",
            "Epoch 83/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.3165 - mae: 1.7780 - val_loss: 9.6225 - val_mae: 2.3746\n",
            "Epoch 84/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.2447 - mae: 1.7067 - val_loss: 9.5807 - val_mae: 2.3623\n",
            "Epoch 85/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2316 - mae: 1.7452 - val_loss: 10.9416 - val_mae: 2.5404\n",
            "Epoch 86/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.1297 - mae: 1.7135 - val_loss: 10.4497 - val_mae: 2.4874\n",
            "Epoch 87/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.1471 - mae: 1.7191 - val_loss: 11.4950 - val_mae: 2.6246\n",
            "Epoch 88/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.0559 - mae: 1.7257 - val_loss: 11.1476 - val_mae: 2.5884\n",
            "Epoch 89/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9708 - mae: 1.7161 - val_loss: 12.8117 - val_mae: 2.7891\n",
            "Epoch 90/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.1216 - mae: 1.7645 - val_loss: 9.9410 - val_mae: 2.4066\n",
            "2번째 폴드 처리중\n",
            "Epoch 1/500\n",
            "19/19 [==============================] - 1s 10ms/step - loss: 528.7328 - mae: 20.8687 - val_loss: 389.5137 - val_mae: 17.8923\n",
            "Epoch 2/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 399.4750 - mae: 17.5757 - val_loss: 272.7546 - val_mae: 14.3979\n",
            "Epoch 3/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 261.4830 - mae: 13.7783 - val_loss: 156.6724 - val_mae: 10.5365\n",
            "Epoch 4/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 138.5291 - mae: 9.4167 - val_loss: 79.1729 - val_mae: 7.2118\n",
            "Epoch 5/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 70.8602 - mae: 6.2762 - val_loss: 47.5976 - val_mae: 5.2902\n",
            "Epoch 6/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 45.5413 - mae: 4.9550 - val_loss: 36.0367 - val_mae: 4.2896\n",
            "Epoch 7/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 33.5401 - mae: 4.1849 - val_loss: 30.8484 - val_mae: 3.7952\n",
            "Epoch 8/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 27.6785 - mae: 3.7687 - val_loss: 28.1897 - val_mae: 3.4884\n",
            "Epoch 9/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 24.1554 - mae: 3.4563 - val_loss: 26.0806 - val_mae: 3.3432\n",
            "Epoch 10/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 21.6000 - mae: 3.2680 - val_loss: 24.2949 - val_mae: 3.2191\n",
            "Epoch 11/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 19.3077 - mae: 3.0960 - val_loss: 22.3923 - val_mae: 3.1036\n",
            "Epoch 12/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 17.6953 - mae: 2.9983 - val_loss: 21.3609 - val_mae: 3.0145\n",
            "Epoch 13/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 16.0297 - mae: 2.8623 - val_loss: 20.7660 - val_mae: 2.9939\n",
            "Epoch 14/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 14.9579 - mae: 2.7538 - val_loss: 19.3204 - val_mae: 2.9162\n",
            "Epoch 15/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 13.9482 - mae: 2.6563 - val_loss: 18.3034 - val_mae: 2.8163\n",
            "Epoch 16/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 12.9044 - mae: 2.5795 - val_loss: 17.6262 - val_mae: 2.8176\n",
            "Epoch 17/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 12.5049 - mae: 2.5717 - val_loss: 17.3565 - val_mae: 2.8207\n",
            "Epoch 18/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 11.9153 - mae: 2.5288 - val_loss: 17.3570 - val_mae: 2.6354\n",
            "Epoch 19/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 11.2358 - mae: 2.4407 - val_loss: 16.6878 - val_mae: 2.6675\n",
            "Epoch 20/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.8590 - mae: 2.4392 - val_loss: 16.1986 - val_mae: 2.6621\n",
            "Epoch 21/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.8234 - mae: 2.4316 - val_loss: 15.8206 - val_mae: 2.6188\n",
            "Epoch 22/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.3345 - mae: 2.3550 - val_loss: 15.7318 - val_mae: 2.6175\n",
            "Epoch 23/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.1698 - mae: 2.3394 - val_loss: 15.4267 - val_mae: 2.6118\n",
            "Epoch 24/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.8441 - mae: 2.3036 - val_loss: 15.1512 - val_mae: 2.5760\n",
            "Epoch 25/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 9.5683 - mae: 2.2851 - val_loss: 15.0801 - val_mae: 2.5637\n",
            "Epoch 26/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.3201 - mae: 2.2653 - val_loss: 14.8649 - val_mae: 2.5579\n",
            "Epoch 27/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.0681 - mae: 2.1892 - val_loss: 14.9760 - val_mae: 2.6753\n",
            "Epoch 28/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.9607 - mae: 2.2220 - val_loss: 14.6958 - val_mae: 2.5763\n",
            "Epoch 29/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.9503 - mae: 2.2310 - val_loss: 14.7778 - val_mae: 2.5376\n",
            "Epoch 30/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.8632 - mae: 2.2162 - val_loss: 14.8166 - val_mae: 2.5296\n",
            "Epoch 31/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.5803 - mae: 2.1370 - val_loss: 14.7013 - val_mae: 2.5298\n",
            "Epoch 32/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.3466 - mae: 2.1109 - val_loss: 14.7174 - val_mae: 2.6425\n",
            "Epoch 33/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.1154 - mae: 2.1030 - val_loss: 15.1778 - val_mae: 2.5996\n",
            "Epoch 34/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.0212 - mae: 2.0943 - val_loss: 14.9622 - val_mae: 2.7232\n",
            "Epoch 35/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.1448 - mae: 2.0822 - val_loss: 14.3312 - val_mae: 2.5035\n",
            "Epoch 36/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.8182 - mae: 2.0381 - val_loss: 14.4691 - val_mae: 2.6207\n",
            "Epoch 37/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.9355 - mae: 2.0812 - val_loss: 14.0935 - val_mae: 2.5027\n",
            "Epoch 38/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.9052 - mae: 2.0677 - val_loss: 13.9572 - val_mae: 2.5412\n",
            "Epoch 39/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.6480 - mae: 2.0401 - val_loss: 14.1455 - val_mae: 2.5146\n",
            "Epoch 40/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.7388 - mae: 2.0055 - val_loss: 13.9396 - val_mae: 2.5049\n",
            "Epoch 41/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.4311 - mae: 2.0205 - val_loss: 14.3171 - val_mae: 2.5494\n",
            "Epoch 42/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.3973 - mae: 2.0279 - val_loss: 14.2604 - val_mae: 2.5131\n",
            "Epoch 43/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1532 - mae: 1.9712 - val_loss: 13.7256 - val_mae: 2.5265\n",
            "Epoch 44/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.2579 - mae: 1.9630 - val_loss: 14.1129 - val_mae: 2.4828\n",
            "Epoch 45/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.0341 - mae: 1.9391 - val_loss: 14.4928 - val_mae: 2.6988\n",
            "Epoch 46/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.1925 - mae: 1.9340 - val_loss: 13.9857 - val_mae: 2.6240\n",
            "Epoch 47/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.8472 - mae: 1.9286 - val_loss: 13.4188 - val_mae: 2.4997\n",
            "Epoch 48/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.8175 - mae: 1.9701 - val_loss: 13.8515 - val_mae: 2.4803\n",
            "Epoch 49/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.8761 - mae: 1.9175 - val_loss: 13.4679 - val_mae: 2.5033\n",
            "Epoch 50/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.7596 - mae: 1.9181 - val_loss: 13.8271 - val_mae: 2.4673\n",
            "Epoch 51/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.6642 - mae: 1.8714 - val_loss: 13.6204 - val_mae: 2.4897\n",
            "Epoch 52/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.6520 - mae: 1.8909 - val_loss: 13.2660 - val_mae: 2.4860\n",
            "Epoch 53/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.5621 - mae: 1.8453 - val_loss: 13.3361 - val_mae: 2.4591\n",
            "Epoch 54/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.5407 - mae: 1.8847 - val_loss: 13.4008 - val_mae: 2.4725\n",
            "Epoch 55/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.4135 - mae: 1.8362 - val_loss: 13.5732 - val_mae: 2.5670\n",
            "Epoch 56/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.2730 - mae: 1.8173 - val_loss: 13.4577 - val_mae: 2.4845\n",
            "Epoch 57/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.2639 - mae: 1.8339 - val_loss: 14.0963 - val_mae: 2.5191\n",
            "Epoch 58/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.3160 - mae: 1.8746 - val_loss: 13.7972 - val_mae: 2.4908\n",
            "Epoch 59/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2145 - mae: 1.8274 - val_loss: 13.4124 - val_mae: 2.4796\n",
            "Epoch 60/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.1678 - mae: 1.8465 - val_loss: 13.4887 - val_mae: 2.4950\n",
            "Epoch 61/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.0459 - mae: 1.7995 - val_loss: 13.4583 - val_mae: 2.4655\n",
            "Epoch 62/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.0742 - mae: 1.8170 - val_loss: 13.7366 - val_mae: 2.5711\n",
            "Epoch 63/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.9570 - mae: 1.8046 - val_loss: 13.7780 - val_mae: 2.5001\n",
            "Epoch 64/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 6.1388 - mae: 1.8330 - val_loss: 13.2986 - val_mae: 2.4834\n",
            "Epoch 65/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.7484 - mae: 1.7756 - val_loss: 13.7623 - val_mae: 2.5839\n",
            "Epoch 66/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9528 - mae: 1.7821 - val_loss: 13.3888 - val_mae: 2.5116\n",
            "Epoch 67/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9018 - mae: 1.8043 - val_loss: 13.4024 - val_mae: 2.5159\n",
            "Epoch 68/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.6414 - mae: 1.7770 - val_loss: 14.0808 - val_mae: 2.6641\n",
            "Epoch 69/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.6845 - mae: 1.7993 - val_loss: 13.0046 - val_mae: 2.4249\n",
            "Epoch 70/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.6365 - mae: 1.7526 - val_loss: 12.9997 - val_mae: 2.4919\n",
            "Epoch 71/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.6791 - mae: 1.7346 - val_loss: 12.9700 - val_mae: 2.4713\n",
            "Epoch 72/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5671 - mae: 1.7711 - val_loss: 13.2875 - val_mae: 2.5228\n",
            "Epoch 73/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 5.4782 - mae: 1.7288 - val_loss: 13.2221 - val_mae: 2.4676\n",
            "Epoch 74/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4368 - mae: 1.7121 - val_loss: 13.1675 - val_mae: 2.5001\n",
            "Epoch 75/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4921 - mae: 1.7357 - val_loss: 13.7849 - val_mae: 2.4893\n",
            "Epoch 76/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4376 - mae: 1.7202 - val_loss: 13.1250 - val_mae: 2.4733\n",
            "Epoch 77/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3452 - mae: 1.7080 - val_loss: 13.3602 - val_mae: 2.4793\n",
            "Epoch 78/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.0937 - mae: 1.6501 - val_loss: 13.2483 - val_mae: 2.5250\n",
            "Epoch 79/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3109 - mae: 1.6794 - val_loss: 13.1158 - val_mae: 2.4833\n",
            "Epoch 80/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3101 - mae: 1.7108 - val_loss: 13.1697 - val_mae: 2.5017\n",
            "Epoch 81/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.1336 - mae: 1.6716 - val_loss: 13.5117 - val_mae: 2.5593\n",
            "Epoch 82/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.1434 - mae: 1.6526 - val_loss: 13.3829 - val_mae: 2.5057\n",
            "Epoch 83/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.1919 - mae: 1.6968 - val_loss: 13.4430 - val_mae: 2.4767\n",
            "Epoch 84/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.0875 - mae: 1.6384 - val_loss: 13.6657 - val_mae: 2.5389\n",
            "Epoch 85/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.9153 - mae: 1.6552 - val_loss: 13.3858 - val_mae: 2.4952\n",
            "Epoch 86/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 5.0366 - mae: 1.6584 - val_loss: 13.3606 - val_mae: 2.5133\n",
            "Epoch 87/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.6259 - mae: 1.6163 - val_loss: 14.0374 - val_mae: 2.5568\n",
            "Epoch 88/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.7826 - mae: 1.6536 - val_loss: 13.6258 - val_mae: 2.4928\n",
            "Epoch 89/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 4.8792 - mae: 1.6309 - val_loss: 12.9267 - val_mae: 2.4816\n",
            "Epoch 90/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.8829 - mae: 1.6208 - val_loss: 13.3455 - val_mae: 2.4710\n",
            "Epoch 91/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.9000 - mae: 1.6069 - val_loss: 13.1046 - val_mae: 2.4776\n",
            "Epoch 92/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.7443 - mae: 1.5953 - val_loss: 13.3731 - val_mae: 2.4916\n",
            "Epoch 93/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.7921 - mae: 1.6192 - val_loss: 12.8887 - val_mae: 2.4860\n",
            "Epoch 94/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.6046 - mae: 1.5889 - val_loss: 12.8161 - val_mae: 2.4496\n",
            "Epoch 95/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.6007 - mae: 1.6063 - val_loss: 13.1786 - val_mae: 2.4765\n",
            "Epoch 96/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.5270 - mae: 1.5646 - val_loss: 12.9102 - val_mae: 2.4820\n",
            "Epoch 97/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.6493 - mae: 1.5864 - val_loss: 13.3582 - val_mae: 2.4656\n",
            "Epoch 98/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.7152 - mae: 1.5881 - val_loss: 13.1168 - val_mae: 2.5372\n",
            "Epoch 99/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.5316 - mae: 1.5639 - val_loss: 13.1169 - val_mae: 2.4843\n",
            "Epoch 100/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.1636 - mae: 1.5217 - val_loss: 13.9910 - val_mae: 2.5108\n",
            "Epoch 101/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.4077 - mae: 1.5693 - val_loss: 13.8746 - val_mae: 2.5222\n",
            "Epoch 102/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.3939 - mae: 1.5406 - val_loss: 13.2703 - val_mae: 2.4555\n",
            "Epoch 103/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.3766 - mae: 1.5488 - val_loss: 12.9979 - val_mae: 2.4401\n",
            "Epoch 104/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.4650 - mae: 1.5851 - val_loss: 13.3204 - val_mae: 2.4558\n",
            "Epoch 105/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.1930 - mae: 1.5424 - val_loss: 13.2778 - val_mae: 2.4436\n",
            "Epoch 106/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.1895 - mae: 1.5282 - val_loss: 13.1302 - val_mae: 2.4525\n",
            "Epoch 107/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 4.0820 - mae: 1.4959 - val_loss: 12.9408 - val_mae: 2.4431\n",
            "Epoch 108/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.2958 - mae: 1.5172 - val_loss: 13.0314 - val_mae: 2.4915\n",
            "Epoch 109/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 4.0121 - mae: 1.4921 - val_loss: 13.3908 - val_mae: 2.5746\n",
            "Epoch 110/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 4.0592 - mae: 1.4981 - val_loss: 13.1152 - val_mae: 2.5487\n",
            "Epoch 111/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.0138 - mae: 1.5062 - val_loss: 13.6123 - val_mae: 2.5130\n",
            "Epoch 112/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.2579 - mae: 1.5623 - val_loss: 12.7527 - val_mae: 2.4855\n",
            "Epoch 113/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.0452 - mae: 1.5044 - val_loss: 12.7700 - val_mae: 2.4403\n",
            "Epoch 114/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 3.9449 - mae: 1.4648 - val_loss: 13.0102 - val_mae: 2.4745\n",
            "Epoch 115/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.7796 - mae: 1.4495 - val_loss: 13.1451 - val_mae: 2.5371\n",
            "Epoch 116/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.0997 - mae: 1.4994 - val_loss: 13.0008 - val_mae: 2.4477\n",
            "Epoch 117/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.8108 - mae: 1.4830 - val_loss: 12.8105 - val_mae: 2.4165\n",
            "Epoch 118/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.8497 - mae: 1.4627 - val_loss: 12.9320 - val_mae: 2.5201\n",
            "Epoch 119/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.7572 - mae: 1.4145 - val_loss: 13.2195 - val_mae: 2.5431\n",
            "Epoch 120/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.9184 - mae: 1.4771 - val_loss: 12.8605 - val_mae: 2.4641\n",
            "Epoch 121/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.7791 - mae: 1.4211 - val_loss: 12.8863 - val_mae: 2.4531\n",
            "Epoch 122/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 3.7721 - mae: 1.4745 - val_loss: 12.8634 - val_mae: 2.4695\n",
            "Epoch 123/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.6445 - mae: 1.4303 - val_loss: 13.2153 - val_mae: 2.5395\n",
            "Epoch 124/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.5387 - mae: 1.4067 - val_loss: 13.2833 - val_mae: 2.5513\n",
            "Epoch 125/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.6967 - mae: 1.4249 - val_loss: 12.9971 - val_mae: 2.4862\n",
            "Epoch 126/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.7901 - mae: 1.4489 - val_loss: 12.9906 - val_mae: 2.4583\n",
            "Epoch 127/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.5978 - mae: 1.4049 - val_loss: 13.2415 - val_mae: 2.4490\n",
            "Epoch 128/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 3.7157 - mae: 1.4124 - val_loss: 13.1595 - val_mae: 2.5352\n",
            "Epoch 129/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.6335 - mae: 1.4326 - val_loss: 13.1277 - val_mae: 2.4948\n",
            "Epoch 130/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.5871 - mae: 1.4219 - val_loss: 12.8615 - val_mae: 2.4493\n",
            "Epoch 131/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.5113 - mae: 1.3982 - val_loss: 13.1773 - val_mae: 2.4726\n",
            "Epoch 132/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.5972 - mae: 1.4108 - val_loss: 12.7203 - val_mae: 2.4702\n",
            "Epoch 133/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 3.5158 - mae: 1.3885 - val_loss: 12.4187 - val_mae: 2.4314\n",
            "Epoch 134/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.5067 - mae: 1.3888 - val_loss: 13.0058 - val_mae: 2.4144\n",
            "Epoch 135/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 3.5841 - mae: 1.4202 - val_loss: 12.5606 - val_mae: 2.4358\n",
            "Epoch 136/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.3341 - mae: 1.3518 - val_loss: 12.8315 - val_mae: 2.3945\n",
            "Epoch 137/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.3009 - mae: 1.3593 - val_loss: 12.8079 - val_mae: 2.4613\n",
            "Epoch 138/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.4899 - mae: 1.3625 - val_loss: 12.7522 - val_mae: 2.4413\n",
            "Epoch 139/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.3961 - mae: 1.3513 - val_loss: 12.7832 - val_mae: 2.4783\n",
            "Epoch 140/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.2205 - mae: 1.3319 - val_loss: 12.6206 - val_mae: 2.4129\n",
            "Epoch 141/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.4028 - mae: 1.3497 - val_loss: 12.8501 - val_mae: 2.4451\n",
            "Epoch 142/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.1765 - mae: 1.3322 - val_loss: 13.2335 - val_mae: 2.4916\n",
            "Epoch 143/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.2357 - mae: 1.3257 - val_loss: 13.3412 - val_mae: 2.5598\n",
            "Epoch 144/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.3762 - mae: 1.3495 - val_loss: 12.6788 - val_mae: 2.4281\n",
            "Epoch 145/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 3.2235 - mae: 1.3245 - val_loss: 13.2869 - val_mae: 2.4693\n",
            "Epoch 146/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 3.0549 - mae: 1.3140 - val_loss: 12.8042 - val_mae: 2.4422\n",
            "Epoch 147/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.2094 - mae: 1.3639 - val_loss: 13.3404 - val_mae: 2.4793\n",
            "Epoch 148/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.1685 - mae: 1.3200 - val_loss: 12.6335 - val_mae: 2.4248\n",
            "Epoch 149/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.1776 - mae: 1.3002 - val_loss: 12.6148 - val_mae: 2.3955\n",
            "Epoch 150/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 3.0738 - mae: 1.2876 - val_loss: 13.8889 - val_mae: 2.5020\n",
            "Epoch 151/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.2480 - mae: 1.3420 - val_loss: 12.5494 - val_mae: 2.4553\n",
            "Epoch 152/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 2.9856 - mae: 1.3004 - val_loss: 13.3696 - val_mae: 2.4856\n",
            "Epoch 153/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.0352 - mae: 1.2735 - val_loss: 12.2804 - val_mae: 2.4364\n",
            "Epoch 154/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.9808 - mae: 1.3053 - val_loss: 13.4306 - val_mae: 2.5294\n",
            "Epoch 155/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.1391 - mae: 1.2923 - val_loss: 12.5353 - val_mae: 2.4097\n",
            "Epoch 156/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 3.0432 - mae: 1.3123 - val_loss: 13.0449 - val_mae: 2.4716\n",
            "Epoch 157/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 2.9095 - mae: 1.2570 - val_loss: 13.0968 - val_mae: 2.5149\n",
            "Epoch 158/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.9069 - mae: 1.2875 - val_loss: 12.4978 - val_mae: 2.4587\n",
            "Epoch 159/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.9796 - mae: 1.2892 - val_loss: 12.6296 - val_mae: 2.4640\n",
            "Epoch 160/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.8941 - mae: 1.2461 - val_loss: 12.8874 - val_mae: 2.4266\n",
            "Epoch 161/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.7914 - mae: 1.2429 - val_loss: 13.2901 - val_mae: 2.5536\n",
            "Epoch 162/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.9046 - mae: 1.2503 - val_loss: 12.3932 - val_mae: 2.4005\n",
            "Epoch 163/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.9778 - mae: 1.2838 - val_loss: 12.3993 - val_mae: 2.4191\n",
            "Epoch 164/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.8815 - mae: 1.2600 - val_loss: 12.1642 - val_mae: 2.3940\n",
            "Epoch 165/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 2.8549 - mae: 1.2444 - val_loss: 12.5653 - val_mae: 2.4704\n",
            "Epoch 166/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.9001 - mae: 1.2799 - val_loss: 13.1757 - val_mae: 2.4606\n",
            "Epoch 167/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.7623 - mae: 1.2276 - val_loss: 12.9792 - val_mae: 2.4699\n",
            "Epoch 168/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.6951 - mae: 1.2279 - val_loss: 12.6066 - val_mae: 2.4659\n",
            "Epoch 169/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.7919 - mae: 1.2829 - val_loss: 12.4538 - val_mae: 2.4799\n",
            "Epoch 170/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.7410 - mae: 1.2610 - val_loss: 12.8505 - val_mae: 2.4690\n",
            "Epoch 171/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.6567 - mae: 1.2194 - val_loss: 12.7153 - val_mae: 2.4584\n",
            "Epoch 172/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.6751 - mae: 1.2293 - val_loss: 12.5101 - val_mae: 2.4275\n",
            "Epoch 173/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.6168 - mae: 1.1952 - val_loss: 12.1952 - val_mae: 2.4266\n",
            "Epoch 174/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.7257 - mae: 1.2375 - val_loss: 12.2584 - val_mae: 2.4322\n",
            "Epoch 175/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.5998 - mae: 1.2111 - val_loss: 12.4328 - val_mae: 2.4210\n",
            "Epoch 176/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 2.7035 - mae: 1.2218 - val_loss: 12.8367 - val_mae: 2.4438\n",
            "Epoch 177/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.6385 - mae: 1.2203 - val_loss: 13.1273 - val_mae: 2.4632\n",
            "Epoch 178/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.6424 - mae: 1.1953 - val_loss: 13.0996 - val_mae: 2.5268\n",
            "Epoch 179/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.5104 - mae: 1.1801 - val_loss: 12.6443 - val_mae: 2.4350\n",
            "Epoch 180/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.6218 - mae: 1.1898 - val_loss: 12.3289 - val_mae: 2.4263\n",
            "Epoch 181/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.4773 - mae: 1.1755 - val_loss: 12.9215 - val_mae: 2.4575\n",
            "Epoch 182/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 2.6414 - mae: 1.1887 - val_loss: 12.3102 - val_mae: 2.4338\n",
            "Epoch 183/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 2.4886 - mae: 1.1790 - val_loss: 12.1831 - val_mae: 2.4228\n",
            "Epoch 184/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 2.5178 - mae: 1.1833 - val_loss: 12.9340 - val_mae: 2.4632\n",
            "3번째 폴드 처리중\n",
            "Epoch 1/500\n",
            "19/19 [==============================] - 1s 10ms/step - loss: 483.9049 - mae: 20.1454 - val_loss: 539.9501 - val_mae: 21.0924\n",
            "Epoch 2/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 363.2772 - mae: 16.9499 - val_loss: 397.6405 - val_mae: 17.6416\n",
            "Epoch 3/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 239.2260 - mae: 13.2262 - val_loss: 245.4473 - val_mae: 13.1764\n",
            "Epoch 4/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 127.0717 - mae: 8.9963 - val_loss: 130.4545 - val_mae: 8.4978\n",
            "Epoch 5/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 64.6033 - mae: 5.8886 - val_loss: 76.9365 - val_mae: 6.1590\n",
            "Epoch 6/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 42.0702 - mae: 4.6089 - val_loss: 56.6695 - val_mae: 5.2015\n",
            "Epoch 7/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 32.9139 - mae: 4.0139 - val_loss: 46.1352 - val_mae: 4.5476\n",
            "Epoch 8/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 27.1477 - mae: 3.5543 - val_loss: 36.9953 - val_mae: 4.0768\n",
            "Epoch 9/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 23.8596 - mae: 3.3641 - val_loss: 34.2487 - val_mae: 3.9031\n",
            "Epoch 10/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 21.6617 - mae: 3.1574 - val_loss: 31.5826 - val_mae: 3.6184\n",
            "Epoch 11/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 19.9245 - mae: 3.0563 - val_loss: 28.7870 - val_mae: 3.5360\n",
            "Epoch 12/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 18.2987 - mae: 2.9417 - val_loss: 27.0165 - val_mae: 3.4625\n",
            "Epoch 13/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 17.0219 - mae: 2.8474 - val_loss: 25.2389 - val_mae: 3.3676\n",
            "Epoch 14/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 15.9963 - mae: 2.7633 - val_loss: 23.6346 - val_mae: 3.2950\n",
            "Epoch 15/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 14.9340 - mae: 2.6408 - val_loss: 21.4752 - val_mae: 3.0597\n",
            "Epoch 16/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 14.1393 - mae: 2.5941 - val_loss: 21.0765 - val_mae: 3.1040\n",
            "Epoch 17/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 13.4078 - mae: 2.5468 - val_loss: 20.5921 - val_mae: 3.0482\n",
            "Epoch 18/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 12.4621 - mae: 2.4875 - val_loss: 21.0063 - val_mae: 3.0993\n",
            "Epoch 19/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 12.3253 - mae: 2.4622 - val_loss: 17.9350 - val_mae: 2.8485\n",
            "Epoch 20/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 11.8629 - mae: 2.4160 - val_loss: 17.8646 - val_mae: 2.8402\n",
            "Epoch 21/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 11.4177 - mae: 2.3721 - val_loss: 17.5154 - val_mae: 2.8255\n",
            "Epoch 22/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 11.1934 - mae: 2.3812 - val_loss: 16.7676 - val_mae: 2.7502\n",
            "Epoch 23/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.8032 - mae: 2.3232 - val_loss: 17.2204 - val_mae: 2.8064\n",
            "Epoch 24/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 10.7542 - mae: 2.3314 - val_loss: 16.3119 - val_mae: 2.7568\n",
            "Epoch 25/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 10.2719 - mae: 2.2598 - val_loss: 15.5524 - val_mae: 2.6955\n",
            "Epoch 26/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.9506 - mae: 2.2551 - val_loss: 16.1874 - val_mae: 2.7565\n",
            "Epoch 27/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.8641 - mae: 2.2527 - val_loss: 15.8129 - val_mae: 2.7478\n",
            "Epoch 28/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 9.6183 - mae: 2.2026 - val_loss: 14.6086 - val_mae: 2.6815\n",
            "Epoch 29/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.5255 - mae: 2.1854 - val_loss: 14.7358 - val_mae: 2.6465\n",
            "Epoch 30/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 9.4589 - mae: 2.1230 - val_loss: 14.3436 - val_mae: 2.7024\n",
            "Epoch 31/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.1508 - mae: 2.1464 - val_loss: 14.3326 - val_mae: 2.6490\n",
            "Epoch 32/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.8860 - mae: 2.1281 - val_loss: 14.3923 - val_mae: 2.6492\n",
            "Epoch 33/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 9.0242 - mae: 2.1148 - val_loss: 14.0719 - val_mae: 2.6483\n",
            "Epoch 34/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.6561 - mae: 2.0953 - val_loss: 13.9547 - val_mae: 2.6313\n",
            "Epoch 35/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 8.7292 - mae: 2.1217 - val_loss: 14.3129 - val_mae: 2.6754\n",
            "Epoch 36/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.6773 - mae: 2.0748 - val_loss: 13.7935 - val_mae: 2.6512\n",
            "Epoch 37/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.4733 - mae: 2.0577 - val_loss: 13.9244 - val_mae: 2.6590\n",
            "Epoch 38/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.0829 - mae: 1.9971 - val_loss: 14.1451 - val_mae: 2.7427\n",
            "Epoch 39/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.3493 - mae: 2.0564 - val_loss: 13.7546 - val_mae: 2.6597\n",
            "Epoch 40/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.2371 - mae: 2.0615 - val_loss: 13.5130 - val_mae: 2.5910\n",
            "Epoch 41/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 8.1709 - mae: 2.0308 - val_loss: 13.2062 - val_mae: 2.5665\n",
            "Epoch 42/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 8.0521 - mae: 1.9861 - val_loss: 13.3977 - val_mae: 2.6413\n",
            "Epoch 43/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.9994 - mae: 1.9968 - val_loss: 13.3120 - val_mae: 2.6418\n",
            "Epoch 44/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 7.7826 - mae: 1.9537 - val_loss: 13.2919 - val_mae: 2.6218\n",
            "Epoch 45/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.6901 - mae: 1.9750 - val_loss: 13.2166 - val_mae: 2.6365\n",
            "Epoch 46/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.7143 - mae: 1.9568 - val_loss: 12.7463 - val_mae: 2.5266\n",
            "Epoch 47/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.8925 - mae: 1.9854 - val_loss: 12.6827 - val_mae: 2.5390\n",
            "Epoch 48/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.4898 - mae: 1.9349 - val_loss: 13.0999 - val_mae: 2.5592\n",
            "Epoch 49/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 7.5851 - mae: 1.9337 - val_loss: 13.6322 - val_mae: 2.7098\n",
            "Epoch 50/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.3513 - mae: 1.8830 - val_loss: 13.6554 - val_mae: 2.6576\n",
            "Epoch 51/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.3668 - mae: 1.9231 - val_loss: 12.6898 - val_mae: 2.5302\n",
            "Epoch 52/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 7.1369 - mae: 1.8824 - val_loss: 12.6101 - val_mae: 2.5349\n",
            "Epoch 53/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1842 - mae: 1.8833 - val_loss: 12.5508 - val_mae: 2.5184\n",
            "Epoch 54/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 7.0959 - mae: 1.8619 - val_loss: 12.8417 - val_mae: 2.6065\n",
            "Epoch 55/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.1279 - mae: 1.8735 - val_loss: 12.5092 - val_mae: 2.5097\n",
            "Epoch 56/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.9998 - mae: 1.8309 - val_loss: 12.7124 - val_mae: 2.5300\n",
            "Epoch 57/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.2018 - mae: 1.8784 - val_loss: 12.5095 - val_mae: 2.5219\n",
            "Epoch 58/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 7.0599 - mae: 1.8784 - val_loss: 12.2588 - val_mae: 2.4715\n",
            "Epoch 59/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.9534 - mae: 1.8778 - val_loss: 12.6978 - val_mae: 2.5507\n",
            "Epoch 60/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.9254 - mae: 1.8413 - val_loss: 12.5405 - val_mae: 2.5290\n",
            "Epoch 61/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.8965 - mae: 1.8431 - val_loss: 12.3125 - val_mae: 2.5009\n",
            "Epoch 62/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.8503 - mae: 1.8543 - val_loss: 12.3112 - val_mae: 2.4867\n",
            "Epoch 63/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.6782 - mae: 1.8079 - val_loss: 12.8162 - val_mae: 2.6336\n",
            "Epoch 64/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.5962 - mae: 1.7878 - val_loss: 12.6908 - val_mae: 2.5642\n",
            "Epoch 65/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.8629 - mae: 1.8367 - val_loss: 12.2208 - val_mae: 2.5073\n",
            "Epoch 66/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.5542 - mae: 1.7964 - val_loss: 11.9113 - val_mae: 2.4507\n",
            "Epoch 67/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.5042 - mae: 1.7979 - val_loss: 12.4579 - val_mae: 2.5333\n",
            "Epoch 68/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2734 - mae: 1.7465 - val_loss: 12.8258 - val_mae: 2.5935\n",
            "Epoch 69/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.4227 - mae: 1.7863 - val_loss: 12.2203 - val_mae: 2.5165\n",
            "Epoch 70/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.5070 - mae: 1.7911 - val_loss: 12.4946 - val_mae: 2.5429\n",
            "Epoch 71/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.4502 - mae: 1.7815 - val_loss: 12.2580 - val_mae: 2.5148\n",
            "Epoch 72/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2168 - mae: 1.7201 - val_loss: 13.2563 - val_mae: 2.7123\n",
            "Epoch 73/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2195 - mae: 1.7500 - val_loss: 12.4001 - val_mae: 2.5320\n",
            "Epoch 74/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2623 - mae: 1.7588 - val_loss: 12.5212 - val_mae: 2.5780\n",
            "Epoch 75/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.2714 - mae: 1.7497 - val_loss: 12.2741 - val_mae: 2.5120\n",
            "Epoch 76/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.0718 - mae: 1.7305 - val_loss: 11.9107 - val_mae: 2.4667\n",
            "Epoch 77/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.3046 - mae: 1.7256 - val_loss: 11.8886 - val_mae: 2.4489\n",
            "Epoch 78/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.1241 - mae: 1.7488 - val_loss: 11.9895 - val_mae: 2.4588\n",
            "Epoch 79/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9244 - mae: 1.6885 - val_loss: 12.6856 - val_mae: 2.5227\n",
            "Epoch 80/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 6.1327 - mae: 1.7412 - val_loss: 12.1081 - val_mae: 2.4832\n",
            "Epoch 81/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.0472 - mae: 1.6929 - val_loss: 12.7379 - val_mae: 2.5971\n",
            "Epoch 82/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9488 - mae: 1.6960 - val_loss: 12.0432 - val_mae: 2.4640\n",
            "Epoch 83/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9442 - mae: 1.7072 - val_loss: 12.1518 - val_mae: 2.4951\n",
            "Epoch 84/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.9223 - mae: 1.6812 - val_loss: 11.8764 - val_mae: 2.4516\n",
            "Epoch 85/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 6.0336 - mae: 1.6997 - val_loss: 12.2041 - val_mae: 2.5418\n",
            "Epoch 86/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.7955 - mae: 1.6779 - val_loss: 11.9807 - val_mae: 2.4896\n",
            "Epoch 87/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5139 - mae: 1.6452 - val_loss: 11.9705 - val_mae: 2.4458\n",
            "Epoch 88/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5977 - mae: 1.6887 - val_loss: 12.7137 - val_mae: 2.5325\n",
            "Epoch 89/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.7152 - mae: 1.6756 - val_loss: 12.0531 - val_mae: 2.4601\n",
            "Epoch 90/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.6984 - mae: 1.6327 - val_loss: 11.9928 - val_mae: 2.4785\n",
            "Epoch 91/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5558 - mae: 1.6433 - val_loss: 11.9829 - val_mae: 2.4549\n",
            "Epoch 92/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4955 - mae: 1.6735 - val_loss: 11.9711 - val_mae: 2.4678\n",
            "Epoch 93/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5091 - mae: 1.6275 - val_loss: 11.9994 - val_mae: 2.5117\n",
            "Epoch 94/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.5999 - mae: 1.6557 - val_loss: 11.9048 - val_mae: 2.4783\n",
            "Epoch 95/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.7258 - mae: 1.6644 - val_loss: 11.7381 - val_mae: 2.4298\n",
            "Epoch 96/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4166 - mae: 1.6318 - val_loss: 11.8738 - val_mae: 2.4436\n",
            "Epoch 97/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3507 - mae: 1.5982 - val_loss: 12.1373 - val_mae: 2.5283\n",
            "Epoch 98/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.4031 - mae: 1.6376 - val_loss: 12.2732 - val_mae: 2.5645\n",
            "Epoch 99/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3095 - mae: 1.6107 - val_loss: 12.0506 - val_mae: 2.4960\n",
            "Epoch 100/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3738 - mae: 1.5933 - val_loss: 11.8852 - val_mae: 2.4645\n",
            "Epoch 101/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.2184 - mae: 1.6081 - val_loss: 11.9005 - val_mae: 2.4387\n",
            "Epoch 102/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 5.3387 - mae: 1.5984 - val_loss: 11.7900 - val_mae: 2.4506\n",
            "Epoch 103/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.3442 - mae: 1.6025 - val_loss: 11.8468 - val_mae: 2.4322\n",
            "Epoch 104/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.1482 - mae: 1.5679 - val_loss: 12.5160 - val_mae: 2.6079\n",
            "Epoch 105/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.2895 - mae: 1.5758 - val_loss: 11.8439 - val_mae: 2.4552\n",
            "Epoch 106/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.0460 - mae: 1.5706 - val_loss: 12.0737 - val_mae: 2.4786\n",
            "Epoch 107/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.1943 - mae: 1.5758 - val_loss: 11.8023 - val_mae: 2.4341\n",
            "Epoch 108/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.0743 - mae: 1.5601 - val_loss: 11.9780 - val_mae: 2.4472\n",
            "Epoch 109/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.9515 - mae: 1.5784 - val_loss: 11.6226 - val_mae: 2.4170\n",
            "Epoch 110/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 5.1060 - mae: 1.5663 - val_loss: 12.2109 - val_mae: 2.4536\n",
            "Epoch 111/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 5.0381 - mae: 1.5308 - val_loss: 11.7263 - val_mae: 2.4406\n",
            "Epoch 112/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.9015 - mae: 1.5350 - val_loss: 11.6144 - val_mae: 2.4130\n",
            "Epoch 113/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.9438 - mae: 1.5406 - val_loss: 12.0966 - val_mae: 2.5139\n",
            "Epoch 114/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 5.0138 - mae: 1.5707 - val_loss: 11.6693 - val_mae: 2.4255\n",
            "Epoch 115/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.9173 - mae: 1.5566 - val_loss: 11.6646 - val_mae: 2.4323\n",
            "Epoch 116/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 4.8524 - mae: 1.5244 - val_loss: 11.4897 - val_mae: 2.4172\n",
            "Epoch 117/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.7517 - mae: 1.5271 - val_loss: 11.5153 - val_mae: 2.4043\n",
            "Epoch 118/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.6323 - mae: 1.5071 - val_loss: 12.8232 - val_mae: 2.5295\n",
            "Epoch 119/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.9872 - mae: 1.5906 - val_loss: 11.5601 - val_mae: 2.4044\n",
            "Epoch 120/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.7691 - mae: 1.5233 - val_loss: 11.3151 - val_mae: 2.3916\n",
            "Epoch 121/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 4.6937 - mae: 1.5013 - val_loss: 11.5573 - val_mae: 2.4291\n",
            "Epoch 122/500\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 4.6656 - mae: 1.5262 - val_loss: 11.5850 - val_mae: 2.4277\n",
            "Epoch 123/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.5744 - mae: 1.5008 - val_loss: 11.6070 - val_mae: 2.4238\n",
            "Epoch 124/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.8317 - mae: 1.4895 - val_loss: 11.5474 - val_mae: 2.4206\n",
            "Epoch 125/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 4.4605 - mae: 1.4679 - val_loss: 11.5676 - val_mae: 2.4011\n",
            "Epoch 126/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.6943 - mae: 1.4760 - val_loss: 11.5828 - val_mae: 2.4153\n",
            "Epoch 127/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.6145 - mae: 1.4792 - val_loss: 11.3724 - val_mae: 2.3765\n",
            "Epoch 128/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.5461 - mae: 1.4993 - val_loss: 11.9356 - val_mae: 2.5023\n",
            "Epoch 129/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.4427 - mae: 1.4590 - val_loss: 11.4317 - val_mae: 2.3885\n",
            "Epoch 130/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.3684 - mae: 1.4910 - val_loss: 11.3888 - val_mae: 2.4053\n",
            "Epoch 131/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.3916 - mae: 1.4628 - val_loss: 11.5285 - val_mae: 2.4057\n",
            "Epoch 132/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.4283 - mae: 1.4363 - val_loss: 12.0962 - val_mae: 2.4848\n",
            "Epoch 133/500\n",
            "19/19 [==============================] - 0s 5ms/step - loss: 4.4668 - mae: 1.4874 - val_loss: 11.8419 - val_mae: 2.4748\n",
            "Epoch 134/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.4507 - mae: 1.4573 - val_loss: 11.6568 - val_mae: 2.4255\n",
            "Epoch 135/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.3037 - mae: 1.4179 - val_loss: 13.2213 - val_mae: 2.6669\n",
            "Epoch 136/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.4191 - mae: 1.4825 - val_loss: 11.8151 - val_mae: 2.4670\n",
            "Epoch 137/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.0629 - mae: 1.3937 - val_loss: 12.5687 - val_mae: 2.4767\n",
            "Epoch 138/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.5005 - mae: 1.5115 - val_loss: 11.6804 - val_mae: 2.4260\n",
            "Epoch 139/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.2088 - mae: 1.4096 - val_loss: 11.6483 - val_mae: 2.4219\n",
            "Epoch 140/500\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 4.2648 - mae: 1.4448 - val_loss: 11.3851 - val_mae: 2.3815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련들이 조기종료되는 것을 확인했다. 이 모델로 test data를 최종 훈련해보자"
      ],
      "metadata": {
        "id": "MYcFlx_4jH06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_x, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCdDLxQ_jGjh",
        "outputId": "1c13107a-be28-46a8-e4ac-b6f377c605b8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 20.6473 - mae: 2.7961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20.64731216430664, 2.796088695526123]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 책대로 epochs=130을 주고 훈련시킨 모델로 실행한 evaluate에서는\n",
        "```\n",
        "[15.926226615905762, 2.63193678855896]\n",
        "```\n",
        "의 결과가 나왔었다. 조기 종료를 시킨 모델로 실행했을 때는\n",
        "```\n",
        "[15.046340942382812, 2.751631736755371]\n",
        "```\n",
        "의 값이 나왔으니 대략 비슷하게 뽑힌것을 알 수 있다. patience 값을 적절히 조절하는 것이 중요할 것 같아"
      ],
      "metadata": {
        "id": "MEKOQMdnjJzK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JkQTTHrdjI7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}