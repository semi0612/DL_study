### ReLU 함수
`y = x` 인 선형함수가 입력값 0이하에서부터 rectified(정류)된 함수

- 딥러닝 분야에서 가장 많이 사용되는 활성화 함수
- Sigmoid, tanh 함수의 Vanashing Gradient 문제 해결
- 입력값이 음수일 경우 출력값과 미분값을 모두 0으로 강제하므로 죽은 뉴련을 회생하는데에는 어려움이 존재한다.
- 구현이 단순하고 연산이 필요없이 임계값(양수/음수 여부)만 활용하므로 연산속도가 빠르다

### Tanh 함수
하이퍼볼릭 탄젠트(Hyperbolic Tangent) 함수라 부르고 tanh로 표기한다.
- 모든 입력값에 대해 출력값이 실수값으로 정의(=Soft Decision)
- 값이 작아질수록 -1, 커질수록 1에 수렴
- 입력값이 0에 가까울수록 미분이 크기 때문에 출력값이 빠르게 변한다.

### Sigmoid 함수
본래 Sigmoid 함수라하면 S자 모양의 함수를 모두 가리키는데, 딥러닝 분야에서는 일반적으로 아래와 같은 특징을 가지는 함수로 통용된다.
- 모든 입력값에 대해 출력값이 실수값으로 정의(=Soft Decision)
- 값이 작아질수록 0, 커질수록 1에 수렴
- 출력이 0~1 사이로 확률 표현이 가능하다(=Binary Classification)

### ※ Vanashing Gradient : 기울기 소실
딥러닝 분야에서 Layer를 많이 쌓을 수록 데이터 표현력이 증가하기 때문에 무조건 학습이 잘 될것만 같지만, 실제로는 Layer가 많아질수록 학습이 잘 되지 않는다. 이는 기울기 소실 현상 때문인데, 역전파(Backpropagation) 과정에서 출력층에서 멀어질 수록 기울기(Gradient) 값이 매우 작아지는 현상을 말한다.
- Sigmoid 함수
시그모이드의 경우 미분값은 입력값이 0일 때 가장 크지만 0.25에 불과하고, x의 값이 크거나 작아짐에 따라 기울기는 거의 0에 수렴한다. 따라서 역전파 과정에서 Sigmoid 함수의 미분값이 거듭 곱해지며 출력층과 멀어질수록 기울기 값은 매우 작아질 수 밖에 없는 것이다. 계산시 정확한 값이 아니라 근사값으로 계산되기 때문에 역전파 과정에서 점차 학습 오차까지 증가하며 모델 학습이 제대로 이루어지지 않을 수 있다.

![image](https://user-images.githubusercontent.com/51469989/212632155-76039611-35b2-4089-b5d0-f4ab910eac6d.png)

- tanh 함수
역전파 과정에서 Sigmoid 함수가 층이 많아질 수록 효과적으로 학습되지 않는 한계점을 개선하기 위한 방법 중 하나로 제안된 함수. 0을 기준으로 출력값이 최대 1, 최소 -1 사이의 값을 가지도록 출력값의 범위를 2배 늘렸다. 이를 통해 미분값 자체는 Sigmoid 함수에 비해 훨씬 커졌지만 그럼에도 x값이 크거나 작아짐에 따라 기울기 크기가 크게 작아지기 때문에 기울기 소실문제를 아예 방지하는 데는 어려움이 있다.

![image](https://user-images.githubusercontent.com/51469989/212632228-0f2ecc5f-7cbd-47ec-95bb-e6c58ffb88a7.png)

- ReLU 함수
입력값이 양수일 경우, 입력값에 상관없이 항상 미분값은 동일하게 1이 출력된다. 즉 역전파 과정에서 기울기가 소실되는 문제를 해결할 수 있게 된것이다. 더불어 Sigmoid, tanh 함수처럼 특별한 연산없이 단순히 임계값(0)에 따라 출력값이 결정되므로 연산 속도가 빠르다는 특징이 생겼다.
하지만 입력값이 음수일 경우 미분값은 항상 0이기 때문에 입력값이 음수인 뉴런은 다시 회생시키기 어렵다는 한계가 존재한다. 이를 'Dying ReLU(죽어가는 relu)' 라 부른다.

![image](https://user-images.githubusercontent.com/51469989/212632301-ca453e4a-383a-481d-8263-224a6ffba29d.png)


- Leaky ReLU 함수
이 죽어가는 relu현상을 보완하기 위해 다양하게 변형된 ReLU 함수가 제안되었고, 그중에서 Leaky ReLU 함수는 입력값이 음수일 때 출력값을 0이 아닌 0.001과 같은 매우 작은 값을 출력하도록 설정하게 된다. 이처럼 음수는 0이 아닌 값이기 때문에 입력값이 음수라도 기울기가 0이 되지 않아 뉴련이 죽는 현상을 방지 할 수 있다.

## 역전파(=오차 역전파; backpropagation)
- 딥러닝의 핵심 알고리즘
- 순전파 알고리즘에서 발생한 오차를 줄이기 위해 새로운 가중치를 업데이트하고, 새로운 가중치로 다시 학습하는 과정을 역전파 알고리즘이라한다. 이러한 역전파 학습을 오차가 0에 가까워질 때까지 반복하는데, 역전파 알고리즘을 실행시 가중치를 결정하는 방법에서는 경사 하강법이 이용된다.
<br><br>
[순전파]<br>
연산이 입력층에서 출력층 방향(앞)으로 이동하는 신경망을 `순방향 신경망(Feed Forward Neural Network, FFNN` 이라 하고, 이처럼 연산이 앞으로 이동하는 과정을 순전파(Feed Forward)라한다. <br>
이러한 순방향 신경망은 입력층에 들어온 입력값을 순전파하여 예측값을 도출하고, 타깃과의 오차를 계산한 뒤 역전파 알고리즘을 이용하여 가중치를 업데이트 하는 과정을 반복하며 학습한다.
<br><br>
[역전파]<br>
이름에서 유추할 수 있듯 출력층에서 입력층 방향으로 오차를 전파시키며 각 층의 가중치를 업데이트 하는 것이다.<br><br>
`<Deep Learning with Python>`<br>
역전파는 최종 손실 값에서 시작하여, 아래층에서 맨 위층까지 거꾸러 거슬러 올라가며 각 파라미터가 손실 값에 기여한 정도를 계산합니다. 그래서 이름이 '역전파' 입니다. 즉 계산 그래프에서 각 노드의 손실 기여도를 역전파합니다.
