{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d3b62d-7004-4513-b9d3-2832f2445800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydataset as py\n",
    "\n",
    "mpg = py.data('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079cf619-f28f-4e8c-a28e-c26cdbb4b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제조사 : audi, ford ,jeep, lincol 만 뽑아주세요\n",
    "\n",
    "li = ['audi','ford','jeep','lincoln']\n",
    "mpg_df_1 = mpg[mpg['manufacturer'].isin(li)]\n",
    "mpg_df_1\n",
    "\n",
    "\n",
    "mpg_df_2 = mpg.query(\"manufacturer in ['audi', 'ford', 'jeep', 'lincoln']\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c7de9b4-0d41-4058-9dd9-60c09567f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  manufacturer drv  cty\n",
      "0         audi   4   20\n",
      "1         audi   f   21\n",
      "2         ford   4   15\n",
      "3         ford   r   18\n",
      "4         jeep   4   17\n",
      "5      lincoln   r   12\n"
     ]
    }
   ],
   "source": [
    "# 제조사 별로 drv에 따른 cty의 최대값을 sn\n",
    "\n",
    "import seaborn as sns\n",
    "drv_cty = mpg_df_1.groupby(['manufacturer', 'drv'])['cty'].max()\n",
    "drv_cty = drv_cty.reset_index()\n",
    "print(drv_cty)\n",
    "#   manufacturer drv  cty\n",
    "# 0         audi   4   20\n",
    "# 1         audi   f   21\n",
    "# 2         ford   4   15\n",
    "# 3         ford   r   18\n",
    "# 4         jeep   4   17\n",
    "# 5      lincoln   r   12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff086e51-f436-45d0-96f2-35f56d953ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>drv</th>\n",
       "      <th>cty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audi</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audi</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ford</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ford</td>\n",
       "      <td>r</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jeep</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lincoln</td>\n",
       "      <td>r</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manufacturer drv  cty\n",
       "0         audi   4   20\n",
       "1         audi   f   21\n",
       "2         ford   4   15\n",
       "3         ford   r   18\n",
       "4         jeep   4   17\n",
       "5      lincoln   r   12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_df_1.groupby(['manufacturer', 'drv'], as_index=False)['cty'].max()\n",
    "drv_cty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d91c5f-dc50-421f-b060-a22456078384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db538113-4af8-4e20-a49d-710aa7cc1a12",
   "metadata": {},
   "source": [
    "문장 토큰화(sentence tokenization)은 문장의 마침표, 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적(492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "792fefc1-4a32-4065-a00d-0269c3abe940",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp310-cp310-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.7/267.7 kB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.9.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1cb8646-f625-474b-88c1-1e413af0efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 다운로드 받기\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "996128a1-e254-48dc-80b9-5d28bba83110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes']\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                You can see it out your window or on your television. \\\n",
    "                You feel it when you go to work, or go to church or pay your taxes'\n",
    "\n",
    "\n",
    "# 문장 토큰화를 진행할 text를 sent_tokenize안에 넣어준다.\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8f713-3973-4c0c-878f-fe209bf0af7c",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "문장을 단어로 토큰화 하는것. 기본적으로 공백, 콤마, 마침표, 개행문자 등으로 단얼르 분리하지만 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea41168d-bb0e-47be-9035-2eabe58b91f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 44\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.', 'You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.', 'You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(text_sample)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752dbbe1-4a0b-431e-8633-5ee3f481cc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed8690-fe54-4b14-8ad1-17c35f5f8022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b469b9-b3ba-46e3-b53a-bca8c3c9c9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382602cf-e41b-46d4-981b-52e5c7163180",
   "metadata": {},
   "source": [
    "word_tokenize, sent_tokenize를 조합하여 문서에 대해 모든 단어를 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7808174-3976-483f-abd8-3ec4d1541830",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Matrix',\n",
       "  'is',\n",
       "  'everywhere',\n",
       "  'its',\n",
       "  'all',\n",
       "  'around',\n",
       "  'us',\n",
       "  ',',\n",
       "  'here',\n",
       "  'even',\n",
       "  'in',\n",
       "  'this',\n",
       "  'room',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'can',\n",
       "  'see',\n",
       "  'it',\n",
       "  'out',\n",
       "  'your',\n",
       "  'window',\n",
       "  'or',\n",
       "  'on',\n",
       "  'your',\n",
       "  'television',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'feel',\n",
       "  'it',\n",
       "  'when',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  'work',\n",
       "  ',',\n",
       "  'or',\n",
       "  'go',\n",
       "  'to',\n",
       "  'church',\n",
       "  'or',\n",
       "  'pay',\n",
       "  'your',\n",
       "  'taxes']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 빈 리스트 생성\n",
    "li = []\n",
    "\n",
    "# text_sample을 문장별로 쪼개기\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "# for 문을 돌리면서 문장별로 단어 쪼개기\n",
    "for sentences in sentences :\n",
    "    li.append(word_tokenize(sentences))\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1d82960-3b47-4d1a-b058-227cabf76cd8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Matrix',\n",
       "  'is',\n",
       "  'everywhere',\n",
       "  'its',\n",
       "  'all',\n",
       "  'around',\n",
       "  'us',\n",
       "  ',',\n",
       "  'here',\n",
       "  'even',\n",
       "  'in',\n",
       "  'this',\n",
       "  'room',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'can',\n",
       "  'see',\n",
       "  'it',\n",
       "  'out',\n",
       "  'your',\n",
       "  'window',\n",
       "  'or',\n",
       "  'on',\n",
       "  'your',\n",
       "  'television',\n",
       "  '.'],\n",
       " ['You',\n",
       "  'feel',\n",
       "  'it',\n",
       "  'when',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  'work',\n",
       "  ',',\n",
       "  'or',\n",
       "  'go',\n",
       "  'to',\n",
       "  'church',\n",
       "  'or',\n",
       "  'pay',\n",
       "  'your',\n",
       "  'taxes']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 컴프리핸션으로\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "[word_tokenize(sentences) for sentences in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0bfcaa-7696-4ff8-b63d-32aa2412815a",
   "metadata": {},
   "source": [
    "3개의 문장을 문장별로 먼저 토큰화 했으므로 word_tokens 변수는 3개의 리스트 객체를 내포하는 리스트가 되었다. 그리고 내포된 개별 리스트 객체는 각각 문장별로 토큰화된 단어를 요소로 가지고 있다.\n",
    "\n",
    "문장을 단어별로 토큰화 할 경우 문맥적인 의미는 무시될 수 밖에 없다. 2의 단어가 뭉쳐서 하나의 의미를 나타내는 경우를 2-gram(bi-gram)이라 하는데 이러한 것이 무시될 수 있다는 건 큰 단점이 될 것이다.\n",
    "\n",
    "*\n",
    "1-gram : uni-gram\n",
    "2-gram : bi-gram\n",
    "3-gram : trigram\n",
    "...\n",
    "n-gram\n",
    "\n",
    "* corpus : 말뭉치, 분석의 기본 단위?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383b7a2-715e-4784-b33f-86ccc9c421d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1162df-706d-4020-b04b-91a09e6ec21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2b3ba4-930d-431d-814f-28f3d2be1c80",
   "metadata": {},
   "source": [
    "### Stop word\n",
    "분석에 큰 의미가 없는 단어를 지칭한다. 가령 영어에서 is, the, a, will등 문장을 구성하는 문법 요소는 맞지만 문맥적으로는 큰 의미가 없는 단어가 이에 해당한다.이 단어는 빈번하게 텍스트에 나타나므로 이것들을 사전에 제거하지 않으면 그 빈번함으로 인해 중요한 단어로 인지 될 수 있기 때문에 이전에 제거해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "404b6f5a-04a3-4623-bfa6-88f4b4af5a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.download('stopwords')\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5f2f029-a070-4f7b-a13f-a5ce5d4c9e17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'x',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  'w',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'r',\n",
       "  'u',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'v',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'r',\n",
       "  '.'],\n",
       " ['u',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'n',\n",
       "  'w',\n",
       "  ' ',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'l',\n",
       "  'e',\n",
       "  'v',\n",
       "  'n',\n",
       "  '.'],\n",
       " ['u',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'e',\n",
       "  'e',\n",
       "  'l',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  'e',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'g',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'r',\n",
       "  'k',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'g',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'h',\n",
       "  'u',\n",
       "  'r',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'p',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'x',\n",
       "  'e']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text=text_sample)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in sentences : \n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱워드를 제거하는 반복문\n",
    "    for word in sentence :\n",
    "        # 소문자로 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱 워드 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords :\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c937cab2-4e5d-4b9a-9f0d-dcec2a06bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes']]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text_sample)\n",
    "word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# 영어 stopword\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# 문장별 단어 토큰화 + stopword 제거\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    \n",
    "    # 문장 토큰의 각 단어 토큰\n",
    "    for word in sentence:\n",
    "        # 소문자 변환\n",
    "        word = word.lower()\n",
    "        # stopword 미포함\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "            \n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4db2ca-543e-461f-a69c-faeecf7298d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00bc4731-7a14-4dd5-8e5e-5e1a0d913d4e",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization\n",
    "Steam : 어간\n",
    "Stemming\n",
    "- 어간추출\n",
    "- 단어의 원형을 찾는데, 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근단어를 추출하는 경향이 있다.\n",
    "- plays -> play\n",
    "\n",
    "Lemmatization\n",
    "- 표제어 추출\n",
    "- 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아준다.\n",
    "- is, was -> be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a293fa7-0e40-43a4-876b-25db5c819ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play play play\n",
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('plays'), stemmer.stem('playing'), stemmer.stem('played'))\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6428a53-fb6f-44c1-a818-6a1d8f2e9520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amus amus amus\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('amuse'), stemmer.stem('amuses'), stemmer.stem('amused'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63dc7e62-4d83-4537-9980-4af4e4069928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happiest\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb97f60-828a-438a-aa91-a77e163e5bfe",
   "metadata": {},
   "source": [
    "정확하지 못한 변환(497)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629bdfa-b19d-4ad5-859c-a896ccb50fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17f1720a-7af8-4780-8b41-3e92cabdb573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "be be be\n",
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# 책에서는 'wordnet'이였지만 바뀐듯 하다\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 동사: v, 형용사: a\n",
    "print(lemma.lemmatize('plays', 'v'))\n",
    "print(lemma.lemmatize('is', 'v'), lemma.lemmatize('was', 'v'), lemma.lemmatize('being', 'v'))\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2099a11-940e-4477-93a1-6c1f7268f366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35e4535-72ed-4b1c-b928-926fe648216e",
   "metadata": {},
   "source": [
    "## Bag of Words - BOW\n",
    "문서가 가지는 모든 단어(words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여 한 뒤 피처 값을 추출하는 모델이다. 쉽고 빠른 구축이 가능하지만 단어의 순서를 고려하지 않으므로 문맥적인 의미 반영이 부족하고 희소 행렬(대부분의 값이 0)의 문제가 있다.\n",
    "\n",
    "### Count 벡터화\n",
    "- 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수\n",
    "- 단순하게 모든 단어의 빈도로 피처 벡터화를 진행한다.\n",
    "- CountVectorizer를 이용해 행은 각 문장(문서)는 \n",
    "- 카운트 벡터화에서는 카운트 값이 높을 수록 중요한 단어로 인식\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "그것이 문서의 특징을 잘 나타낼 수 있는 단어 뿐 아니라 언어의 특성상 문장에서 자주 사용될 수 밖에 없는 단어까지 높은 값을 부여하게 되는 것이 문제이다. 이러한 문제를 보완하기 위해 TF-IDF(Term Frequency Inverse Document Frequency) 벡터화를 사용한다.\n",
    "\n",
    "TF-IDF : 개별 문서에서 자주 나타내는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 페널티를 주는 방식으로 값을 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa2a48db-0af0-48ce-af08-833e378bec48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((5,5))\n",
    "a[4] = 1\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d78101e-fbc6-429a-8ebc-2bb6e01447b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 희소행렬 만들기\n",
    "data = np.array([3, 1, 2])\n",
    "row = np.array([0, 0, 1])\n",
    "col = np.array([0, 2, 1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 COO형식으로 희소 행렬 생성\n",
    "# coo_matrix((M, N), [dtype])\n",
    "sparse_coo = sparse.coo_matrix((data, (row, col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da9eb991-2ba9-4051-b542-001fffdaba00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "181e7fef-9fba-49ec-ad2d-d29120fdcca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coo_matrix(D)\n",
    "sparse.coo_matrix(sparse_coo.toarray()).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc4c0c6e-be11-4b90-befd-c359d9a2a437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 5],\n",
       "       [1, 4, 0, 3, 2, 5],\n",
       "       [0, 6, 0, 3, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 8],\n",
       "       [1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dense2를 위처럼 생성하세요\n",
    "# dense2 = np.array([[0,0,1,0,0,5],\n",
    "#                   [1,4,0,3,2,5],\n",
    "#                   [0,6,0,3,0,0],\n",
    "#                   [2,0,0,0,0,0],\n",
    "#                   [0,0,0,7,0,8],\n",
    "#                   [1,0,0,0,0,0]])\n",
    "\n",
    "# 0이 아닌 데이터\n",
    "data = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행과 열 위치를 각각 배열로 생성\n",
    "row = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# \n",
    "sparse.coo_matrix((data, (row, col))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b713a31-bb76-4741-bb16-35f59908379f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 5],\n",
       "       [1, 4, 0, 3, 2, 5],\n",
       "       [0, 6, 0, 3, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 8],\n",
       "       [1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행 위치 배열의 고유한 값에 시작 위치 인덱스를 배열로 생성\n",
    "row_index = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "sparse_csr = sparse.csr_matrix((data, col, row_index))\n",
    "sparse_csr.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6ca06-8851-481e-92be-e9682a104961",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac9d56-881b-40ca-a075-d73174a40898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfac14e-23e1-4904-9fab-c622d84a1a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5e6f5-1047-4699-a221-f37194fd3f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5956f5-a68b-4c75-a81c-288f033c1667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1e9a5-1168-4b0f-9bc1-e4dd272c9794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e11ff9-6bc9-42d3-9e82-0e5f56b3453b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IMDB 영화평\n",
    "\n",
    "# 데이터 로드\n",
    "import pandas as pd\n",
    "\n",
    "review_df = pd.read_csv('./labeledTrainData.tsv', header=0, sep='\\t', quoting=3)\n",
    "review_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff558199-c2bb-43ad-a218-c18d69574501",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df[\"review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e5cfb1-7d61-4604-b856-28422c463146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         With all this stuff going down at the moment ...\n",
       "1           The Classic War of the Worlds   by Timothy ...\n",
       "2         The film starts with a manager  Nicholas Bell...\n",
       "3         It must be assumed that those who praised thi...\n",
       "4         Superbly trashy and wondrously unpretentious ...\n",
       "                               ...                        \n",
       "24995     It seems like more consideration has gone int...\n",
       "24996     I don t believe they made this film  Complete...\n",
       "24997     Guy is a loser  Can t get girls  needs to bui...\n",
       "24998     This    minute documentary Bu uel made in the...\n",
       "24999     I saw this movie as a child and it broke my h...\n",
       "Name: review, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# <br> html 태그는 replace 함수를 사용해 공백으로 변환\n",
    "review_df['review'] = review_df['review'].str.replace('<br />', ' ')\n",
    "review_df['review'] = review_df['review'].apply(lambda x : re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "review_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad69b26-bc4c-4d83-9ccc-6430aa10f299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = y_target = review_df[\"sentiment\"]\n",
    "feature_df = X_feature = review_df.drop(['id', 'sentiment'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_feature, y_target, test_size=0.3, random_state=156)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "017011e0-0423-48af-8d0c-d4ec2d1446b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.8859, ROC-AUC: 0.9503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 피처 벡터화: CountVectorizer, ML: LogisticRegression\n",
    "pipeline = Pipeline([(\"cnt_vect\", CountVectorizer(stop_words=\"english\", ngram_range=(1,2) ) ),\n",
    "                     (\"LR\", LogisticRegression(solver='liblinear', C=10) )\n",
    "                    ])\n",
    "\n",
    "# 학습\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "# 예측\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "# 평가\n",
    "acc = accuracy_score(y_test, pred)\n",
    "auc = roc_auc_score(y_test, pred_probs)\n",
    "\n",
    "print(f\"예측 정확도: {acc:.4f}, ROC-AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33137a38-e4fa-4882-887b-2ae394dba8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.8936, ROC-AUC: 0.9598\n"
     ]
    }
   ],
   "source": [
    "# 피처 벡터화: TfidfVectorizer, ML: LogisticRegression\n",
    "pipeline = Pipeline([(\"tfidf_vect\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2) ) ),\n",
    "                     (\"LR\", LogisticRegression(C=10) )\n",
    "                    ])\n",
    "\n",
    "# 학습\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "# 예측\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "# 평가\n",
    "acc = accuracy_score(y_test, pred)\n",
    "auc = roc_auc_score(y_test, pred_probs)\n",
    "\n",
    "print(f\"예측 정확도: {acc:.4f}, ROC-AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8ff46-4d5b-4984-a419-af1a88cf88f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
