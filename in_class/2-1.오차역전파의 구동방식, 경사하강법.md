## 오차 역전파의 구동방식
구동방식
1. 임의의 초기 가중치(W)를 준 뒤 결과(Y; out)를 계산한다.
2. 계산 결과와 우리가 원하는 값 사이의 오차를 구한다
3. 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트 한다
4. 위 과정을 더이상 오차가 줄어들지 않을 때까지 반복한다.
<br><br>
오차가 작아지는 방향으로 업데이트한다 <- 는 말은 미분 값이 0에 가까워지는 방향으로 나아간다는 말이다. 즉, 기울기가 0이 되는 방향으로 가야한다는 말이고 이는 가중치에서 기울기를 뺐을 때 가중치의 변화가 전혀 없는 상태를 말하는 것이다.
<br><br>
역전파는 연쇄법칙을 계산 그래프에 적용한 것 뿐이다. 최종 손실 값에서 시작하여 아래층에서 맨 위층까지 거꾸러 거슬러 올라가 각 파라미터가 손실 값에 기여한 정도를 계산하기 때문에 이름이 역전파 인것이다. 즉, 계산 그래프에서 각 노드의 손실 기여도를 역전파합니다. 텐서플로는 자동 미분(automatic differenitiation)이 가능하기 때문에 정방향 패스를 작성하는 것 외에 다른 작업 없이 미분 가능한 텐서 연산의 어떤 조합에 대해서도 그레디언트를 계산할 수 있다.

## 경사 하강법(Gradient Descent)
- 기본적인 함수 최적화(optimization) 방법 중 하나 -최적화란 함수의 최대값 또는 최소값을 찾는 것
- 반복적으로 기울기를 변화시켜서 기울기가 0인 한점(m)의 값을 찾아내는 방법<br>
=오차의 변화에 따라 이차 함수 그래프를 만들고 적절한 학습률을 설정해 미분 값이 0인 지점을 구하는 것
- 그래프의 기울기 a를 너무 크게 작거나 작게 잡아도 오차는 커진다. 아래 그래프(사진)에서 오차가 가장 작은 점을 잡으려면 x가 그래프의 가장 아래쪽 볼록한 부분에 있을 때일 것이다. 즉, 기울기 a가 m의 위치에 있을 때인데. 컴퓨터를 이용해 m의 값을 구하려면 임의의 한 점(a1)을 찍고 이 점을 m에 가까운 쪽으로 점점 이동 시키는 과정이 필요하다. 이렇게 그래프에서 오차를 비교하며 가장 작은 방향으로 이동시키는 방법이 있는데 바로 이 미분 기울기를 이용하는 것이 경사하강법
![image](https://user-images.githubusercontent.com/51469989/212635243-1a941b65-0586-4e90-aa77-d4040164bb36.png)

## 고급 경사 하강법
가중치를 업데이트하는 방법인 경사하강법(GD)은 정확하게 가중치를 찾아가지만, 한 번 업데이트할 때마다 전체 데이터를 미분해야 하므로 계산량이 매우 많다는 단점이 있다. 이런 점을 보완하여 고급 경사 하강법들이 등장하고 있다.

<br>

아래 표는 고급 경사 하강법의 종류에 대해 서술하고 있다. 11/18일의 `Optimization` 과 적절히 함께 보며 이해해보자

<br>

|고급 경사 하강법|개요|효과|케라스 사용법|
|:---|:---|:---|:---|
|확률적 경사 하강법 (SGD)|랜덤하게 추출한 일부 데이터를 사용해<br><br> 더 빨리, 자주 업데이트를 하게 하는 것|속도 개선|keras.optimizers.SGD(lr = 0.1)<br><br>케라스 최적화 함수를 이용|
|모멘텀 (Monentum)|관성의 방향을 고려해 진동과 폭을 줄이는 효과|정확도 개선|keras.optimizers.SGD(lr = 0.1, momentum = 0.9)<br><br>모멘텀 계수를 추가한다.|
|네스테로프 모멘텀 (NAG)|모멘텀이 이동시킬 방향으로 미리 이동하여 그래디언트를 계산<br><br>불필요한 이동을 줄이는 효과|정확도 개선|keras.optimizer.SGD(lr = 0.1, momentum = 0.9, nesterov = True)|
|아다그라드(Adagrad)|변수의 업데이트가 잦으면 학습률을 적게 하여 이동 보폭을 조절하는 방법|보폭 크기 개선|keras.optimizers.Adagrad(lr = 0.01, epsilon = 1e - 6)<br><br>아다그라드 함수를 사용한다.<br><br>※ 참고 : 여기서 epsion, rho, decay 같은 파라미터를 바꾸지 않고 그대로 사용하기를 권장<br><br>따라서 lr 즉, learning rate(학습률) 값만 적절히 조절하면 된다.
|알엠에스프롭(RMSProp)|아다그라드의 보폭 민감도를 보완한 방법|보폭 크기 개선|keras.optimizer.RMSprop(lr = 0.001, rho = 0.9, epsion = 1e - 08, decay = 0.0)<br><br>알엠에스프롭 함수를 사용|
|아담(Adam)|모멘텀과 알엠에스프롭 방법을 합친 방법|정확도와 보폭 크기 개선|keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e - 08, decay = 0.0)<br><br>아담 함수를 사용한다|

<br><br>
위 표는 방법이 개발된 순서대로 정리되어 있다. 마지막에 기재된 아담이 현재 가장 많이 사용되는 고급 경사 하강법이다. 다양한 실험을 통해 Adam의 성능이 증명되었기 때문인데, 일부에서는 오히려 아담이 over fitting을 가속화한다고 주장하기도 한다. 일반적으로 Adam은 초기 설정을 하지 않아도 성능이 좋다고 하지만 실험결과 초기값을 어떻게 잡냐에 따라 Adam이 오히려 SGD 보다 성능이 떨어지는 경우가 있는 것이 발견되었기 때문에 꼭 Adam을 고집할 필요는 없을 것 같다.
