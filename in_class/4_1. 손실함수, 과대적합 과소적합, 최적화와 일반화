## 손실함수(loss function)
= 오차함수(error function)<br>
= 비용함수(cost function)<br>
= 목적함수(objective function)<br><br>
머신러닝 혹은 딥러닝 모델의 출력값과 사용자가 원하는 출력값의 오차를 의미
- MSE(Mean Squared Error) : 예측한 값과 실제 값 사이의 평균 제곱 오차를 정의한다. 공식이 매우 간단하며, 차가 커질수록 제곱 연산으로 인해 값이 더욱 뚜렷해진다. 또한 제곱으로 인해 오차가 양수거나 음수거나 누적값을 증가시킨다.
- RMSE(Root Mean Squared Error)
MSE에 루트를 씌운 것으로 MSE와 기본적으로 동일. MSE 값은 오류의 제곱을 구하기 때문에 실제 오류 평균보다 더 커지는 특성이 있어 MSE에 루트를 씌운 RMSE는 값의 왜곡을 줄여준다.

### 엔트로피(Entropy)와 크로스 엔트로피(Cross-Entropy)
- Entropy : 정보이론에서의 엔트로피는 불확실성을 나타내며 엔트로피가 높다는 것은 정보는 많고 확률은 낮다는 것을 의미
<br><br>
예시) 공꺼내기
  - 전체 공이 100개일 때 공 하나만 빨간색이고 나머지는 모두 검은색이라면 직관적으로 임의로 공을 하나 뽑았을 때 검은색이 나올 확률이 매우 크다는 것을 알 수 있다.
  - 전체 공이 100개일 때 빨간색과 검은색의 비율이 5:5라면 첫번째 사례보다 검은색 공을 뽑을 확률이 적어짐을 알 수 있는데, 이때 엔트로피가 이전 사례보다 훨씬 크게 계산된다.
<br><br>
- Cross Entropy : 두 확률 분포의 차이를 구하기 위해서 사용된다. 딥러닝에서는 실제 데이터의 확률분포와 학습된 모델이 계산한 확률 분포의 차이를 구하는데 사용되는데 실제분포인 q와 모델링으로 q분포를 예측하고자 하는 p가 모두 들어가서 크로스 엔트로피라고 하는 것이다.
<br><br>
머신러닝을 통한 예측 모형은 훈련 데이터에서 실제 분포인 q를 알 수 있기 때문에 cross entropy를 계산할 수 있다. 즉, 훈련 데이터를 사용한 예측 모형에서 cross entropy는 실제 값과 예측값의 차이를 계산하는데 사용할 수 있다는 것이다.

## 과소적합 과대적합
- 과소적합 : 훈련데이터의 손실이 낮아질 수록 테스트 데이터의 손실도 낮아질 때를 말한다. 모델의 성능이 계속 발전 될 여지가 있는, 네트워크가 훈련 데이터에 있는 모든 관련 패턴을 학습하지 못한 상황을 말한다.
- 과(대)적합 : 훈련 데이터에서 특정 횟수만큼 반복하고 난 후에는 일반화 성능이 더 이상 높아지지 않으며 오히려 검증 셋의 성능이 멈추고 감소되기 시작하는 걸 말한다. 이는 훈련 데이터에 특화된 패턴을 학습하기 시작했다는 의미이다. 데이터에 잡음이 있거나 불확실성이 존재하거나, 드문 특성이 포함되어 있을 때 발생할 가능성이 특히 높다.

## 최적화와 일반화
- 최적화(optimization) : 가능한 훈련 데이터에서 최고의 성능을 얻으려고 모델을 조정하는 과정
- 일반화(generalization) : 훈련된 모델이 이전에 본 적 없는 데이터에서 얼마나 잘 수행되는지
