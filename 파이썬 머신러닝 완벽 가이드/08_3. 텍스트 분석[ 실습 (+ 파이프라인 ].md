위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---

### 텍스트 분류 실습
사이킷런은 `fetch_20newsgroups` API를 이용해 뉴스그룹의 분류를 수행해 볼 수 있는 예제 데이터를 제공한다. 이 데이터 셋을 이용해 어떤 뉴스 그룹인지 텍스트 분류를 적용해봅시다.

```python
from sklearn.datasets import fetch_20newsgroups

new_data = fetch_20newsgroups(subset='all', random_state=15)

new_data
```
![](https://velog.velcdn.com/images/cyhse7/post/5184ee23-149c-4030-893b-7c229188128e/image.png)

잘.. 가져왔다. 옆에 스크롤 크기가 보이는가? 생각보다 더 양이 많았기에 그만큼 가져오는데 시간이 꽤나 걸렸다.

fetch_20newsgroups은 사이킷런의 다른 데이터 셋 예제와 같이 파이썬 딕셔너리와 유사한 Bunch 객체를 반환한다.

```python
new_data.keys()
```
```
dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])
```
```python
import pandas as pd

print("target 클래스의 값과 분포도")
print(pd.Series(new_data.target).value_counts().sort_index())
print('target 클래스의 이름들')
print(new_data.target_names)
```
```
target 클래스의 값과 분포도
0     799
1     973
2     985
3     982
4     963
5     988
6     975
7     990
8     996
9     994
10    999
11    991
12    984
13    990
14    987
15    997
16    910
17    940
18    775
19    628
dtype: int64
target 클래스의 이름들
['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']
```

타깃은 0부터 19까지 20개로 구성되어 있는 모습이다. 데이터 한 개만 추출해 어떤 모습인지 확인

```python
new_data.data[0]
```
```
From: matt@centerline.com (Matt Landau)
Subject: Re: Asynchronous X Windows?
Organization: CenterLine Software, Inc.
Lines: 45
Distribution: inet
NNTP-Posting-Host: 140.239.1.32

In <ellis.735675321@nova> ellis@nova.gmi.edu (R. Stewart Ellis) writes:
>>Is there anyway to use X Windows over an async line?  Clearly you could use
>                                x
>It is X window.

No, it isn't.  It is the "X Window System", or "X11", or "X" or any of
a number of other designations accepted by the X Consortium.  In fact,
doing "man X" on pretty much any X11 machine will tell you:

     The X Consortium requests that the following names  be  used
     when referring to this software:

                                  X
                           X Window System
                             X Version 11
                     X Window System, Version 11
                                 X11

There is no such thing as "X Windows" or "X Window", despite the repeated
misuse of the forms by the trade rags.  This probably tells you something
about how much to trust the trade rags -- if they can't even get the NAME
of the window system right, why should one trust anything else they have 
to say?

...
이하 생략
```
텍스트 데이터를 확인해보면 뉴스 그룹 기사의 내용뿐 아니라 뉴스그룹 제목, 작성자, 소속, 이메일 등의 다양한 정보를 가지고 있다. 이 중에서 순수한 텍스트만으로 구성된 기사내용으로 어떤 뉴스 그룹에 속하는 지 분류할 것이기 때문에 파라미터를 이용해 헤더와 푸터 등을 제거하자

```python
# subset='train'으로 학습용 데이터만 추출
# remove=('headers', 'footers', 'quotes')로 내용만 추출
train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=15)

X_train = train_news.data
y_train = train_news.target

X_train[0]
```
```
I live at sea-level, and am called-upon to travel to high-altitude cities
quite frequently, on business.  The cities in question are at 7000 to 9000
feet of altitude.  One of them especially is very polluted...

Often I feel faint the first two or three days.  I feel lightheaded, and
my heart seems to pound a lot more than at sea-level.  Also, it is very
dry in these cities, so I will tend to drink a lot of water, and keep
away from dehydrating drinks, such as those containing caffeine or alcohol.

Thing is, I still have symptoms.  How can I ensure that my short trips there
(no, I don't usually have a week to acclimatize) are as comfortable as possible?
Is there something else that I could do?

A long time ago (possibly two years ago) there was a discussion here about
altitude adjustment.  Has anyone saved the messages?

Many thanks,
```

train으로 추출된 데이터 하나만 살펴보면, 아까 봤던 내용보다는 훨씬 정돈된 것을 확인해볼 수 있다.

```python
# subset='test'으로 테스트용 데이터만 추출
# remove=('headers', 'footers', 'quotes')로 내용만 추출
test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), random_state=15)

X_test = test_news.data
y_test = test_news.target

print(f'학습 데이터 크기 : {len(train_news.data)}, 테스트 데이터 크기 : {len(test_news.data)}')
```
```
학습 데이터 크기 : 11314, 테스트 데이터 크기 : 7532
```

#### 피처 벡터화 변환

먼저 CountVectorizer를 이용해 학습 데이터의 텍스트를 피처 벡터화 해보자. 한가지 유의해야할 점은 테스트 데이터에서 CountVectorizer 를 적용할 때는 반드시 학습 데이터를 이용해 fit()이 수행된 CountVectorizer 객체를 이용해 테스트 데이터를 변환해야 한다는 것이다.

테스트 데이터의 피처 벡터화 시 fit_transform 을 사용하면 안된다는 점도 유의 사항이다.

```python
from sklearn.feature_extraction.text import CountVectorizer

# Count Vectorization으로 피처 벡터화 변환 수행
cnt_vect = CountVectorizer()
cnt_vect.fit(X_train)
X_train_cnt_vect = cnt_vect.transform(X_train)

# 학습 데이터로 fit()된 CountVectorizer를 이용해 테스트 데이터를 피처 벡터화 변환수행
X_test_cnt_vect = cnt_vect.transform(X_test)

print(f'학습 데이터 텍스트의 CountVectorizer Shape : {X_train_cnt_vect.shape}')
```
```
학습 데이터 텍스트의 CountVectorizer Shape : (11314, 101631)
```
```python
X_test_cnt_vect
```
```
<7532x101631 sparse matrix of type '<class 'numpy.int64'>'
	with 665738 stored elements in Compressed Sparse Row format>
```

11314 개의 문서에서 피처(단어)가 101631개 만들어졌다. 이제 이렇게 피처 벡터화된 데이터에 로지스틱 회귀를 적용해 뉴스그룹에 대한 분류를 예측해보면

#### 모델 학습/예측/평가

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 로지스틱을 이용하여 학습/예측/평가 수행
lr_clf = LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_cnt_vect, y_train)
pred = lr_clf.predict(X_test_cnt_vect)

print(f'CountVectorized Logistic Regression의 예측 정확도는 {accuracy_score(y_test, pred):.3f} 이다.')
```
```
CountVectorized Logistic Regression의 예측 정확도는 0.617 이다.
```
Count 기반으로 피처 벡터화가 적용된 데이터 세트에 대한 로지스틱 회귀 예측의 정확도는 약 0.617이다. 이번에는 TF-IDF 기반으로 벡터화를 변경해 예측 모델을 수행해보자.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF 벡터화를 적용해 줬을 때 학습 데이터와 테스트 데이터 셋 변환
tfidf_vect = TfidfVectorizer()
tfidf_vect.fit(X_train)
X_train_tfidf_vect = tfidf_vect.transform(X_train)
X_test_tfidf_vect = tfidf_vect.transform(X_test)

# 로지스틱 회귀를 이용해 학습/예측 평가 
lr_clf = LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_tfidf_vect, y_train)
pred = lr_clf.predict(X_test_tfidf_vect)

print(f'TR-IDF Logistic Regression의 예측 정확도는 {accuracy_score(y_test, pred):.3f}')
```
```
TR-IDF Logistic Regression의 예측 정확도는 0.678
```
단순 카운트보다 높은 예측 정확도가 나왔다. 일반적으로 문서 내에 텍스트가 많고, 문서가 많은 경우에는 TF_IDF 벡터화가 더 좋은 예측 결과를 도출

#### 파라미터 조정

텍스트 분석에서 머신러닝 모델의 성능을 향상시키는 방법 중 하나는 피처 전처리를 수행하는 것이다. 앞의 TF-IDF 벡터화는 기본 파라미터만 적용했지만, 좀 더 다양한 파라미터를 적용해보자

- 피처 벡터화에서 `stop_words`를 기존 None에서 'english'로 지정한다.
- `ngram_range`로 단어를 2개씩 묶어서 피처로 추출한다
- `max_df`로 전체 문서를 통틀어 300번 이상 존재하는 단어는 제외 

```python
# stop words 필터링을 추가하고
# ngram을 기본 (1, 1)에서 (1, 2)로 변경해 피처 벡터화 적용
tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)

tfidf_vect.fit(X_train)
X_train_tfidf_vect = tfidf_vect.transform(X_train)
X_test_tfidf_vect = tfidf_vect.transform(X_test)

lr_clf = LogisticRegression(solver='liblinear')
lr_clf.fit(X_train_tfidf_vect, y_train)
pred = lr_clf.predict(X_test_tfidf_vect)

print(f'TF-IDF Vectorized Logistic Regression의 예측 정확도는 {accuracy_score(y_test, pred)}')
```
```
TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.6901221455124801
```
앞서 파라미터를 디폴트로 설정하고 돌렸을 때보다는 정확도가 조금 더 증가했다.


#### 로지스틱 하이퍼 파라미터

이번에는 GridSearchCV를 이용해 로지스틱 회귀의 최적 하이퍼 파라미터를 찾아보자

```python
from sklearn.model_selection import GridSearchCV

# 최적 C값 도출 튜닝 수행
# CV는 3 폴드 셋
params = {'C' : [0.01, 0.1, 1, 5, 10]}
grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)
grid_cv_lr.fit(X_train_tfidf_vect, y_train)
```
```
# 최적 C값 도출 튜닝 수행
# CV는 3 폴드 셋
params = {'C' : [0.01, 0.1, 1, 5, 10]}
grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)
grid_cv_lr.fit(X_train_tfidf_vect, y_train)
```
```python
print(f'Best C parameter : {grid_cv_lr.best_params_}')
```
```
Best C parameter : {'C': 10}
```
C가 10일 때 그리드서치CV의 교차 검증 테스트 셋에서 가장 좋은 성능을 나타냈다.
```python
# Best C값으로 학습된 grid_cv로 예측 및 정확도 평가
pred = grid_cv_lr.predict(X_test_tfidf_vect)
print(f'예측 정확도는 {accuracy_score(y_test, pred):.3f}')
```
```
예측 정확도는 0.704
```

이를 테스트 데이터 세트에 적용했더니 약 0.704로 이전보다 수치가 향상되었다. 시간이 꽤 오래 걸리니 유의해서 돌리자..

#### 파이프라인(Pipeline) 사용
pipeline()은 전처리와 ML 학습을 한번에 수행 가능하다. 피처 벡터화 결과를 별도 데이터로 저장하지 않고 바로 ML데이터로 입력되어 수행시간 절약이 가능하고 직관적인 모델 코드를 생성할 수 있다.

```python
from sklearn.pipeline import Pipeline

pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english')),
                     ('lr_clf', LogisticRegression(random_state=156))])

# 별도의 TfidVectorizer 객체의 fit(), transform()과
# LogisticRegression의 fit(), predict()가 필요 없다

# pipeline의 fit()과 predict()만으로 한꺼번에 피처 벡터화와 ML 학습/예측이 가능하다
pipeline.fit(X_train, y_train)
pred = pipeline.predict(X_test)

print(f'Pipeline을 통한 Logistic Regression의 예측 정확도는 {accuracy_score(y_test, pred):.3f}')
```
```
Pipeline을 통한 Logistic Regression의 예측 정확도는 0.691
```
사이킷런은 GridSearchCV 클래스의 생성 파라미터로 Pipeline을 입력해 하이퍼 파라미터 튜닝을 GridSearchCV 방식으로 진행할 수 있게 해준다.

```python
from sklearn.pipeline import Pipeline

pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english')),
                     ('lr_clf', LogisticRegression())])

# Pipeline에 기술된 각각의 객체 변수에 언더바(_) 2개를 연달아 붙여 GridSearchCV에 사용될
# 파라미터/하이퍼 파라미터 이름과 값을 설정
params = { 'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],
           'tfidf_vect__max_df': [100, 300, 700],
           'lr_clf__C': [1,5,10]
}

# GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력
grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)
grid_cv_pipe.fit(X_train, y_train)

print(grid_cv_pipe.best_params_, grid_cv_pipe.best_score_)

pred = grid_cv_pipe.predict(X_test)
lr_acc = accuracy_score(y_test, pred)

print(f'Pipeline을 통한 Logistic Regression 의 예측 정확도: {lr_acc:.3f}')
```
실행 시간이 정말 오래 걸린다. ~~결국 결과를 보지 못했다 다음에 다시 시도해 보는 것으로~~

로지스틱 회귀 외에도 텍스트 분류에는 서포트 벡터머신(Support Vector Machine)이나 나이브 베이즈(Naive Bayes)알고리즘도 희소 행렬 기반의 텍스트 분류에 자주 사용되는 머신러닝 알고리즘이다.
