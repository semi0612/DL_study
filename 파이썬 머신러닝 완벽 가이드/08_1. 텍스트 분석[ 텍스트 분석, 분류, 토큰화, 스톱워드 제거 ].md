위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

----


# 텍스트 분석
## NLP? text 분석?
머신러닝이 보편화 되면서 NLP(natural language processing)와 텍스트 분석(Text Analytics, 이하 TA)을 구분하는 것이 큰 의미는 없어보이지만, 굳이 구분하자면 NLP는 머신이 인간의 언어를 이해하고 해석하는 데 더 중점을 두고 기술이 발전해왔으며, 덱스트 마이닝(Text Mining)이라고도 불리는 텍스트 분석은 비정형 텍스트에서 의미있는 정볼르 추출하는 것에 조금 더 중점을 두고 기술이 발전해 왔다.

## 텍스트 분류
- Text Classification
- 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법
예를 들어 특정 신문기사 내용이 어떤 카테고리(연애/정치/사회 등)에 속하는 지 자동으로 분류하거나 스팸 메일 검출 같은 프로그램이 이에 속한다
- 지도 학습을 적용

### 감성분석
- 텍스트에서 나타나는 감정/판단/믿음/의견/기분 등의 주관적인 요소를 분석하는 기법 통칭
- SNS 감정분석, 영화나 제품에 대한 긍종 또는 부정 리뷰, 여론조사 의견 분석 등의 영역에서 활용
- 지도학습, 비지도 학습 모두 가능하다.

### 텍스트 요약
- 텍스트 내에서 중요하다고 생각되는 주제나 중심 사상을 추출하는 기법
- 대표적으로 토픽 모델링이 있다.

### 텍스트 군집화와 유사도 측정
- 비슷한 유형의 문서에 대해 군집활르 수행하는 기법
- 텍스트 분류를 비지도 학습으로 수행하는 방법의 일종으로 사용될 수 있다.
- 유사도 측정 역시 문서들간 유사도를 측정해 비슷한 문서끼리 모을 수 있는 방법이다.

## 텍스트 분석 이해
텍스트 분석은 비정형 데이터인 텍스트를 분석하는 것이다. 머신러닝 알고리즘은 숫자형의 피처 기반 데이터만 입력 받을 수 있기 때문에, 비정형 텍스트 데이터를 어떻게 피처 형태로 추출하고 추출된 피처에 의미있는 값을 부여하는 가 하는 것은 매우 중요한 요소이다.

텍스트를 word(또는 word의 일부) 기반의 다수 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여하면 텍스트는 단어의 조합인 벡터값으로 표현 될 수 있는데, 이렇게 텍스트를 변환하는 것을 피처 벡터화(Feature Vectorization) 또는 피처 추출(Feature Extraction) 이라한다.

### 프로세스
머신러닝 기반의 텍스트 분석 프로세스는 다음과 같은 프로세스 순으로 수행한다

1. `텍스트 사전 준비작업(텍스트 전처리)`
-텍스트를 피처로 만들기 전에 미리 클렌징, 대소문자 변경, 특수문자 삭제 등의 클렌징 작업
-단어(word)등의 토큰화 작업, 의미없는 단어(Stop word)제거 작업, 어근 추출(Stemming/Lemmatization) 등의 텍스트 정규화 작업수행을 통칭
2. `피처 벡터화/추출`
-사전 준비작업으로 가공된 텍스트에서 피처를 추출하고 벡터 값을 할당한다.
-대표적으로 BOW(Bag of Words)와 Word2Vec방법이 있으며, BOW는 Count기반과 Tf-IDF 기반 벡터화가 있다.
3. `ML모델 수립 및 학습/예측/평가`
-피처 벡터화된 데이터 셋에 ML 모델을 적용해 수행한다.

### 텍스트 분석 패키지
- NLTK(Natural Language Toolkit for Python) : 파이썬의 가장 대표적인 NLP 패키지. 방대한 양의 데이터 셋과 서브 모듈을 가지고 있다. 많은 NLP패키지가 NLTK의 영향을 받아 작성되고 있지만, 수행 속도 측면에서는 아쉬운 부분이 있어 실제 대량의 데이터 기반에서는 제대로 활용되지 못하는 듯하다.
- Gensim : 토픽 모델링 분야에서 가장 두각을 나타내는 패키지. 오래전부터 토픽 모델링을 쉽게 구현할 수 있는 기능을 제공해왔으며, Word2Vec 구현등 다양한 신기능도 제공한다. SpaCy와 함께 가장 많이 사용되는 패키지
- SpaCy : 뛰어난 수행 성능으로 최근 가장 주목을 받는 패키지이다. 많은 NLP 애플리케이션에서 사용 중

## text 전처리 - text 정규화
텍스트 자체를 바로 피처로 만들수가 없기에 이를 위해 사전에 텍스트를 가공하는 준비 작업이 필요하다.

### 클렌징
분석에 오히려 방해되는 불필요한 문자, 기호 등을 사전에 제거하는 작업이다.

예를들어 HTML, XML태그나 특정기호 등을 사전에 제거

### 텍스트 토큰화
주어진 코퍼스(corpus; 말뭉치;)에서 토큰이라불리는 단위로 나누는 작업을 뜻한다. 토큰의 단위는 상황에 따라 다르지만 보통 의미있는 단위로 토큰을 정의한다.

문장/단어 토큰화로 유형을 나눠볼 수 있다.

NLTK는 이를 위해 다양한 API를 제공하며, 사용하기 위한 install 설치는 아래와 같이 할 수 있다.
```python
pip install nltk
```

#### 문장 토큰화
문장 토큰화(sentence tokenization)은 문장의 마침표, 개행문자(\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적이다. 정규 표현식에 따른 문장 토큰화도 가능하지만 NLTK는 많이 쓰이는 `sent_tokenize`를 이용해 수행해보자
```python
from nltk import sent_tokenize
import nltk

# 마침표, 개행 문자등의 데이터 세트를 다운로드
nltk.download('punkt')

text_sample = 'The Matrix is everywhere its all around us, here even in this room. \
                You can see it out your window or on your television. \
                You feel it when you go to work, or go to church or pay your taxes'


# 문장 토큰화를 진행할 text를 sent_tokenize안에 넣어준다.
sentences = sent_tokenize(text=text_sample)
print(type(sentences), len(sentences))
print(sentences)
```
```
<class 'list'> 3
['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes']
```
반환되는 것은 각각의 문장으로 구성된 list 객체

#### 단어 토큰화
문장을 단어로 토큰화 하는것. 기본적으로 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하지만 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행할 수 있다.
```python
from nltk import word_tokenize

words = word_tokenize(text_sample)
print(type(words), len(words))
print(words)
```
```
<class 'list'> 44
['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.', 'You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.', 'You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes']
```

#### 조합하여
word_tokenize, sent_tokenize를 조합하여 문서에 대해 모든 단어를 토큰화해 보자. 위에서 만들어줬던 (3개의 문장으로 된) `text_sample`을 문장으로 나누고, 개별 문장을 다시 단어로 토큰화 하면 되는 것이다.
```python
from nltk import word_tokenize, sent_tokenize

# 빈 리스트 생성
li = []

# text_sample을 문장별로 쪼개기
sentences = sent_tokenize(text=text_sample)

# for 문을 돌리면서 문장별로 단어 쪼개기
for sentences in sentences :
    li.append(word_tokenize(sentences))
li
```
```
[['The',  'Matrix',  'is',  'everywhere',  'its',  'all',  'around',  'us',  ',',  'here',  'even',  'in',  'this',  'room',  '.']
, ['You',  'can',  'see',  'it',  'out',  'your',  'window',  'or',  'on',  'your',  'television',  '.']
, ['You',  'feel',  'it',  'when',  'you',  'go',  'to',  'work',  ',',  'or',  'go',  'to',  'church',  'or',  'pay',  'your',  'taxes']]
```
당연히 list 컴프리핸션으로도 가능하다
```python
sentences = sent_tokenize(text=text_sample)
[word_tokenize(sentences) for sentences in sentences]
```
이와같은 식으로. 결과는 같아 보기 때문에 옮겨오지 않았다.

3개의 문장을 문장별로 먼저 토큰화 했으므로 word_tokens 변수는 3개의 리스트 객체를 내포하는 리스트가 되었다. 그리고 내포된 개별 리스트 객체는 각각 문장별로 토큰화된 단어를 요소로 가지고 있다.

하지만 2개의 단어가 뭉쳐서 하나의 의미를 나타내는 경우를 2-gram(bi-gram)이라 하며, 문장을 단어별로 토큰화 할 경우 문맥적인 의미는 무시될 수 밖에 없다. 이러한 문제를 조금이라도 해결해보고자 도입된 n-gram 이라는 것도 있다.

```
1-gram : uni-gram
2-gram : bi-gram
3-gram : trigram
...
n-gram
```

### 스톱워드(Stop word) 제거
분석에 큰 의미가 없는 단어를 지칭한다. 가령 영어에서 is, the, a, will등 문장을 구성하는 문법 요소는 맞지만 문맥적으로는 큰 의미가 없는 단어가 이에 해당한다.이 단어는 빈번하게 텍스트에 나타나므로 이것들을 사전에 제거하지 않으면 그 빈번함으로 인해 중요한 단어로 인지 될 수 있기 때문에 이전에 제거해야한다.
```python
import nltk
nltk.download('stopwords')
print(nltk.corpus.stopwords.words('english')[:20])
```
```
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']
```

여러 문장을 문장별로 토큰화 후 stopword제거를 해본다면
```python
sentences = sent_tokenize(text_sample)
word_tokens = [word_tokenize(sentence) for sentence in sentences]

# 영어 stopword
stopwords = nltk.corpus.stopwords.words('english')

# 문장별 단어 토큰화 + stopword 제거
all_tokens = []

for sentence in word_tokens:
    filtered_words=[]
    
    # 문장 토큰의 각 단어 토큰
    for word in sentence:
        # 소문자 변환
        word = word.lower()
        # stopword 미포함
        if word not in stopwords:
            filtered_words.append(word)
            
    all_tokens.append(filtered_words)
    
print(all_tokens)
```
```
[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes']]
```
