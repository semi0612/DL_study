ìœ„í‚¤ë¶ìŠ¤ì˜ 'íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²… ê°€ì´ë“œ' ê°œì • 2íŒìœ¼ë¡œ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹¤ìŠµ ê³¼ì •ê³¼ ì´í•´ì— ë”°ë¼ ë‚´ìš©ì˜ ëˆ„ë½ ë° ì½”ë“œì˜ ë³€í˜•ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

------

## scikit-learn ì„¤ì¹˜í•˜ê¸°
í™˜ê²½ : jupyter lab
```python
!pip install scikit-learn

import sklearn
print(sklearn.__version__)
# 1.1.2
```
## ë¶“ê½ƒ í’ˆì¢… ë¶„ë¥˜í•˜ê¸°
1. data set ì„ train data ì™€ test dataë¡œ ë¶„ë¥˜
2. í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•´ ëª¨ë¸ í•™ìŠµ
3. í•™ìŠµëœ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ data set ë¶„ë¥˜ (ì˜ˆì¸¡)
4. ì˜ˆì¸¡ëœ ê²°ê³¼ê°’ê³¼ ì‹¤ì œ ê²°ê³¼ê°’ì„ ë¹„êµí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

iris = load_iris()

iris.keys()
# dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])

type(iris)
# sklearn.utils._bunch.Bunch

# feature ë§Œìœ¼ë¡œ ëœ ë°ì´í„° numpy ë¥¼ ê°€ì§„ 'data' ë¥¼ ë”°ë¡œ ë³€ìˆ˜ì— ì €ì¥
iris_data = iris['data']
iris_data

# lable ë°ì´í„° numpyë¥¼ ê°€ì§„ 'target' ë°ì´í„°ë¥¼ ë”°ë¡œ ë³€ìˆ˜ì— ì €ì¥
iris_label = iris.target
print('targetê°’ : ', iris_label)
print('targetëª… : ', iris.target_names)
# targetê°’ :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2]
# targetëª… :  ['setosa' 'versicolor' 'virginica']

import pandas as pd
import numpy as np

# ë¶“ê½ƒ ë°ì´í„° ì…‹ì„ ìì„¸íˆ ë³´ê¸°ìœ„í•´ DataFrameìœ¼ë¡œ ë³€í™˜
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df['label'] = iris.target
iris_df
```
![](https://velog.velcdn.com/images/cyhse7/post/e281d79d-469c-4a15-a160-f6067893da0a/image.png)
```python
# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ test_sizeì˜ ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ê² ë‹¤.
# 0.2 = í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ìš©í• ê±´ ì „ì²´ì—ì„œ 20% ì´ë‹¤.
# ê¸°ë³¸ê°’ì€ 0.24 ì¦‰ 25% ì´ë‹¤
# random_state = random ê°’ì„ ë§Œë“œëŠ” seed ì™€ ê°™ì€ ì˜ë¯¸. ì¼ì •í•œ ê²°ê³¼ë¥¼ ì–»ê¸°ìœ„í•´ í•„ìš”í•˜ë‹¤
x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2, random_state = 1)

# DecisionTreeClassifier : ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ í´ë˜ìŠ¤
# random_stateë¡œ ì½”ë“œë¥¼ ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ ë™ì¼í•œ í•™ìŠµ/ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆê²Œë” ì„¤ì •í•œë‹¤.
dt_clf = DecisionTreeClassifier(random_state = 11)

# í•™ìŠµ ìˆ˜í–‰
# DecisionTreeClassifierì˜ fit ë©”ì„œë“œ ì‚¬ìš©
# fit(í•™ìŠµìš© ë°ì´í„° ì†ì„±, ê²°ì •ê°’ ë°ì´í„° ì†ì„±)
# fitting = ì í•©í•˜ë‹¤, í•™ìŠµí•œë‹¤, í›ˆë ¨í•œë‹¤.
dt_clf.fit(x_train, y_train)

# í•™ìŠµì´ ì™„ë£Œëœ DecisionTreeClassifier ê°ì²´ì—ì„œ test data setìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤
# DecisionTreeClassifier ê°ì²´ì˜ predict() ë©”ì„œë“œ ì´ìš©
pred = dt_clf.predict(x_test)

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬(DecisionTreeClassifier)ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í‰ê°€
# ì •í™•ë„ë¥¼ ì¸¡ì •í•  ê²ƒì´ë‹¤
# accuracy_score(í…ŒìŠ¤íŠ¸(ì‹¤ì œ) ë ˆì´ë¸” ë°ì´í„° ì…‹, ì˜ˆì¸¡ ë ˆì´ë¸” ë°ì´í„° ì…‹) 
from sklearn.metrics import accuracy_score
print('ì˜ˆì¸¡ ì •í™•ë„:{0:.4f}'.format(accuracy_score(y_test, pred)))

# ì˜ˆì¸¡ ì •í™•ë„:0.9667
```
ì˜ˆì¸¡ ì •í™•ë„ 0.9967 ë¡œ ê½¤ë‚˜ ì˜ í›ˆë ¨ëœ ëª¨ìŠµ.

<br><br>

### +) ğŸŒ± ì‚¬ì´í‚· ëŸ° í”„ë ˆì„ì›Œí¬ ìµíˆê¸°
ì§€ë„í•™ìŠµì˜ ë‘ ì¶•ì¸ ë¶„ë¥˜ì™€ íšŒê·€ì˜ ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ ëª¨ë“  ì‚¬ì´í‚·ëŸ° í´ë˜ìŠ¤ëŠ” `fit()`ê³¼ `predict()`ë¡œ í•™ìŠµê³¼ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

- ëª¨ë¸ì˜ í•™ìŠµì„ ìœ„í•œ `fit()`, í•™ìŠµëœ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ìœ„í•´ ì œê³µë˜ëŠ” ë©”ì„œë“œ `predict()`

- ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ `Classifier`, íšŒê·€ ì•Œê³ ë¦¬ì¦˜ `Regressor` -> ë‘ ê°€ì§€ë¥¼ í•©ì³ì„œ `Estimator`
- í‰ê°€í•¨ìˆ˜ `cross_val_score`ì™€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ ê°™ì€ `GridSearchCV`ì„ ì§€ì›í•˜ëŠ” í´ë˜ìŠ¤ëŠ” `Estimator`ì„ ì¸ìë¡œ ë°›ëŠ”ë‹¤.
- train_test_split()
```
from sklearn.model_selection import train_test_split
train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)

arrays : ë¶„í• ì‹œí‚¬ ë°ì´í„°ë¥¼ ì…ë ¥ (Python list, Numpy array, Pandas dataframe ë“±..)
test_size : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨(float)ì´ë‚˜ ê°¯ìˆ˜(int) (default = 0.25)
train_size : í•™ìŠµ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨(float)ì´ë‚˜ ê°¯ìˆ˜(int) (default = test_sizeì˜ ë‚˜ë¨¸ì§€)
random_state : ë°ì´í„° ë¶„í• ì‹œ ì…”í”Œì´ ì´ë£¨ì–´ì§€ëŠ”ë° ì´ë¥¼ ìœ„í•œ ì‹œë“œê°’ (intë‚˜ RandomStateë¡œ ì…ë ¥)
shuffle : ì…”í”Œì—¬ë¶€ì„¤ì • (default = True)
stratify : ì§€ì •í•œ Dataì˜ ë¹„ìœ¨ì„ ìœ ì§€í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Label Setì¸ Yê°€ 25%ì˜ 0ê³¼ 75%ì˜ 1ë¡œ ì´ë£¨ì–´ì§„ Binary Setì¼ ë•Œ, stratify=Yë¡œ ì„¤ì •í•˜ë©´ ë‚˜ëˆ„ì–´ì§„ ë°ì´í„°ì…‹ë“¤ë„ 0ê³¼ 1ì„ ê°ê° 25%, 75%ë¡œ ìœ ì§€í•œ ì±„ ë¶„í• ëœë‹¤.
```


<br><br>

### +) ğŸŒ± êµì°¨ê²€ì¦
í•™ìŠµì„ ë„ˆë¬´ ë§ì´ í•˜ê²Œ ë˜ë©´ ëª¨ë¸ì´ ê³¼í•˜ê²Œ í•™ìŠµ ë°ì´í„°ì—ë§Œ ìµœì í™”ë˜ì–´ ì‹¤ì œ ë°ì´í„°ë¥¼ ë„£ì—ˆì„ ë•Œ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ êµì°¨ ê²€ì¦ì´ë¼ëŠ”ê²Œ í•„ìš”í•˜ë‹¤.

ë°ì´í„°ì˜ í¸ì¦ì„ ë§‰ê¸° ìœ„í•´ì„œ ì—¬ëŸ¬ ì„¸íŠ¸ë¡œ êµ¬ì„±ëœ í•™ìŠµë°ì´í„°ì™€ ê²€ì¦ë°ì´í„°ë¡œ í•™ìŠµê³¼ ê²€ì¦(í‰ê°€)ë¥¼ ë°˜ë³µ ìˆ˜í–‰í•˜ê²Œ ë˜ëŠ” ê²ƒì„ ë§í•œë‹¤.

<br>

#### K í´ë“œ êµì°¨ ê²€ì¦
ê°€ì¥ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” êµì°¨ ê²€ì¦ ê¸°ë²•ì´ë‹¤. Kê°œì˜ ë°ì´í„° í´ë“œ ì„¸íŠ¸ë¥¼ ë§Œë“¤ì–´ì„œ Kë²ˆë§Œí¼ ê° í´ë“œ ì„¸íŠ¸ì— í•™ìŠµê³¼ ê²€ì¦ í‰ê°€ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•.
ì‚¬ì´í‚·ëŸ°ì—ëŠ” `KFold` ì™€ `Stratified-KFold` í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤.
-> `Stratified-KFold`
ë¶ˆê· í˜•í•œ(imbalanced) ë¶„í¬ë„ë¥¼ ê°€ì§„ ë ˆì´ë¸”(ê²°ì • í´ë˜ìŠ¤) ë°ì´í„° ì§‘í•©ì„ ìœ„í•œ K í´ë“œ ë°©ì‹ì´ë‹¤. ë¶ˆê· í˜•í•œ ë¶„í¬ë„ë¥¼ ê°€ì§„ ë ˆì´ë¸” ë°ì´í„° ì§‘í•©ì€ íŠ¹ì • ë ˆì´ë¸” ê°’ì´ íŠ¹ì´í•˜ê²Œ ë§ê±°ë‚˜ ë§¤ìš° ì ì–´ì„œ ê°’ì˜ ë¶„í¬ê°€ í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì¹˜ëŠ” ê²ƒì„ ë§í•œë‹¤.

`Stratified-KFold` ëŠ” ì›ë³¸ ë°ì´í„°ì˜ ë ˆì´ë¸” ë¶„í¬ë¥¼ ë¨¼ì € ê³ ë ¤í•œ ë’¤ ì´ ë¶„í¬ì™€ ë™ì¼í•˜ê²Œ í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¶„ë°°í•˜ë©° KFoldê°€ ë ˆì´ë¸” ë°ì´í„° ì§‘í•©ì´ ì›ë³¸ ë°ì´í„° ì§‘í•©ì˜ ë ˆì´ë¸” ë¶„í¬ë¥¼ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ì œëŒ€ë¡œ ë¶„ë°°í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•´ì¤€ë‹¤.

KFold
```python
from sklearn.model_selection import KFold

iris = load_iris()     # íŒŒì¼ë¶€ë¥´ê¸°

iris_data = iris.data  # data(ì •ë³´)ì™€ target(ë§¤ì¹˜ëŒ€ìƒ) ì €ì¥
iris_label= iris.target

dt_clf = DecisionTreeClassifier()      # ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ 
kfold = KFold(n_splits=5)

cv_accuracy = []

# KFoldê°ì²´ì˜ split( ) í˜¸ì¶œí•˜ë©´ í´ë“œ ë³„ í•™ìŠµìš©, ê²€ì¦ìš© í…ŒìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤. ì´ê±° ì¤‘ìš”í•¨.  
for train_index, test_index  in kfold.split(iris_data):     
    # print(train_index, test_index)
    # print("---"*10)
    # kfold.split( )ìœ¼ë¡œ ë°˜í™˜ëœ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµìš©, ê²€ì¦ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
    X_train, X_test = iris_data[train_index], iris_data[test_index]
    y_train, y_test = iris_label[train_index], iris_label[test_index]

    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)    
   
    accuracy = accuracy_score(y_test,pred)   
    cv_accuracy.append(accuracy)
 
    print("ê°œë³„ ê²€ì¦ ì •í™•ë„: ", cv_accuracy)
print('í‰ê·  ê²€ì¦ ì •í™•ë„:', np.mean(cv_accuracy))


# ê°œë³„ ê²€ì¦ ì •í™•ë„:  [1.0]
# ê°œë³„ ê²€ì¦ ì •í™•ë„:  [1.0, 1.0]
# ê°œë³„ ê²€ì¦ ì •í™•ë„:  [1.0, 1.0, 0.9]
# ê°œë³„ ê²€ì¦ ì •í™•ë„:  [1.0, 1.0, 0.9, 0.9333333333333333]
# ê°œë³„ ê²€ì¦ ì •í™•ë„:  [1.0, 1.0, 0.9, 0.9333333333333333, 0.8]
# í‰ê·  ê²€ì¦ ì •í™•ë„: 0.9266666666666665
```

Stratified KFlod
```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold


iris = load_iris()
features = iris_data = iris.data
labels = iris_label= iris.target

dct_clf = DecisionTreeClassifier()

s_fold = StratifiedKFold(n_splits=3)
n_iter = 0

for train_index, test_index in s_fold.split(features, labels) :
    # print(train_index, test_index)
    x_train, x_test = iris_data[train_index], features[test_index]
    y_train, y_test = labels[train_index], labels[test_index]
    
    dt_clf.fit(x_train, y_train)
    pred = dt_clf.predict(x_test)
    
    acc = accuracy_score(y_test, pred)
    cv_accuracy.append(acc)
 
    print("ê°œë³„ ê²€ì¦ ì •í™•ë„: ", acc)
print('í‰ê·  ê²€ì¦ ì •í™•ë„:', np.mean(cv_accuracy))


# ê°œë³„ ê²€ì¦ ì •í™•ë„:  0.98
# ê°œë³„ ê²€ì¦ ì •í™•ë„:  0.94
# ê°œë³„ ê²€ì¦ ì •í™•ë„:  0.96
# í‰ê·  ê²€ì¦ ì •í™•ë„: 0.9484848484848485
```
<br>

#### cross_val_score()
êµì°¨ê²€ì¦ì„ ë³´ë‹¤ ì‰½ê²Œ í• ìˆ˜ ìˆë„ë¡ ì œê³µë˜ëŠ” ì‚¬ì´í‚·ëŸ°ì˜ API ì¤‘ ëŒ€í‘œì ì¸ ê²ƒì´ë‹¤. classifierê°€ ì…ë ¥ë˜ë©´ `Stratified K Flod` ë°©ì‹ìœ¼ë¡œ ë ˆì´ë¸”ê°’ì˜ ë¶„í¬ì— ë”°ë¼ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì…‹ì„ ë¶„í• í•œë‹¤. íšŒê·€ì˜ ê²½ìš°ì—ëŠ” ê·¸ ë°©ì‹ìœ¼ë¡œ ë¶„í• í• ìˆ˜ ì—†ìœ¼ë¯€ë¡œ `KFold` ë°©ì‹ìœ¼ë¡œ ë¶„í• 
```
cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')

estimator : ì‚¬ì´í‚·ëŸ°ì˜ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤ì¸ Clasifier ë„ëŠ” íšŒê·€ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤ì¸ Regressor
X : í”¼ì²˜ ë°ì´í„° ì…‹
y : ë ˆì´ë¸” ë°ì´í„° ì…‹
scoring : ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
cv : êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜

ìˆ˜í–‰ í›„ ë°˜í™˜ ê°’ : cvë¡œ ì§€ì •ëœ íšŸìˆ˜ë§Œí¼ scoring íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •ëœ í‰ê°€ ì§€í‘œë¡œ í‰ê°€ ê²°ê³¼ê°’ì„ ë°°ì—´ë¡œ ë°˜í™˜
				 ì¼ë°˜ì ìœ¼ë¡œ ê·¸ ê°’ì„ í‰ê· í•˜ì—¬ í‰ê°€ ìˆ˜ì¹˜ë¡œ ì‚¬ìš©í•œë‹¤.
```

`cross_val_score()` ëŠ” estimatorë¥¼ í•™ìŠµ(fit), ì˜ˆì¸¡(predict), í‰ê°€(evaluation) ì‹œì¼œì£¼ê¸° ë•Œë¬¸ì— ê°„ë‹¨í•˜ê²Œ êµì°¨ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.
```python
# êµì°¨ê²€ì¦
from sklearn.model_selection import cross_val_score, cross_validate

iris_data1 = load_iris()

data = iris_data1.data
label = iris_data1.target

dt_clf = DecisionTreeClassifier()

scores = cross_val_score(dt_clf, data, label, scoring = 'accuracy', cv = 3)
print('êµì°¨ ê²€ì¦ë³„ ì •í™•ë„ : ', np.round(scores, 4))
print('í‰ê·  ê²€ì¦ ì •í™•ë„ : ', np.round(np.mean(scores), 4))

# êµì°¨ ê²€ì¦ë³„ ì •í™•ë„ :  [0.98 0.92 0.98]
# í‰ê·  ê²€ì¦ ì •í™•ë„ :  0.96
```
<br>

#### GridSearchCV
êµì°¨ê²€ì¦ê³¼ ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•œë²ˆì— í•´ì£¼ëŠ” API. Classifier ë‚˜ Regressor ê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì— ì‚¬ìš©ë˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥í•˜ë©´ì„œ í¸ë¦¬í•˜ê²Œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ë°©ì•ˆì„ ì œê³µí•œë‹¤.

```
GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)[

estimator : classifier, regressor, pipeline ë“±ì´ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.
param_grid : estimatorì˜ íŠœë‹ì„ ìœ„í•˜ì—¬ íŒŒë¼ë¯¸í„°, ì‚¬ìš©ë  íŒŒë¼ë¯¸í„°ë¥¼ dictionary í˜•íƒœë¡œ ë§Œë“¤ì–´ì„œ ë„£ëŠ”ë‹¤.
scoring : ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì¸¡ì •í•  í‰ê°€ ë°©ë²•ì„ ë„£ëŠ”ë‹¤. ë³´í†µ accuracy ë¡œ ì§€ì •í•˜ì—¬ì„œ ì •í™•ë„ë¡œ ì„±ëŠ¥ í‰ê°€ë¥¼ í•œë‹¤.
cv : êµì°¨ ê²€ì¦ì—ì„œ ëª‡ê°œë¡œ ë¶„í• ë˜ëŠ”ì§€ ì§€ì •í•œë‹¤.
refit : Trueê°€ ë””í´íŠ¸ë¡œ Trueë¡œ í•˜ë©´ ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ì„œ ì¬í•™ìŠµ ì‹œí‚¨ë‹¤.
```
ì•Œê³ ë¦¬ì¦˜ì— ì‚¬ìš©ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥í•˜ë©´ì„œ í¸ë¦¬í•˜ê²Œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ë°©ì•ˆì„ ì œê³µ. (Grid=ê²©ì)
```python
from sklearn.model_selection import GridSearchCV

# ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³  í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
iris_data2 = load_iris()

data = iris_data2.data
label = iris_data2.target

x_train, x_test, y_train, y_test = train_test_split(data, iris_data2.target, test_size=0.2, random_state=121)


# í…ŒìŠ¤íŠ¸í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ
## í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ëª…ì¹­ì€ ë¬¸ìì—´ key ê°’ìœ¼ë¡œ, í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ê°’ì€ ë¦¬ìŠ¤íŠ¸í˜•ìœ¼ë¡œ ì„¤ì •
parameters = {'max_depth' : [1,2,3], 'min_samples_split' : [2, 3]}

# DecisionTreeClassifier : ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ í´ë˜ìŠ¤
dtree = DecisionTreeClassifier()

# param_gridì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ 3ê°œì˜ train, test set foldë¡œ ë‚˜ëˆ„ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•  ê²ƒì´ë‹¤
# refit = TrueëŠ” ê¸°ë³¸ê°’. ê°€ì¥ ì¢‹ì€ íŒŒë¼ë¯¸í„° ì„¤ì •ìœ¼ë¡œ ì¬í•™ìŠµ ì‹œí‚¤ê² ë‹¤ëŠ” ì˜µì…˜ì´ë‹¤.
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)


# í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë¥¼ fit ë©”ì„œë“œì— ì¸ìë¡œ ì…ë ¥
# í•™ìŠµ ë°ì´í„°ë¥¼ (grid_dtreeì˜) cvì— ê¸°ìˆ ëœ í´ë”©ì„¸íŠ¸ë§Œí¼ ë¶„í• í•˜ì—¬ param_gridì— ê¸°ìˆ ëœ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼
# ìˆœì°¨ì ìœ¼ë¡œ ë³€ê²½í•˜ë©´ì„œ í•™ìŠµ ë° í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ cv_result ì†ì„±ì— ê¸°ë¡í•œë‹¤.
# ë¶“ê½ƒ í•™ìŠµ ë°ì´í„°ë¡œ param_gridì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€
grid_dtree.fit(x_train, y_train)

# ê²°ê³¼ë¥¼ ì¶”ì¶œí•´ DFí˜•ìœ¼ë¡œ ë³€í™˜
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']]
scores_df
```
![](https://velog.velcdn.com/images/cyhse7/post/391e1e0c-6ab2-4bb2-85f9-d91425063452/image.png)
`params` ì¹¼ëŸ¼ : ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ ì ìš©ëœ ê°œë³„ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’
`rank_test_score` : í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë³„ë¡œ ì„±ëŠ¥ì´ ì¢‹ì€ scoreìˆœìœ„. 1ì´ ê°€ì¥ ë›°ì–´ë‚œ ìˆœìœ„ë¯¸ë©° ì´ë•Œì˜ íŒŒë¼ë¯¸í„°ê°€ ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°
`mean_test_score` : ê°œë³„ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ë³„ë¡œ cvì˜ í´ë”© í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì´ ìˆ˜í–‰í•œ í‰ê°€ ê¸°ì¤€ê°’

```python
print('GridSerachCV ìµœì ì˜ íŒŒë¼ë¯¸í„° : ', grid_dtree.best_params_)
print(f'GridSerachCV ìµœê³  ì •í™•ë„ : {grid_dtree.best_score_:.4f}')
# GridSerachCV ìµœì ì˜ íŒŒë¼ë¯¸í„° :  {'max_depth': 3, 'min_samples_split': 2}
# GridSerachCV ìµœê³  ì •í™•ë„ : 0.9750


# refitìœ¼ë¡œ ì´ë¯¸ í•™ìŠµëœ estimator ë°˜í™˜
estimator = grid_dtree.best_estimator_
pred = estimator.predict(x_test)
print('{0:.4f}'.format(accuracy_score(y_test, pred)))
# 0.9667
```

ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ `GridSearchCV`ë¥¼ ì´ìš©í•´ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìˆ˜í–‰í•œ ë’¤ì— ë³„ë„ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì´ë¥¼ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©ë°©ë²•ì´ë‹¤.

<br>

### ğŸŒ± ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ì£¼ìš” í”„ë¡œì„¸ìŠ¤
ì¼ë°˜ì ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ì£¼ìš” í”„ë¡œì„¸ìŠ¤ëŠ” í”¼ì²˜ì˜ ê°€ê³µ, ë³€ê²½, ì¶”ì¶œì„ ìˆ˜í–‰í•˜ëŠ” í”¼ì²˜ ì²˜ë¦¬(feature processing), ML ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ/ì˜ˆì¸¡ ìˆ˜í–‰, ê·¸ë¦¬ê³  ëª¨ë¸ í‰ê°€ì˜ ë‹¨ê³„ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤. 

----

## ğŸ§ƒ set_index()
DataFrame ë‚´ì˜ ì—´ì„ ì´ìš©í•œ ì¸ë±ìŠ¤ ì„¤ì •ì´ë‹¤. ì´ ë©”ì„œë“œëŠ” ì—´ì˜ label í˜¹ì€ lable listë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
`DF.set_index(keys, drop = True, append = False, inplace = Flase)`

keys : ì¸ë±ìŠ¤ë¡œ ì„¸íŒ…í•˜ë ¤ëŠ” ì—´
-> ë³µìˆ˜ê°œì˜ ì—´ ì…ë ¥ì‹œ ë©€í‹° ì¸ë±ìŠ¤ê°€ ëœë‹¤.
drop : ì¸ë±ìŠ¤ë¡œ ì„¸íŒ…í•œ ì—´ì„ DataFrameë‚´ì—ì„œ ì‚­ì œí• ì§€ ì—¬ë¶€ ê²°ì •
append : ê¸°ì¡´ì— ì¡´ì¬í•˜ë˜ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí• ì§€ ì—¬ë¶€ ê²°ì •
inplace : ì›ë³¸ ê°ì²´ë¥¼ ë³€ê²½í• ì§€ì˜ ì—¬ë¶€ ì„ íƒ

```python
# ì˜ˆì‹œë¡œ ì‚¬ìš©í•  DataFrame
import pydataset
mpg = pydataset.data('mpg')
new_mpg = mpg.iloc[:5, 2:5]
print(new_mpg)
#    displ  year  cyl
# 1    1.8  1999    4
# 2    1.8  1999    4
# 3    2.0  2008    4
# 4    2.0  2008    4
# 5    2.8  1999    6
```

'displ' ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ë§Œë“¤ì–´ë³´ê² ë‹¤
```python
print(new_mpg.set_index('displ'))
#        year  cyl
# displ           
# 1.8    1999    4
# 1.8    1999    4
# 2.0    2008    4
# 2.0    2008    4
# 2.8    1999    6
```
indexí™” ì‹œí‚¬ 'displ' ì»¬ëŸ¼ì„ keysë¡œ ì ì–´ì£¼ì—ˆë‹¤. ë‹¤ë¥¸ ì˜µì…˜ë“¤ì€ ê¸°ì¬í•˜ì§€ ì•Šì•„ ë””í´íŠ¸ ê°’ìœ¼ë¡œ ì¶œë ¥ëœ ëª¨ìŠµì´ë‹¤.

`drop = False` ê°’ìœ¼ë¡œ ì„¤ì •í•  ê²½ìš°
```python
print(new_mpg.set_index(keys = 'displ', drop = False ))
#        displ  year  cyl
# displ                  
# 1.8      1.8  1999    4
# 1.8      1.8  1999    4
# 2.0      2.0  2008    4
# 2.0      2.0  2008    4
# 2.8      2.8  1999    6
```
'displ' ì»¬ëŸ¼ì´ ì¸ë±ìŠ¤ê°€ ë˜ì—ˆìŒì—ë„ ì‚­ì œë˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‚¨ì•„ìˆëŠ” ê²ƒì„ í™•ì¸

`append = True` ì˜ ê²½ìš°
```python
print(new_mpg.set_index(keys = 'displ', append = True ))
#          year  cyl
#   displ           
# 1 1.8    1999    4
# 2 1.8    1999    4
# 3 2.0    2008    4
# 4 2.0    2008    4
# 5 2.8    1999    6
```
ì›ë˜ ìˆë˜ ì¸ë±ìŠ¤ê°€ ì‚­ì œë˜ì§€ ì•Šì€ ìƒíƒœë¡œ ìƒˆë¡­ê²Œ 'displ' ì»¬ëŸ¼ì´ ì¸ë±ìŠ¤í™” ë˜ì—ˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤(ë©€í‹° ì¸ë±ìŠ¤)



## ğŸ§ƒ reset_index()
ê¸°ë³¸ê°’ìœ¼ë¡œ ì¸ë±ìŠ¤ ê°’ì„ ì»¬ëŸ¼í™” ì‹œí‚¤ëŠ” ë° ì‚¬ìš©í•œë‹¤. ì¦‰, `set_index()` ì˜ ê¸°ëŠ¥ì„ ì—­ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì— ê°€ê¹ë‹¤.
`DF.reset_index(drop = False, inplace = False)`

drop : ì¸ë±ìŠ¤ë¡œ ì„¸íŒ…í•œ ì—´ì„ DataFrame ë‚´ì—ì„œ ì‚­ì œí• ì§€ ê²°ì •í•œë‹¤
inplace : ì›ë³¸ ê°ì²´ë¥¼ ì´ëŒ€ë¡œ ë³€ê²½í• ì§€ ê²°ì •í•œë‹¤.


ì˜ˆì‹œë¡œ ì‚¬ìš©í•  ë°ì´í„°ì˜ í˜„ì¬ ëª¨ìŠµì´ë‹¤.
```python
print(new_mpg)
#    displ  year  cyl
# 1    1.8  1999    4
# 2    1.8  1999    4
# 3    2.0  2008    4
# 4    2.0  2008    4
# 5    2.8  1999    6
```

ê´„í˜¸ì•ˆì— ì•„ë¬´ëŸ° ê°’ì„ ì ì§€ ì•Šê³  ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
```python
print(new_mpg.reset_index())
#    index  displ  year  cyl
# 0      1    1.8  1999    4
# 1      2    1.8  1999    4
# 2      3    2.0  2008    4
# 3      4    2.0  2008    4
# 4      5    2.8  1999    6
```
ì¸ë±ìŠ¤ì˜€ë˜ ê°’ë“¤ì´ ìë™ìœ¼ë¡œ ìƒê¸´ 'index' ì»¬ëŸ¼ ì•ˆìœ¼ë¡œ ë°€ë ¤ë“¤ì–´ê°„ ëª¨ìŠµì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. info()ë¡œ ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•˜ë©´ 'int64' ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤.

`drop = True` ì¸ ê²½ìš°
```python
# ê·¸ëƒ¥ í•˜ë©´ ì°¨ì´ê°€ ëˆˆì— ë³´ì´ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë‹ˆ displ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤í™” ì‹œì¼œì£¼ê² ë‹¤
new_mpg.set_index(keys = 'displ', inplace=True)
print(new_mpg)
#        year  cyl
# displ           
# 1.8    1999    4
# 1.8    1999    4
# 2.0    2008    4
# 2.0    2008    4
# 2.8    1999    6

# displ ì»¬ëŸ¼ì´ ì¸ë±ìŠ¤ê°€ ëœ ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ ìƒíƒœì—ì„œ dropì„ í•´ë³´ë©´
print(new_mpg.reset_index(drop = True))
#    year  cyl
# 0  1999    4
# 1  1999    4
# 2  2008    4
# 3  2008    4
# 4  1999    6
```
ì¸ë±ìŠ¤ì˜€ë˜ displì´ ì‚­ì œë˜ê³  ì •ìˆ˜í˜• indexê°€ ë‚¨ì•„ìˆëŠ” ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤

------

## loc[] ì—°ì‚°ì
1. ê°œë³„ ë˜ëŠ” ì—¬ëŸ¬ ì¹¼ëŸ¼ ê°’ ì „ì²´ë¥¼ ì¶”ì¶œí•˜ê³ ì í•œë‹¤ë©´ `iloc[]`ë‚˜ `loc[]` ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  `data_df['ì¹¼ëŸ¼ëª…']` ìœ¼ë¡œë„ ì¶©ë¶„í•˜ë‹¤. í•˜ì§€ë§Œ í–‰ê³¼ ì—´ì„ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì•¼ í•œë‹¤ë©´ `iloc[]`ë‚˜ `loc[]`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
2. `iloc[]`ì™€ `loc[]` ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ëª…ì¹­ ê¸°ë°˜ ì¸ë±ì‹±ê³¼ ìœ„ì¹˜ ê¸°ë°˜ ì¸ë±ì‹±ì˜ ì°¨ì´ë¥¼ ë¨¼ì € ì´í•´í•´ì•¼ í•œë‹¤. DataFrameì˜ ì¸ë±ìŠ¤ë‚˜ ì¹¼ëŸ¼ëª…ìœ¼ë¡œ ë°ì´í„°ì— ì ‘ê·¼í•˜ëŠ” ê²ƒì€ ëª…ì¹­ ê¸°ë°˜ ì¸ë±ì‹±ì´ë‹¤. 0ë¶€í„° ì‹œì‘í•˜ëŠ” í–‰, ì—´ì˜ ìœ„ì¹˜ ì¢Œí‘œë§Œ ì˜ì¡´í•˜ëŠ” ê²ƒì´ ìœ„ì¹˜ ê¸°ë°˜ ì¸ë±ì‹±
3. `iloc[]`ëŠ” ìœ„ì¹˜ ê¸°ë°˜ ì¸ë±ì‹±ë§Œ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ í–‰ê³¼ ì—´ ìœ„ì¹˜ ê°’ìœ¼ë¡œ ì •ìˆ˜í˜• ê°’ì„ ì§€ì •í•´ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜
4. `loc[]`ëŠ” ëª…ì¹­ê¸°ë°˜ ì¸ë±ì‹±ë§Œ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ í–‰ ìœ„ì¹˜ì— DataFrame ì¸ë±ìŠ¤ê°€ ì˜¤ë©°, ì—´ ìœ„ì¹˜ì—ëŠ” ì¹¼ëŸ¼ëª…ì„ ì§€ì •í•´ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜
5. ëª…ì¹­ ê¸°ë°˜ ì¸ë±ì‹±ì—ì„œ ìŠ¬ë¼ì´ì‹±ì„ `ì‹œì‘ì :ì¢…ë£Œì ` ìœ¼ë¡œ ì§€ì •í•  ë•Œ ì‹œì‘ì ì—ì„œ ì¢…ë£Œì ì„ í¬í•¨í•œ ìœ„ì¹˜ì— ìˆëŠ” ë°ì´í„°ê°€ ë°˜í™˜
```pyton
df.loc['three', 'Name'] : ì¸ë±ìŠ¤ê°€ 'three'ì¸ í–‰ì˜ 'Name' ì¹¼ëŸ¼ ê°’ì„ ë°˜í™˜. ë‹¨ì¼ê°’
df.loc['one':'three', 'Name':'Gender'] : ì¸ë±ìŠ¤ê°’ one ë¶€í„° three ê¹Œì§€ / ì»¬ëŸ¼ì€ Name ë¶€í„° Gender ê¹Œì§€ DataFrameì´ ë°˜í™˜
df.loc[df['year']>2014] : boolí˜• ê°€ëŠ¥. yearì¹¼ëŸ¼ê°’ì´ 2014 ì´ˆê³¼ì¸ ë°ì´í„°ë§Œ ì¸ë±ì‹±ìœ¼ë¡œ ì¶”ì¶œ


df.iloc[1,0] : ì¸ë±ìŠ¤ 1ë²ˆì˜ 0ë²ˆ ì¹¼ëŸ¼ ê°’ ë°˜í™˜, ë‹¨ì¼ ê°’
df.iloc[0:2, [0, 1]] : 0:2 ìŠ¬ë¼ì´ì‹± ë²”ìœ„ì˜ ì²«ë²ˆì§¸(0)~ë‘ë²ˆì§¸í–‰(1) / ì²«ë²ˆì§¸(0)~ë‘ë²ˆì§¸ ì—´(1)ì— í•´ë‹¹í•˜ëŠ” DataFrame ë°˜í™˜
df.iloc[0:2, 0:3] : 0:2 ìŠ¬ë¼ì´ì‹± ê°’ ë¶€í„° 0:3 ìŠ¬ë¼ì´ì‹± ê°’ ì‚¬ì´ì˜ ëª¨ë“  DataFrameê°’ ë°˜í™˜
```

----


# ë°ì´í„° ì „ì²˜ë¦¬
ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì–´ë–¤ ë°ì´í„°ë¥¼ ê°€ì§€ëŠëƒì— ë”°ë¼ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ë§Œí¼ ì¤‘ìš”í•œ ë¶€ë¶„.

`ê²°ì†ê°’`, `NaN`, `Null` ê°’ì€ í—ˆìš©ë˜ì§€ ì•Šìœ¼ë©°, ì´ëŸ¬í•œ ê°’ì€ ê³ ì •ëœ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì–´ì•¼ í•œë‹¤. ë˜í•œ ë¬¸ìì—´ ê°’ì„ ì…ë ¥ê°’ìœ¼ë¡œ í—ˆìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¸ì½”ë”©í•˜ì—¬ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.(í…ìŠ¤íŠ¸í˜• í”¼ì²˜ëŠ” í”¼ì²˜ ë²¡í„°í™”(feature vectorization)ë“±ì˜ ê¸°ë²•ìœ¼ë¡œ)

--------
# ë°ì´í„° ì¸ì½”ë”©
ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ëŒ€í‘œì ì¸ ì¸ì½”ë”© ë°©ì‹ì€ **ë ˆì´ë¸” ì¸ì½”ë”©(Label encoding)**ê³¼ **ì›-í•« ì¸ì½”ë”©(One Hot encoding)**

## ë ˆì´ë¸” ì¸ì½”ë”©
ì¹´í…Œê³ ë¦¬ë¥¼ ì½”ë“œí˜• ìˆ«ìê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤. ì‚¬ì´í‚·ëŸ°ì˜ ë ˆì´ë¸”ì¸ì½”ë”©ì€ LabelEncoder í´ë˜ìŠ¤ë¡œ êµ¬í˜„í•˜ë©°, ê°ì²´ ìƒì„± í›„ fitê³¼ transform í˜¹ì€ fit_transform ìœ¼ë¡œ ë ˆì´ë¸” ì¸ì½”ë”©ì„ ìˆ˜í–‰í•œë‹¤.
```python
from sklearn.preprocessing import LabelEncoder

items = ['TV', 'ëƒ‰ì¥ê³ ','ì „ìë ˆì¸ì§€','ì»´í“¨í„°','ì„ í’ê¸°','ì„ í’ê¸°','ë¯¹ì„œ','ë¯¹ì„œ']

# LabelEncoderë¥¼ ê°ì²´ë¡œ ìƒì„±í•œ í›„, fitê³¼ transform()ìœ¼ë¡œ ë ˆì´ë¸”ì¸ì½”ë”© ìˆ˜í–‰
encoder = LabelEncoder()
encoder.fit_transform(items)
# array([0, 1, 4, 5, 3, 3, 2, 2], dtype=int64)
```
ìœ„ì²˜ëŸ¼ ì˜ˆë¥¼ ë“¤ì€ ë°ì´í„°ì˜ êµ¬ë¶„ì´ `['TV', 'ëƒ‰ì¥ê³ ','ì „ìë ˆì¸ì§€','ì»´í“¨í„°','ì„ í’ê¸°','ì„ í’ê¸°','ë¯¹ì„œ','ë¯¹ì„œ']` ë¡œ ë˜ì–´ìˆë‹¤ë©´ tv:1/ëƒ‰ì¥ê³ :2 ... ì™€ ê°™ì€ ìˆ«ìí˜• ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤.

LabelEncoderê°ì²´ì˜ classes_ ì†ì„±ê°’ìœ¼ë¡œ ì¸ì½”ë”© í´ë˜ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ë„ ìˆë‹¤. ì´ ì†ì„±ì€ 0ë²ˆë¶€í„° ìˆœì„œëŒ€ë¡œ ë³€í™˜ëœ ì¸ì½”ë”© ê°’ì— ëŒ€í•œ ì›ë³¸ê°’ì„ ê°€ì§€ê³  ìˆìŒì„ ì•Œìˆ˜ ìˆê²Œ í•œë‹¤.
```python
encoder.classes_
# array(['TV', 'ëƒ‰ì¥ê³ ', 'ë¯¹ì„œ', 'ì„ í’ê¸°', 'ì „ìë ˆì¸ì§€', 'ì»´í“¨í„°'], dtype='<U5')
```
`inverse_transform()`ì„ í†µí•´ ì¸ì½”ë”© ëœ ê°’ì„ ë‹¤ì‹œ ë””ì½”ë”© í•  ìˆ˜ë„ ìˆë‹¤.
```python
encoder.inverse_transform([0, 1, 4, 5, 3, 3, 2, 2])
# array(['TV', 'ëƒ‰ì¥ê³ ', 'ì „ìë ˆì¸ì§€', 'ì»´í“¨í„°', 'ì„ í’ê¸°', 'ì„ í’ê¸°', 'ë¯¹ì„œ', 'ë¯¹ì„œ'], dtype='<U5')
```

## ì›-í•« ì¸ì½”ë”©
í”¼ì²˜ê°’ì˜ ìœ í˜•ì— ë”°ë¼ ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ì¶”ê°€í•´ 'ê³ ìœ ê°’'ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ë§Œ 1ì„ í‘œì‹œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì„ í‘œì‹œí•˜ëŠ” ë°©ì‹ì´ë‹¤.

ì‚¬ì´í‚·ëŸ°ì—ì„œ One-Hot ì¸ì½”ë”©ì€ OneHotEncoder í´ë˜ìŠ¤ë¡œ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë©°, ì…ë ¥ê°’ìœ¼ë¡œ 2ì°¨ì› ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒê³¼ ë³€í™˜ê°’ì´ í¬ì†Œí–‰ë ¬(Sparse Matrix) í˜•íƒœì´ë¯€ë¡œ ì´ë¥¼ ë‹¤ì‹œ toarray() ë©”ì„œë“œë¥¼ ì´ìš©í•´ ë°€ì§‘í–‰ë ¬(Dense Matrix)ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤ëŠ” ì£¼ì˜ì ì´ ìˆë‹¤.
```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np

items = ['TV', 'ëƒ‰ì¥ê³ ','ì „ìë ˆì¸ì§€','ì»´í“¨í„°','ì„ í’ê¸°','ì„ í’ê¸°','ë¯¹ì„œ','ë¯¹ì„œ']

# 2ì°¨ì› ndarrayë¡œ ë³€í™˜
items = np.array(items).reshape(-1, 1)

# ì› í•« ì¸ì½”ë”©ì„ ì ìš©
onehot_encoder = OneHotEncoder()
onehot_encoder.fit(items)
onehot_labels = onehot_encoder.transform(items)

# OneHotEncoderë¡œ ë³€í™˜í•œ ê²°ê³¼ëŠ” í¬ì†Œí–‰ë ¬ì´ë¯€ë¡œ toarray()ë¥¼ ì´ìš©í•´ ë°€ì§‘ í–‰ë ¬ë¡œ ë³€í™˜
print('One-Hot Encoding Data')
print(onehot_labels.toarray())
print('One-Hot Encoding ë°ì´í„° ì°¨ì›')
print(onehot_labels.shape)

# One-Hot Encoding Data
# [[1. 0. 0. 0. 0. 0.]
#  [0. 1. 0. 0. 0. 0.]
#  [0. 0. 0. 0. 1. 0.]
#  [0. 0. 0. 0. 0. 1.]
#  [0. 0. 0. 1. 0. 0.]
#  [0. 0. 0. 1. 0. 0.]
#  [0. 0. 1. 0. 0. 0.]
#  [0. 0. 1. 0. 0. 0.]]
# One-Hot Encoding ë°ì´í„° ì°¨ì›
# (8, 6)
```
8ê°œì˜ ë ˆì½”ë“œì™€ 1ê°œì˜ ì¹¼ëŸ¼ì„ ê°€ì§„ ì›ë³¸ë°ì´í„°ê°€ 8ê°œì˜ ë ˆì½”ë“œì™€ 6ê°œì˜ ì¹¼ëŸ¼ì„ ê°€ì§„ ë°ì´í„°ë¡œ ë³€í™˜ë˜ì—ˆë‹¤. ì²«ë²ˆì§¸ ì¹¼ëŸ¼ì´ TV, ë‘ë²ˆì§¸ ì¹¼ëŸ¼ì´ ëƒ‰ì¥ê³  ë¡œ itemsì— ë„£ì–´ì¤€ ìˆœì„œë¥¼ ë”°ë¼ ì¹¼ëŸ¼ì˜ ìˆœì„œê°€ ë˜ì—ˆë‹¤.

ë°ì´í„°ì˜ ë‘ ë²ˆì§¸ ë ˆì½”ë“œì¸ ëƒ‰ì¥ê³ ë¥¼ í™•ì¸í•˜ë©´, ë³€í™˜ëœ ë°ì´í„°ì˜ ë‘ ë²ˆì§¸ ë ˆì½”ë“œì—ì„œ ëƒ‰ì¥ê³ ì— í•´ë‹¹í•˜ëŠ” ì¹¼ëŸ¼ë§Œ 1 ê°’ìœ¼ë¡œ ë‚˜ë¨¸ì§€ ì¹¼ëŸ¼ì€ 0 ê°’ìœ¼ë¡œ ë³€í™˜ëœê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

### pandas one-hot
íŒë‹¤ìŠ¤ì—ì„œëŠ” `get_dummis()` ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”ìš± ì‰½ê²Œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
```python
import pandas as pd

df = pd.DataFrame({'items' : ['TV', 'ëƒ‰ì¥ê³ ','ì „ìë ˆì¸ì§€','ì»´í“¨í„°','ì„ í’ê¸°','ì„ í’ê¸°','ë¯¹ì„œ','ë¯¹ì„œ']})
pd.get_dummies(df)
```

------

## í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ê³¼ ì •ê·œí™”
ë³€ìˆ˜ëŠ” ìì‹ ë§Œì˜ ê°’ ë²”ìœ„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆì§€ë§Œ ì´ë ‡ê²Œ ì œê°ê°ì¸ ë²”ìœ„ëŠ” ê°’ì— ë”°ë¼ ì˜í–¥ë ¥ì´ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ì•Œê³ ë¦¬ì¦˜ì— ì¹˜ëª…ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ê°’ ë²”ìœ„ë¥¼ ì¼ì •í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë§ì¶”ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ê³  ì´ë¥¼ feature scalingì´ë¼ê³  í•œë‹¤. ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” í‘œì¤€í™”(Standardizaion)ì™€ ì •ê·œí™”(Normalizaion)ê°€ ìˆë‹¤.

í˜¼ì„ ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¼ë°˜ì ì¸ ì˜ë¯¸ì˜ í‘œì¤€í™”ì™€ ì •ê·œí™”ë¥¼ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ í†µì¹­í•˜ê³ , ì„ í˜•ëŒ€ìˆ˜ ê°œë…ì˜ ì •ê·œí™”ë¥¼ ë²¡í„° ì •ê·œí™”ë¡œ ì§€ì¹­

ì‚¬ì´í‚·ëŸ°ì—ëŠ” ëŒ€í‘œì ì¸ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ í´ë˜ìŠ¤ë¡œ `StandardScaler`ì™€ `MinMaxScaler` ê°€ ìˆë‹¤.
 
### í‘œì¤€í™”
ë°ì´í„°ì˜ í”¼ì²˜ ê°ê°ì˜ í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ 1ì¸ ê°€ìš°ì‹œì•ˆ ì •ê·œë¶„í¬ë¥¼ ê°€ì§„ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

```python
def standardize(x) :
    result = (x - x.mean()) / x.std()
    return result

iris_df.apply(standardize)
```

### StandardScaler
í‘œì¤€í™”ë¥¼ ì§€ì›í•˜ëŠ” í´ë˜ìŠ¤. ê°œë³„ í”¼ì²˜ë¥¼ í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ 1ì¸ ê°’ìœ¼ë¡œ ë³€í™˜í•´ì¤€ë‹¤. íŠ¹íˆ ì‚¬ì´í‚·ëŸ°ì—ì„œ êµ¬í˜„í•œ RBF ì»¤ë„ì„ ì´ìš©í•˜ëŠ” ì„œí¬íŠ¸ ë²¡í„°ë¨¸ì‹ (Support Vector Machine) ì´ë‚˜ ì„ í˜•íšŒê·€(Linear Regression), ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression) ì€ ë°ì´í„°ê°€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •í•˜ê³  êµ¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì— ì‚¬ì „ì— í‘œì¤€í™”ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì€ ì˜ˆì¸¡ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•œ ìš”ì†Œê°€ ë  ìˆ˜ ìˆë‹¤.

```python
from sklearn.preprocessing import StandardScaler

# ê°ì²´ ìƒì„±
scaler = StandardScaler()

# StandardScaler ë¡œ ë°ì´í„° ì…‹ ë³€í™˜
iris_scaled = scaler.fit_transform(iris_df)

# transform() ì‹œ ìŠ¤ì¼€ì¼ ë³€í™˜ëœ ë°ì´í„° ì…‹ì´ Numpy ndarray ë¡œ ë°˜í™˜ë˜ì–´ ì´ë¥¼ DFí˜•ìœ¼ë¡œ ë³€í™˜
iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)

# print()
# print
print('feature ë“¤ì˜ í‰ê· ê°’: ', iris_df_scaled.mean())
print()
print('feature ë“¤ì˜ ë¶„ì‚°ê°’: ', iris_df_scaled.var())



# feature ë“¤ì˜ í‰ê· ê°’:  sepal length (cm)   -1.690315e-15
# sepal width (cm)    -1.842970e-15
# petal length (cm)   -1.698641e-15
# petal width (cm)    -1.409243e-15
# dtype: float64

# feature ë“¤ì˜ ë¶„ì‚°ê°’:  sepal length (cm)    1.006711
# sepal width (cm)     1.006711
# petal length (cm)    1.006711
# petal width (cm)     1.006711
# dtype: float64
```



### ì •ê·œí™”(Normalizaion)
ì¼ë°˜ì ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ í”¼ì²˜ì˜ í¬ê¸°ë¥¼ í†µì¼í•˜ê¸° ìœ„í•´ í¬ê¸°ë¥¼ ë³€í™˜í•´ì£¼ëŠ” ê°œë…ì´ë‹¤. ê°œë³„ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ëª¨ë‘ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ, í¬ê¸°ë¥¼ ëª¨ë‘ ë™ì¼í•œ ë‹¨ìœ„ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì´ë‹¤.
![](https://velog.velcdn.com/images/cyhse7/post/59d12ddc-0d61-48c3-83b8-84326ac2b63a/image.png)
```python
def min_max(x) :
    result = (x - x.min()) / (x.max() - x.min())
    return result

iris_df.apply(min_max)

# í˜¹ì€
iris_df.apply(lambda x:(x - x.min()) / (x.max() - x.min()))
```

### MinMaxScaler
ë°ì´í„° ê°’ì„ 0ê³¼ 1ì‚¬ì´ì˜ ë²”ìœ„ê°’ìœ¼ë¡œ ë³€í™˜(ìŒìˆ˜ê°’ì´ ìˆë‹¤ë©´ -1ì—ì„œ 1ê°’ìœ¼ë¡œ ë³€í™˜)
```python
from sklearn.preprocessing import MinMaxScaler

# ê°ì²´ ìƒì„±
scaler = MinMaxScaler()

# MinMaxScaler ë¡œ ë°ì´í„° ì…‹ ë³€í™˜
iris_scaled = scaler.fit_transform(iris_df)

# transform() ì‹œ ìŠ¤ì¼€ì¼ ë³€í™˜ëœ ë°ì´í„° ì…‹ì´ Numpy ndarray ë¡œ ë°˜í™˜ë˜ì–´ ì´ë¥¼ DFí˜•ìœ¼ë¡œ ë³€í™˜
iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)

# print
print('feature ë“¤ì˜ ìµœì†Œê°’: ', iris_df_scaled.min())
print()
print('feature ë“¤ì˜ ìµœëŒ€ê°’: ', iris_df_scaled.max())


# feature ë“¤ì˜ ìµœì†Œê°’: sepal length (cm)    0.0
# sepal width (cm)     0.0
# petal length (cm)    0.0
# petal width (cm)     0.0
# dtype: float64

# feature ë“¤ì˜ ìµœëŒ€ê°’:  sepal length (cm)    1.0
# sepal width (cm)     1.0
# petal length (cm)    1.0
# petal width (cm)     1.0
# dtype: float64
```

### train / test dataì˜ ìŠ¤ì¼€ì¼ë§ ë³€í™˜ ì‹œ ìœ ì˜ì 
Scaler ê°ì²´ ì´ìš©í•´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë³€í™˜ ì‹œ `fit()`, `transform()`, `fit_transform()` ë©”ì†Œë“œë¥¼ ì´ìš©í•œë‹¤.

`fit()` : ë°ì´í„° ë³€í™˜ì„ ìœ„í•œ ê¸°ì¤€ ì •ë³´ ì„¤ì •(ì˜ˆ: ë°ì´í„°ì…‹ì˜ ìµœëŒ€/ìµœì†Œê°’ ì„¤ì • ë“±)
`transform()` : ì„¤ì •ëœ ì •ë³´ë¥¼ ì´ìš©í•´ ë°ì´í„° ë³€í™˜
`fit_transform()` : ë‘ ë©”ì†Œë“œë¥¼ í•œ ë²ˆì— ì ìš©í•˜ëŠ” í•™ìŠµ ë°ì´í„°ë¡œ fit()ì´ ì ìš©ëœ ìŠ¤ì¼€ì¼ë§ ê¸°ì¤€ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•´ì•¼ í•¨

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ë°˜ë“œì‹œ test dataëŠ” train dataì˜ ìŠ¤ì¼€ì¼ë§ ê¸°ì¤€ì— ë”°ë¼ì•¼ í•˜ê¸° ë•Œë¬¸ì—, test dataì— ë‹¤ì‹œ fitì„ ì ìš©í•´ì„œëŠ” ì•ˆëœë‹¤. train dataë¡œ ì´ë¯¸ fit()ì´ ì ìš©ëœ Scaler ê°ì²´ë¥¼ ì´ìš©í•˜ì—¬ `transform()`ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.

âˆ´
1. ê°€ëŠ¥í•˜ë‹¤ë©´ ì „ì²´ ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ë§ ë³€í™˜ì„ ì ìš©í•œ ë’¤ì— trainê³¼ test ë°ì´í„°ë¡œ ë¶„ë¦¬
2. ê·¸ê²ƒì´ ì—¬ì˜ì¹˜ ì•Šë‹¤ë©´ test data ë³€í™˜ì‹œì—ëŠ” fit() ì´ë‚˜ fit_transform()ì„ ì ìš©í•˜ì§€ ì•Šê³ , í•™ìŠµë°ì´í„°ë¡œ ì´ë¯¸ fit()ëœ Scaler ê°ì²´ë¥¼ ì´ìš©í•˜ì—¬ transform() ë³€í™˜í•´ì•¼í•œë‹¤.


----

## íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡
ìºê¸€ì—ì„œ ì œê³µí•˜ëŠ” íƒ€ì´íƒ€ë‹‰ íƒ‘ìŠ¹ì ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì¡´ì ì˜ˆì¸¡ ìˆ˜í–‰. matplotlibê³¼ seabornì„ ì‚¬ìš©í•´ ì‹œê°í™”ë„ ê°™ì´ ì§„í–‰í•˜ë©° ë°ì´í„° ë¶„ì„

```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

titianic_df = pd.read_csv('titanic_train.csv')
titianic_df[:3]
```
![](https://velog.velcdn.com/images/cyhse7/post/f2a9f3a9-2d47-45c7-919c-57aaf1f00113/image.png)

`PassengerId` : íƒ‘ìŠ¹ì ë°ì´í„° ì¼ë ¨ë²ˆí˜¸
`suvived` : ìƒì¡´ ì—¬ë¶€(0=ì‚¬ë§/1=ìƒì¡´)
`Pclass` : í‹°ì¼“ì˜ ì„ ì‹¤ ë“±ê¸‰(1=ì¼ë“±ì„/2=ì´ë“±ì„/3=ì‚¼ë“±ì„)
`sex` : íƒ‘ìŠ¹ì ì„±ë³„
`name` : íƒ‘ìŠ¹ì ì´ë¦„
`Age` : íƒ‘ìŠ¹ì ë‚˜ì´
`sibsp` : ê°™ì´ íƒ‘ìŠ¹í•œ í˜•ì œìë§¤ ë˜ëŠ” ë°°ìš°ì ì¸ì›ìˆ˜
`parch` : ê°™ì´ íƒ‘ìŠ¹í•œ ë¶€ëª¨ë‹˜ ë˜ëŠ” ì–´ë¦°ì´ ì¸ì›ìˆ˜
`ticket` : í‹°ì¼“ë²ˆí˜¸
`fare` : ìš”ê¸ˆ
`cabin` : ì„ ì‹¤ ë²ˆí˜¸
`embarked` : ì¤‘ê°„ ì •ì°©í•­êµ¬(C=Cherbourg/Q=Queenstown/S=Southampton)

```python
titianic_df.info()

>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
```
`RangeIndex` : ì „ì²´ ë¡œìš°ìˆ˜
`Data columns (total 12 columns)` : ì¹¼ëŸ¼ ìˆ˜ 12ê°œ
íŒë‹¤ìŠ¤ì—ì„œì˜ object Typeì€ string íƒ€ì…ìœ¼ë¡œ ë´ë„ ë¬´ë°©

```python
titianic_df.isna().sum()

>
PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64
```
ì‚¬ì´í‚·ëŸ° ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ Null ê°’ì´ ìˆìœ¼ë©´ ì•ˆë˜ê¸° ë•Œë¬¸ì— í™•ì¸í•´ë³´ë‹ˆ, Ageì™€ Cabin, Embarkedì— ê²°ì†ê°’ì´ ìˆëŠ” ê²ƒì´ í™•ì¸ ë˜ì—ˆë‹¤.
```python
# Ageì˜ Null ê°’ì€ í‰ê· ê°’ìœ¼ë¡œ
titianic_df['Age'].fillna(titianic_df['Age'].mean(), inplace = True)

# ë‚˜ë¨¸ì§€ Null ê°’ì€ 'N'ìœ¼ë¡œ ì±„ìš°ê¸°
titianic_df['Cabin'].fillna('N', inplace = True)
titianic_df['Embarked'].fillna('N', inplace = True)

titianic_df.isnull().sum().sum()
# 0
```

`Sex` ë³´ë‹¤ `gender` í‘œí˜„ì„ ë§ì´ ì‚¬ìš©í•´ì„œ ê·¸ëŸ°ê°€ ì»¬ëŸ¼ì´ë¦„ì„ ë°”ê¿”ì£¼ê³  ì‹¶ì—ˆë‹¤.
```python
titianic_df = titianic_df.rename(columns = {'Sex' : 'Gender'})
titianic_df
```

íŠ¹ì´ì ì´ ì—†ëŠ”ì§€ ê°’ ë¶„í¬ë¥¼ ì‚´í´ë³´ê¸°ë¡œ í•œë‹¤
```python
print('gender ë¶„í¬ : \n', titianic_df['Gender'].value_counts())
print()
print('Cabin ê°’ ë¶„í¬ : \n', titianic_df['Cabin'].value_counts())
print()
print('Embarked ê°’ ë¶„í¬ : \n', titianic_df['Embarked'].value_counts())

>
gender ë¶„í¬ : 
male      577
female    314
Name: Gender, dtype: int64

Cabin ê°’ ë¶„í¬ : 
N              687
C23 C25 C27      4
G6               4
B96 B98          4
C22 C26          3
              ... 
E34              1
C7               1
C54              1
E36              1
C148             1
Name: Cabin, Length: 148, dtype: int64

Embarked ê°’ ë¶„í¬ : 
S    644
C    168
Q     77
N      2
Name: Embarked, dtype: int64
```
`isna()`ë¡œ í™•ì¸í–ˆì„ ë•Œë„ ê·¸ë¬ì§€ë§Œ Cabin(ì„ ì‹¤)ì˜ ê²½ìš° Nì´ ë„ˆë¬´ ë§ë‹¤. ë¬´ì—‡ë³´ë‹¤ `C23 C25 C27` ê°’ê³¼ ê°™ì´ ì—¬ëŸ¬ ì„ ì‹¤ì´ í•œêº¼ë²ˆì— í‘œê¸°ëœ ì´ìƒí•œê²½ìš°ê°€(...) ê½¤ ë‚˜ì™”ê¸° ë•Œë¬¸ì— ë“±ê¸‰ì„ ë‚˜íƒ€ë‚´ëŠ” ì• ë¬¸ìë§Œ ì¶”ì¶œí•˜ê¸°ë¡œ í•œë‹¤.
```python
titianic_df['Cabin'] = titianic_df['Cabin'].str[:1]
print(titianic_df['Cabin'][:3])

>
0    N
1    C
2    N
Name: Cabin, dtype: object
```
### ë°ì´í„° íƒìƒ‰
1. ì–´ë–¤ ìœ í˜•ì˜ ìŠ¹ê°ì˜ ìƒì¡´í™•ë¥ ì´ ë” ë†’ì•˜ëŠ”ê°€?

`1-1`
ì„±ë³„ì— ë”°ë¥¸ ìƒì¡´í™•ë¥ 
```python
titianic_df.groupby(['Gender', 'Survived'])['Survived'].count()

# Gender  Survived
# female  0            81
#         1           233
# male    0           468
#         1           109
# Name: Survived, dtype: int64
```
ìƒëŒ€ì ìœ¼ë¡œ ì—¬ì„±ì˜ ìƒì¡´ì´ ë” ë§ì•˜ìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ í•œëˆˆì— ë¹„êµí•´ë³´ê² ë‹¤.
```python
sns.barplot(titianic_df, x='Gender', y='Survived')
```
![](https://velog.velcdn.com/images/cyhse7/post/89215a23-8406-414c-989d-87176275cd46/image.png)
ì—¬ê¸°ì— ë¶€ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ì˜ˆìƒí•´ë³¼ ìˆ˜ ìˆëŠ” ê°ì‹¤ ë“±ê¸‰ ì¡°ê±´ì„ ì¶”ê°€í•˜ì—¬ ìƒì¡´í™•ë¥ ì„ ì‹œê°í™”í•˜ë©´
```python
sns.barplot(titianic_df, x='Pclass', y='Survived', hue='Gender')
```
![](https://velog.velcdn.com/images/cyhse7/post/446058f7-3343-462e-9135-4c30494113c7/image.png)
1. ëŒ€ì²´ì ìœ¼ë¡œ ì—¬ì„±ì˜ ìƒì¡´ë¥ ì€ ë†’ì•˜ë˜ ë°˜ë©´, ë‚¨ì„±ì˜ ìƒì¡´ë¥ ì€ ë‚®ë‹¤.
2. 1, 2ë“±ê¸‰ ì„ ì‹¤ì˜ ì—¬ì„± ìƒì¡´ë¥ ì€ í¬ê²Œ ì°¨ì´ê°€ ë‚˜ì§€ ì•Šì§€ë§Œ 3ë“±ê¸‰ ì„ ì‹¤ì˜ ê²½ìš° í›… ë–¨ì–´ì¡Œë‹¤.
3. ë‚¨ì„±ì˜ ê²½ìš° ê·¸ë‚˜ë§ˆ 1ë“±ê¸‰ ì„ ì‹¤ì˜ ìƒì¡´í™•ë¥ ì´ ë†’ë‹¤.

`1-2`
ë‚˜ì´ì— ë”°ë¥¸ ìƒì¡´ í™•ë¥ 
ê°’ì˜ ì¢…ë¥˜ê°€ êµ‰ì¥íˆ ì„¸ì„¸í•˜ê²Œ ë¶„í¬ë˜ì–´ìˆê¸° ë•Œë¬¸ì— ê°„ê²©ì„ ì •í•´ì¤€ ë’¤ í™•ì¸í•´ë³¸ë‹¤.
```python
# ì…ë ¥ëœ ageì— ë”°ë¼ êµ¬ë¶„ê°’ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ì‘ì„±
def get_category(age) :
    cat = ''
    if age <= -1 : cat = 'Unknown'
    elif age <= 5 : cat = 'Baby'
    elif age <= 12 : cat = 'Child'
    elif age <= 18 : cat = 'Tennager'
    elif age <= 25 : cat = 'Student'
    elif age <= 35 : cat = 'Young Adult'
    elif age <= 60 : cat = 'Adult'
    
    return cat
```
ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
```python
plt.figure(figsize = (10, 6))
group_names = ['Unknown', 'Baby', 'Child', 'Tennager', 'Student', 'Young Adult', 'Adult', 'Elderly']

titianic_df['Age_cat'] = titianic_df['Age'].apply(lambda x : get_category(x))
sns.barplot(x = 'Age_cat', y = 'Survived', hue = 'Gender', data = titianic_df, order = group_names)
```
![](https://velog.velcdn.com/images/cyhse7/post/7766b8d5-182e-4ddf-a246-3e3f14fdc289/image.png)

1. ì—¬ì„±ì˜ ê²½ìš° Childë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ì—°ë ¹ëŒ€ë“¤ì€ ê³ ë¥´ê²Œ ìƒì¡´ë¥ ì´ ë†’ë‹¤.
2. ì—¬ì„± Elderlyì˜ ê²½ìš° ë‹¤ë¥¸ ì—°ë ¹ëŒ€ë³´ë‹¤ë„ ë†’ë‹¤.
3. ë‚¨ì„±ì˜ ê²½ìš° Babyì˜ ê²½ìš°ê°€ ê°€ì¥ ë†’ê³  ê·¸ ë‹¤ìŒì€ Child ì´ì™¸ì˜ ì—°ë ¹ëŒ€ë“¤ì€ ê³ ë¥´ê²Œ ë‚®ì€ ëª¨ìŠµ í™•ì¸.

```python
# ì‚¬ìš©í•´ì¤€ 'Age_cat' ì¹¼ëŸ¼ì€ ì‚­ì œí•´ì¤€ë‹¤
titianic_df.drop('Age_cat', axis=1, inplace=True)
```

ë¬¸ìì—´ ì¹´í…Œê³ ë¦¬ë¥¼ `LabelEncoder` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸” ì¸ì½”ë”©ì„ ì ìš©í•´ ìˆ«ìí˜• ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜í•œë‹¤.
```python
from sklearn.preprocessing import LabelEncoder

def encode_features(dataDF) :
    # ë³€í™˜í•´ì¤„ ì¹¼ëŸ¼ ëª…
    features = ['Cabin', 'Gender', 'Embarked']
    for feature in features :
        le = LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])
        
    return dataDF

titianic_df = encode_features(titianic_df)
titianic_df[:3]
```
![](https://velog.velcdn.com/images/cyhse7/post/5701a2b7-e962-4f94-b695-694302786928/image.png)
```python
# forë¬¸ì„ ëŒë¦¬ì§€ ì•Šê³  í•˜ë‚˜í•˜ë‚˜ ì§€ì •í•´ì¤˜ë„ ê´œì°®ë‹¤
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
titianic_df['Cabin'] = encoder.fit_transform(titanic['Cabin']).tolist()
titianic_df['Gender'] = encoder.fit_transform(titanic['Gender']).tolist()
titianic_df['Embarked'] = encoder.fit_transform(titanic['Embarked']).tolist()
```

### ë°ì´í„° ì „ì²˜ë¦¬
ë°ì´í„°ë¥¼ ì‚´í´ë³´ë©° í•´ì¤¬ë˜ ì „ì²˜ë¦¬ë“¤ì„ í•œë²ˆì— ëª¨ì•„ ì²˜ë¦¬í•´ì£¼ëŠ” í•¨ìˆ˜ `transform_features(df)` ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤.
```python
# Null ì²˜ë¦¬ í•¨ìˆ˜
def fillna(df) :
    df['Age'].fillna(titianic_df['Age'].mean(), inplace = True)
    df['Cabin'].fillna('N', inplace = True)
    df['Embarked'].fillna('N', inplace = True)
    df['Fare'].fillna(0, inplace = True)
    return df

# ì•Œê³ ë¦¬ì¦˜ì— ë¶ˆí•„ìš”í•œ í”¼ì²˜ ì œê±°
def drop_features(df) :
    df.drop(['PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)
    return df

# ë ˆì´ë¸” ì¸ì½”ë”© ìˆ˜í–‰
def format_features(df) :
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features :
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# ìœ„ì—ì„œ ì„¤ì •í•œ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì„ ëª¨ë‘ í˜¸ì¶œí•˜ê² ë‹¤.
def transform_features(df) :
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
```
ì´í›„ ì •ë‹µê°’ì´ë¼ í•  ìˆ˜ ìˆëŠ” `Survived` ì†ì„±ë§Œ ë³„ë„ë¡œ ë¶„ë¦¬í•´ í´ë˜ìŠ¤ ê²°ì •ê°’ ë°ì´í„° ì…‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
```python
# ë°ì´í„°ë¥¼ ì‚´í´ë³´ë©° ê°’ì´ ë³€í˜•ë˜ì—ˆì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ë°ì´í„° ì¬ë¡œë”©
titianic_df = pd.read_csv('titanic_train.csv')

# feature data setì™€ data label set ì¶”ì¶œ
y_titanic_df = titianic_df['Survived']
x_titanic_df = titianic_df.drop('Survived', axis = 1)

X_titanic_df = transform_features(x_titanic_df)
```

### train_test_split()
train data setì„ ê¸°ë°˜ìœ¼ë¡œ `train_test_split()`ë¥¼ ì´ìš©í•´ ë³„ë„ì˜ test data setì„ ì¶”ì¶œí•œë‹¤. ë¹„ìœ¨ì€ 20%ë¡œ í•œë‹¤.
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size = 0.2, random_state=11)
```

### ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ìƒì¡´ìì˜ˆì¸¡
ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì¸ ê²°ì • íŠ¸ë¦¬, ëœë¤ í¬ë ˆìŠ¤íŠ¸, ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì´ìš©í•´ ìƒì¡´ìë¥¼ ì˜ˆì¸¡í•´ë³¸ë‹¤.
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# random_state = 11 / ì˜ˆì œë¥¼ ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ ê°™ì€ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•œ ìš©ë„
# ê²°ì •íŠ¸ë¦¬ 
dt_clf = DecisionTreeClassifier(random_state=11)
# random_Forest
rf_clf = RandomForestClassifier(random_state=11)
# ë¡œì§€ìŠ¤í‹± íšŒê·€
# solver='liblinear'
# ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ liblinearë¡œ ì„¤ì •í•˜ê² ë‹¤
# ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ë°ì´í„° ì…‹ì—ì„œì˜ ì´ì§„ ë¶„ë¥˜ì— ì¢‹ì€ ì„±ëŠ¥
lr_clf = LogisticRegression(solver='liblinear')


# DecisionTreeClassifier
# í•™ìŠµ
dt_clf.fit(X_train, y_train)
# ì˜ˆì¸¡
dt_pred = dt_clf.predict(X_test)
# í‰ê°€
print('DecisionTreeClassifier ì •í™•ë„ : {0:.4f}'.format(accuracy_score(y_test, dt_pred)))


#random_Forest í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€
rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)
print('random_Forest ì •í™•ë„ : {0:.4f}'.format(accuracy_score(y_test, rf_pred)))


# ë¡œì§€ìŠ¤í‹±
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
print('ë¡œì§€ìŠ¤í‹± ì •í™•ë„ : {0:.4f}'.format(accuracy_score(y_test, lr_pred)))
```
ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤
`DecisionTreeClassifier ì •í™•ë„ : 0.7877`
`random_Forest ì •í™•ë„ : 0.8547`
`ë¡œì§€ìŠ¤í‹± ì •í™•ë„ : 0.8659`

ë°˜ë³µì ì¸ í–‰ìœ„ë¥¼ í•˜ê¸° ì‹«ë‹¤ë©´ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ forë¬¸ìœ¼ë¡œ ë„£ì–´ì¤˜ë„ ë ê±° ê°™ì•˜ë‹¤.
```python
def test(clf) :
    clf.fit(X_train, y_train)
    clf_pred = clf.predict(X_test)
    print('ì •í™•ë„ : {0:.4f}'.format(accuracy_score(y_test, clf_pred)))
    
for i in (dt_clf, rf_clf, lr_clf) :
    test(i)

# ì •í™•ë„ : 0.7877
# ì •í™•ë„ : 0.8547
# ì •í™•ë„ : 0.8659
```
ì •í™•ë„ê°€ ì´ì™€ê°™ì´ 'ë¡œì§€ìŠ¤í‹± íšŒê·€'ê°€ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì— ë¹„í•´ ë¹„êµì  ë†’ê²Œ ë‚˜ì˜¨ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, ì ì€ ë°ì´í„°ì–‘ìœ¼ë¡œ ìµœì í™” ì‘ì—…ì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ì§„í–‰í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— ì„£ë¶ˆë¦¬ í‰ê°€í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì´ë‹¤.

ì´ì œ êµì°¨ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ë“¤ì„ ì¶”ê°€ì ìœ¼ë¡œ í‰ê°€í•´ë³¸ë‹¤.


#### êµì°¨ê²€ì¦ : KFold
```python
from sklearn.model_selection import KFold

# í´ë“œ ì…‹ì´ 5ê°œì¸ KFold ê°ì²´ë¥¼ ìƒì„±
def exec_kfold(clf, folds = 5) :
    kfold = KFold(n_splits = folds)
    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ê°ì²´ ìƒì„±
    scores = []
    
    
    # KFold êµì°¨ ê²€ì¦ ìˆ˜í–‰
    for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)) :
        # X_titianic_df ë°ì´í„°ì—ì„œ êµì°¨ ê²€ì¦ë³„ë¡œ í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ëŠ” index ìƒì„±
        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]
        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]
        
        # Classifier í•™ìŠµ
        clf.fit(X_train, y_train)
        # ì˜ˆì¸¡
        clf_pred = clf.predict(X_test)
        # ì •í™•ë„ ê³„ì‚°
        accuracy = accuracy_score(y_test, clf_pred)
        
        # ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì£¼ê¸°
        scores.append(accuracy)
        print('êµì°¨ê²€ì¦ {0} ì •í™•ë„ : {1:.4f}'.format(iter_count, accuracy))
        
    # 5ê°œì˜ í´ë“œì—ì„œ í‰ê·  ì •í™•ë„ ê³„ì‚°
    mean_score = np.mean(scores)
    print('í‰ê·  ì •í™•ë„ : {0:.4f}'.format(mean_score))
    
# í˜¸ì¶œ
exec_kfold(dt_clf, folds = 5)
```
`êµì°¨ê²€ì¦ 0 ì •í™•ë„ : 0.7542`
`êµì°¨ê²€ì¦ 1 ì •í™•ë„ : 0.7809`
`êµì°¨ê²€ì¦ 2 ì •í™•ë„ : 0.7865`
`êµì°¨ê²€ì¦ 3 ì •í™•ë„ : 0.7697`
`êµì°¨ê²€ì¦ 4 ì •í™•ë„ : 0.8202`
`í‰ê·  ì •í™•ë„ : 0.7823`

1ì°¨ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì•˜ë˜ 'ë¡œì§€ìŠ¤í‹± íšŒê·€'ì˜ ê²°ê³¼ì™€ ë¹„ìŠ·í•˜ê²Œ ë†’ì•˜ë˜ 'ëœë¤í¬ë ˆìŠ¤íŠ¸'ì˜ ê²°ê³¼ë„ ê¶ê¸ˆí•´ì¡Œë‹¤.
```python
print(exec_kfold(lr_clf, folds = 5))
print(exec_kfold(rf_clf, folds = 5))
```
```python
# ë¡œì§€ìŠ¤í‹± ê²°ê³¼
êµì°¨ê²€ì¦ 0 ì •í™•ë„ : 0.7933
êµì°¨ê²€ì¦ 1 ì •í™•ë„ : 0.7921
êµì°¨ê²€ì¦ 2 ì •í™•ë„ : 0.7753
êµì°¨ê²€ì¦ 3 ì •í™•ë„ : 0.7472
êµì°¨ê²€ì¦ 4 ì •í™•ë„ : 0.8427
í‰ê·  ì •í™•ë„ : 0.7901
```
ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš° ì—­ì‹œë‚˜ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ì— ë¹„í•˜ì—¬ ì¡°ê¸ˆ ë” ë†’ê²Œ ë‚˜ì˜¤ê¸°ëŠ” í–ˆì§€ë§Œ í¬ê²Œ ì°¨ì´ë¥¼ ë³´ì´ì§€ ì•ŠëŠ” ëª¨ìŠµì´ ì˜ì™¸ì˜€ê³ ,
```python
# ëœë¤ í¬ë ˆìŠ¤íŠ¸
êµì°¨ê²€ì¦ 0 ì •í™•ë„ : 0.7933
êµì°¨ê²€ì¦ 1 ì •í™•ë„ : 0.8090
êµì°¨ê²€ì¦ 2 ì •í™•ë„ : 0.8371
êµì°¨ê²€ì¦ 3 ì •í™•ë„ : 0.7753
êµì°¨ê²€ì¦ 4 ì •í™•ë„ : 0.8596
í‰ê·  ì •í™•ë„ : 0.8148
```
ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ë˜ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ê²½ìš°ì—ëŠ” í‰ê·  ì •í™•ë„ë§Œ ë´ë„ í™•ì—°í•˜ê²Œ ë†’ì€ ëª¨ìŠµì„ ë³´ì—¬ì¤¬ë‹¤.


#### êµì°¨ê²€ì¦ : cross_val_score
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv = 5)

for i, accuracy in enumerate(scores) :
    print('êµì°¨ê²€ì¦ {0} ì •í™•ë„ : {1:.4f}'.format(i, accuracy))
    
print('í‰ê· ì •í™•ë„ : {0:.4f}'.format(np.mean(scores)))
```
`êµì°¨ê²€ì¦ 0 ì •í™•ë„ : 0.7430`
`êµì°¨ê²€ì¦ 1 ì •í™•ë„ : 0.7753`
`êµì°¨ê²€ì¦ 2 ì •í™•ë„ : 0.7921`
`êµì°¨ê²€ì¦ 3 ì •í™•ë„ : 0.7865`
`êµì°¨ê²€ì¦ 4 ì •í™•ë„ : 0.8427`
`í‰ê· ì •í™•ë„ : 0.7879`


`cross_val_score`ì™€ `KFold` ì˜ í‰ê· ì •í™•ë„ê°€ ì•„ì£¼ ì•½ê°„ì´ì§€ë§Œ ë‹¤ë¥´ë‹¤. ì´ëŠ” cross_val_scoreê°€ `StratifiedDFold`ë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ì´ê²ƒë„ ì•ì„  ì„¸ ê²½ìš°ì˜ ê²°ê³¼ì¹˜ê°€ ê¶ê¸ˆí–ˆê¸° ë•Œë¬¸ì— í•¨ìˆ˜ë¡œ ë¬¶ì–´ì„œ forë¬¸ ëŒë ¤ì¤˜ë´¤ë‹¤.
```python
from sklearn.model_selection import cross_val_score

def cross_val_score_test(clf) : 
    scores = cross_val_score(clf, X_titanic_df, y_titanic_df, cv = 5)
    for i, accuracy in enumerate(scores) :
        print('êµì°¨ê²€ì¦ {0} ì •í™•ë„ : {1:.4f}'.format(i, accuracy))
    print('í‰ê· ì •í™•ë„ : {0:.4f}'.format(np.mean(scores)))

for j in  (dt_clf, rf_clf, lr_clf) :
    cross_val_score_test(j)
    print()



>
DecisionTreeClassifier(random_state=11) ì˜ ê²°ê³¼ì…ë‹ˆë‹¤
êµì°¨ê²€ì¦ 0 ì •í™•ë„ : 0.7430
êµì°¨ê²€ì¦ 1 ì •í™•ë„ : 0.7753
êµì°¨ê²€ì¦ 2 ì •í™•ë„ : 0.7921
êµì°¨ê²€ì¦ 3 ì •í™•ë„ : 0.7865
êµì°¨ê²€ì¦ 4 ì •í™•ë„ : 0.8427
í‰ê· ì •í™•ë„ : 0.7879

RandomForestClassifier(random_state=11) ì˜ ê²°ê³¼ì…ë‹ˆë‹¤
êµì°¨ê²€ì¦ 0 ì •í™•ë„ : 0.7933
êµì°¨ê²€ì¦ 1 ì •í™•ë„ : 0.7978
êµì°¨ê²€ì¦ 2 ì •í™•ë„ : 0.8483
êµì°¨ê²€ì¦ 3 ì •í™•ë„ : 0.7640
êµì°¨ê²€ì¦ 4 ì •í™•ë„ : 0.8652
í‰ê· ì •í™•ë„ : 0.8137

LogisticRegression(solver='liblinear') ì˜ ê²°ê³¼ì…ë‹ˆë‹¤
êµì°¨ê²€ì¦ 0 ì •í™•ë„ : 0.7877
êµì°¨ê²€ì¦ 1 ì •í™•ë„ : 0.7921
êµì°¨ê²€ì¦ 2 ì •í™•ë„ : 0.7753
êµì°¨ê²€ì¦ 3 ì •í™•ë„ : 0.7640
êµì°¨ê²€ì¦ 4 ì •í™•ë„ : 0.8202
í‰ê· ì •í™•ë„ : 0.7879
```
`corss_val_score()` ì„ ì‚¬ìš©í•´ì¤¬ì„ ë•Œ ì—­ì‹œ! `RandomForestClassifier`ì˜ ê²°ê³¼ê°€ ê°€ì¥ ë†’ê²Œ ë‚˜ì™”ë‹¤. ì˜ ë§ëŠ” ëª¨ë¸ì¸ê°€ë³´ë‹¤.



ë§ˆì§€ë§‰ìœ¼ë¡œ `GridSearchCV`ë¥¼ ì´ìš©í•˜ì—¬ ì„¸ ëª¨ë¸ì˜ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ë³´ì.
```python
# cv = 5ë¡œ ê³ ì •
# max_depth, min_samples_split, min_samples_leafë¥¼ ë³€ê²½í•˜ë©´ì„œ ì„±ëŠ¥ì„ ì¸¡ì •
from sklearn.model_selection import GridSearchCV

# íŒŒë¼ë¯¸í„° ì„¤ì •
param = {
    'max_depth' : [2, 3, 5, 10],
    'min_samples_split' : [2, 3, 5], 
    'min_samples_leaf' : [1, 5, 8]
    }

# í•¨ìˆ˜ ìƒì„±
def grid_clf(clf) :
    grid_clf = GridSearchCV(clf, param_grid = param, scoring='accuracy', cv = 5)
    grid_clf.fit(X_train, y_train)
    
    # ì¶œë ¥ì‹œ ì–´ë–¤ ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ê³  ì‹¶ì–´ì„œ ì¶”ê°€
    s = str(clf).split('(')    
    print(f'{s[0]} ìµœì  í•˜ì´í¼ íŒŒë¦¬ë¯¸í„° : ', grid_clf.best_params_)
    print(f'{s[0]} ìµœê³  ì •í™•ë„ : ', round(grid_clf.best_score_, 4))
    best_clf = grid_clf.best_estimator_
    
    # GridSearchCVì˜ ìµœì  í•˜ì´í¼ íŒŒë¦¬ë¯¸í„°ë¡œ í•™ìŠµëœ Estimatorë¡œ ì˜ˆì¸¡ ë° í‰ê°€
    good_pred = best_clf.predict(X_test)
    accuracy = accuracy_score(y_test, good_pred)
    print('í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œì˜ ì •í™•ë„ : {0:.4f}'.format(accuracy))
    

# ì‹¤í–‰í•´ë³´ê¸°....ë‘ê·¼
for j in (dt_clf, rf_clf) :
    grid_clf(j)
    

    
>
DecisionTreeClassifier ìµœì  í•˜ì´í¼ íŒŒë¦¬ë¯¸í„° :  {'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 2}
DecisionTreeClassifier ìµœê³  ì •í™•ë„ :  0.7992
í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œì˜ ì •í™•ë„ : 0.8715
RandomForestClassifier ìµœì  í•˜ì´í¼ íŒŒë¦¬ë¯¸í„° :  {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}
RandomForestClassifier ìµœê³  ì •í™•ë„ :  0.8146
í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œì˜ ì •í™•ë„ : 0.8827
```
ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸ë§Œ ëŒë¦° ê²°ê³¼ì´ë‹¤. ë¹„ë“±í•˜ê²Œ ë†’ì•˜ìœ¼ë‚˜ `RandomForestClasifier` ê°€ ì¡°ê¸ˆ ë” ë†’ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ê·¸ëŸ°ë° 'ë¡œì§€ìŠ¤í‹± íšŒê·€'ì˜ ê²½ìš° ì˜¤ë¥˜ê°€ ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤. ì™œì¼ê¹Œ...?

-----

### ë¡œì§€ìŠ¤í‹± íšŒê·€
ì´ë¦„ì€ íšŒê·€ì¸ë° ë¶„ë¥˜ì— ì‚¬ìš©ëœë‹¤. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ = ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜

ì‚¬ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„° ì˜µì…˜ê°’ì´ ë‹¬ë¼ì„œ ì•ˆë˜ì—ˆë˜ ê²ƒì´ë‹¤!!
```python
params = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 5, 10]}

lrc_grid = GridSearchCV(lr_clf, param_grid = params, scoring='roc_auc', cv = 5)
lrc_grid.fit(X_train, y_train)

print('ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°:', lrc_grid.best_params_)
print('ìµœê³  ì •í™•ë„ : {0:.4f}'.format(lrc_grid.best_score_))

best_lrc = lrc_grid.best_estimator_
# GridSearchCVì˜ ìµœì  í•˜ì´í¼ íŒŒë¦¬ë¯¸í„°ë¡œ í•™ìŠµëœ Estimatorë¡œ ì˜ˆì¸¡ ë° í‰ê°€
good_pred = best_lrc.predict(X_test)
accuracy = accuracy_score(y_test, good_pred)
print('í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œì˜ ì •í™•ë„ : {0:.4f}'.format(accuracy))
```
```
ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°: {'C': 5, 'penalty': 'l1'}
ìµœê³  ì •í™•ë„ : 0.8444
í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œì˜ ì •í™•ë„ : 0.8547
```
~~ì†ì´ ì‹œì›í•˜ë„¹~~
