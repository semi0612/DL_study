위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---



## SVD(Singular Value Decomposition)
역시나 PCA와 유사한 행렬 분해 기법을 이용한다. 하지만 PCA의 경우는 정방행렬만 고유벡터로 분해할 수 있는 반면, SVD는 정방행렬 뿐 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있다는 점이 다르다.
![](https://velog.velcdn.com/images/cyhse7/post/1c23a425-e308-40da-9d80-d2d4a022f70c/image.png)
SVD는 m x n 행렬 A를 위와 같이 분해한다
- U : m x m 행렬. 속한 벡터는 특이벡터(Singular Vector)로 서로 직교한다.
- Σ: m x n 행렬로 대각원소(행렬 A의 특이값)만 0이 아니고 나머지 값은 0이다.
- Vt : n x n 행렬로 속한 벡터는 특이벡터(Singular Vector)로 서로 직교한다.

SVD는 특이값 분해로 분리며, 행렬 U와 A에 속한 벡터는 특이벡토(Singular vetor)이다. 모든 특이벡터는 서로 직교하는 성질을 가짐. 즉 Σ0이 위치한 0이 아닌 값이 행렬 A의 특이 값이 되는 것

### 구현하며 이해해보기
넘파이의 SVD를 이용해 연산을 수행하고, 분해가 어떤식으로 진행되는지 간단한 예제로 확인해보자
넘파이의 svd모듈 가져오기
```python
import numpy as np
from numpy.linalg import svd
```
4 4랜덤 행렬 생성
※ 랜덤 행렬인 이유 : 개별 로우의 의존성을 없애기 위해서 이다.
```python
np.random.seed(121)
a = np.random.randn(4, 4)
print(np.round(a, 3))
```
```
[[-0.212 -0.285 -0.574 -0.44 ]
 [-0.33   1.184  1.615  0.367]
 [-0.014  0.63   1.71  -1.327]
 [ 0.402 -0.191  1.404 -1.969]]
```
위에서 생성해준 행렬에 SVD를 적용하여 U와 S(Sigma), Vt를 도출해내보자. SVD분해는 numpy.linalg.svd에 파라미터로 원본 행렬을 입력하면 U행렬, Sigma행렬, V 전치 행렬을 반환해준다.
```python
U, Sigma, Vt = svd(a)
print(U.shape, Sigma.shape, Vt.shape, '\n')
print(f'-----U matrix:\n{np.round(U, 3)}')
print(f'-----Sigma value:\n{np.round(Sigma, 3)}')
print(f'-----V transpose matrix:\n{np.round(Vt, 3)}')
```
```
(4, 4) (4,) (4, 4) 

-----U matrix:
[[-0.079 -0.318  0.867  0.376]
 [ 0.383  0.787  0.12   0.469]
 [ 0.656  0.022  0.357 -0.664]
 [ 0.645 -0.529 -0.328  0.444]]
-----Sigma value:
[3.423 2.023 0.463 0.079]
-----V transpose matrix:
[[ 0.041  0.224  0.786 -0.574]
 [-0.2    0.562  0.37   0.712]
 [-0.778  0.395 -0.333 -0.357]
 [-0.593 -0.692  0.366  0.189]]
```
U행렬과 vt행렬이 4x4 행렬로, Sigma의 경우에는 1차원 행렬인 (4,)로 반환되었다.

분해된 이 U, Sigma, Vt를 이용해 다시 원본 행렬로 복원이 가능한지도 확인해보자. 원본 행렬로의 복원은 이들을 내적하면된다. 유의할 점은 시그마의 경우 0이 아닌 값만 1차원으로 추출했으므로 다시 0을 포함한 대칭 행렬로 변환한 뒤에 내적을 수행해야 한다는 점이다.

```python
# Sigma를 다시 (0을 포함한) 대칭 행렬로 변환
Sigma_mat = np.diag(Sigma)
a_1 = np.dot(np.dot(U, Sigma_mat), Vt)
print(np.round(a_1, 3))
```
```
[[-0.212 -0.285 -0.574 -0.44 ]
 [-0.33   1.184  1.615  0.367]
 [-0.014  0.63   1.71  -1.327]
 [ 0.402 -0.191  1.404 -1.969]]
```
~~오와~~ 제대로 복원이 되었다. 이번에는 데이터 세트가 로우간 의존성이 있을 경우 어떻게 시그마 겂이 변하고, 이에 따른 차원 축소가 진행 될 수 있는지 알아보자.

```python
# 세번째 로우는 '첫번째 로우+두번째 로우'로
a[2] = a[0]+ a[1]
# 4번째 로우는 첫번째 로우와 같다고 추가
a[3] = a[0]
```
로우간 관계성이 매우 높아진 상황에서 이 데이터를 SVD로 다시 분해해보자
```python
U, Sigma, Vt = svd(a)
print(U.shape, Sigma.shape, Vt.shape, '\n')
print(f'Sigma value:\n{np.round(Sigma, 3)}')
```
```
(4, 4) (4,) (4, 4) 

Sigma value:
[2.663 0.807 0.    0.   ]
```
차원은 이전과 같지만 시그마의 값 중 두가지가 0으로 변해버렸다. 이는 선형 독립인 로우 벡터의 개수가 2개라는 의미라고 한다.

### Truncated SVD
Truncated SVD 역시 Full SVD를 축약한 방법으로 Σ의 대각원소 중에 상위 몇 개만 추출하는 방식이다. (연산을 수행한 뒤 원본 행렬을 분해한 U, Sigma, Vt행렬을 반환하지 않는다)

그냥 SVD보다도 더욱 차원을 줄인 방식으로 과정 자체는 별 차이가 없다. 인위적으로 더 작은 차원으로 분해하기 때문에 완벽하게 원본 행렬 복구는 불가능하다.


```python
# TruncatedSVD
from sklearn.decomposition import TruncatedSVD, PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

iris = load_iris()
X_feature = iris.data

# 2개의 주요 컴포넌트로 TruncatedSVD 변환
tsvd = TruncatedSVD(n_components=2)
iris_tsvd = tsvd.fit_transform(X_feature)

# 산점도 2차원으로 TruncatedSVD 변환된 데이터 표현. 품종은 색으로 구분
plt.scatter(x=iris_tsvd[:, 0], y=iris_tsvd[:,1], c=iris.target)
plt.xlabel("TruncatedSVDComponent 1")
plt.ylabel("TruncatedSVDComponent 2")
```
![](https://velog.velcdn.com/images/cyhse7/post/12ae936d-67e4-4a0f-8ffc-b837a84af344/image.png)

위에서는 스케일링 과정을 하지 않았는데, 차이가 눈에 띄게 있을 지 궁금했다.
```python
# 표준 정규 분포로 스케일링
X_scaled = StandardScaler().fit_transform(X_feature)

# 스케일링 된 데이터를 기반으로 TruncatedSVD 변환 수행
tsvd = TruncatedSVD(n_components=2)
iris_tsvd_sc = tsvd.fit_transform(X_feature)

# 산점도 2차원으로 TruncatedSVD 변환된 데이터 표현. 품종은 색으로 구분
plt.scatter(x=iris_tsvd_sc[:, 0], y=iris_tsvd_sc[:,1], c=iris.target)
plt.xlabel("TruncatedSVDComponent 1")
plt.ylabel("TruncatedSVDComponent 2")
```
![](https://velog.velcdn.com/images/cyhse7/post/b99a2744-9dc4-4b55-941f-43ceb8f06159/image.png)

시각화 한 결과만을 봤을 때는 스케일링(표준 정규 분포)을 한 것과 하지 않은 것 사이에 큰 차이가 보이지 않는 것 같았다.

```python
[iris_tsvd_sc == iris_tsvd]
```
```
[array([[False, False],
        [False, False],
        [ True, False],
        [ True, False],
        [False, False],
        [ True, False],
        [False, False],
        [ True, False],
        [False, False],
        [False, False],
        [ True, False],
        [ True, False],
        [False, False],
        [ True, False],
        [ True, False],
        [False, False],
```
그래서 무작정 비교를 하였고 결과는 위와 같이 나오게 되었다. 시각화만 봤을 때는 꽤 같아 보였는데 완전히 같은건 아니구나

사이킷런의 TruncatedSVD와 PCA 클래스 구현을 조금 더 자세히 들여다보면 두 개 클래스 모두 SVD를 이용해 행렬을 분해한다. 시각화를 통해보면 두 개가 거의 동일함을 알 수 있다. 
```python
from sklearn.preprocessing import StandardScaler

iris = load_iris()
X_feature = iris.data
y_label = iris.target

# 스케일링
scaler = StandardScaler()
iris_scaled = scaler.fit_transform(X_feature)

# 스케일링 된 데이터를 기반으로 TSVD 변환 수행
tsvd = TruncatedSVD(n_components=2)
iris_tsvd = tsvd.fit_transform(iris_scaled)

# 스케일링 된 데이터를 기반으로 PCA 수행
pca = PCA(n_components=2)
iris_pca = pca.fit_transform(iris_scaled)

# 시각화
# tsvd 변환을 왼쪽에 pca변환으 오른쪽에 표현
fig, (ax1, ax2) = plt.subplots(figsize=(9, 4), ncols=2)
ax1.scatter(x=iris_tsvd[:,0], y=iris_tsvd[:,1], c=iris.target)
ax2.scatter(x=iris_pca[:,0], y=iris_pca[:,1], c=iris.target)
ax1.set_title('TSVD Transformd')
ax2.set_title('PCA Transformed')
```
![](https://velog.velcdn.com/images/cyhse7/post/056953c2-1787-4cff-998c-01d13ce020f1/image.png)

```python
# 두 방법의 컴포넌트 차이 평균
var1 = (iris_pca - iris_tsvd).mean()
print(f"{var1:.3f}")

# 원본 피처별 컴포넌트 비율 차이 평균
var2 = (pca.components_ - tsvd.components_).mean()
print(f"{var2:.3f}")
```
```
0.000
0.000
```
컴포넌트 값의 차이와 원본 피처별 컴포넌트 비율 값의 차이를 계산해보니 0에 가까운 값으로 두 방법의 결과가 거의 동일함을 알 수 있었다. 즉, 데이터 스케일링을 통해 데이터의 중심이 동일해지면 두 방법은 동일한 변환값을 반환한다.

<br>

데이터 세트가 스케일링으로 데이터 중심이 동일해지면 사이킷런의 SVD와 PCA는 동일한 변환을 수행한다. 이는 PCA가 SVD 알고리즘으로 구현되었음을 의미하지만, PCA는 밀집행렬에 대한 변환만 가능하며 SVD는 희소행렬에 대한 변환도 가능하다

-----------
## NMF(Non-Negative Matrix Factorization)

Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사(Low-Rank Approximation) 방식의 변형이다. 원본 행렬 내의 모든 원소값이 모두 양수라는게 보장되면 두 개의 양수 행렬로 분해 가능하다. 
```
X = WH
```
X가 `m x n` 행렬이라면 W와 H는 각각 `m x p 행렬W`, `p x n 행렬H` 가 되고 여기서 p는 임의의 값으로 상황에 맞게 설정해 줄수 있는 값이다.
```
예를 들어 [4x6 행렬A]가 있다면
[4x2 행렬W]와 [2x6 행렬H]로 분해할 수 있는 것이다.
```
W는 원본행에 대한 잠재 요소를 특성으로 가지며, H는 원본열에 대한 잠재 요소를 특성으로 가진다.

```PYTHON
from sklearn.decomposition import NMF
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# 데이터 불러오기
iris = load_iris()
iris_data = iris.data

# NMF클래스를 이용하여 사용해보기. 2개의 컴포넌트
nmf = NMF(n_components=2)
# 학습
iris_nmf = nmf.fit_transform(iris_data)

# 시각화
plt.scatter(x=iris_nmf[:,0], y=iris_nmf[:,1], c=iris.target)

plt.xlabel('NMF Component 1')
plt.ylabel('NMF Component 2')
```
![](https://velog.velcdn.com/images/cyhse7/post/f83024f5-3ea1-44d0-bbd6-a2eb411dbb8d/image.png)
