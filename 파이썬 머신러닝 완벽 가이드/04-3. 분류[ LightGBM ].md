위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

----

# LightGBM
XGBoost와 동일하게 조기 중단(early stopping)이 가능하다. XGBClassifier와 동일하게 LGBMClassifier의 fit()에 조기 중단 관련 파라미터를 설정해주면 된다.

[장단점]
- XGB보다 학습에 걸리는 시간이 훨씬 적으며 메모리 사용량도 상대적으로 적다
- 카테고리형 피처를 자동 변환하고, 예측 성능에서 큰 차이를 보이지 않는다
- 적은 데이터 셋에 적용할 경우 과적합이 발생하기 쉽다는 것은 아쉬운점(적은 데이터 셋의 기준은 애매하지만 LightGBM의 공식 문서에서는 일반적으로 10,000건 이하의 데이터 셋 정도라고 기술하고 있다.)

[트리기반 알고리즘의 특징]
- 트리의 깊이를 효과적으로 줄이기 위한 균형 트리 분할(Level Wise) 방식을 사용한다. 최대한 균형 잡힌 트리를 유지하면서 분할하기 때문에 깊이가 최소화되며 오버피팅에 보다 강한 구조를 가진다
- 하지만 균형을 맞추기 위한 시간이 꽤 걸린다는 단점


[LightGBM의 특징]
- 일반 GBM 계열의 트리 분할 방식과는 다르게 리프 중심 트리 분할(Leaf Wise) 방식을 사용
- 트리의 균형을 맞추지 않기 때문에 비태징적인 규칙 트리가 생성, 최대 손실 값을 가지는 리프 노드를 지속적으로 분할하여 깊이가 증가하게 된다.
- 이렇게 생성된 규칙트리는 학습을 반복할 수록 결국은 균형 트리 분할 방식보다 예측 오류 손실을 최소화할 수 있다는 것이 구현 사상이다.

## 주요 파라미터(사이킷런 기준)
- `n_estimators[default:100]` : GBM과 XGB의 n_estimators와 같은 파라미터
- `learning_rate [defalut: 0.1]`: GBM과 XGB의 학습률(learning_rate)과 같은 파라미터, 일반적으로 n_estimators를 높이고 learning_rate를 낮추면 예측 성능이 향상하지만 마찬가지로 과적합 이슈 및 소요 시간 증가의 문제가 있다.
- `max_depth [default: 1]` : 트리 기반 알고리즘의 max_depth와 같다. 0보다 작은 값을 지정하면 깊이 제한이 없다. LightGBM은 Leaf Wise 방식이므로 깊이가 상대적으로 더 깊어질 수 있다.
- `min_child_samples [default: 20]` : 결정 트리의 min_samples_leaf와 같은 파라미터로 리프 노드가 되기 위해 최소한으로 필요한 샘플 수
- `num_leaves [default: 31]` : 하나의 트리가 가질 수 있는 최대 리프 개수
- `boosting [default: gbdt]` : 부스팅의 트리를 생성하는 알고리즘을 지정하며 'gbdt'는 일반적인 그래디언트 부스팅 결정 트리이며 'rf'는 랜덤 포레스트
- `subsample [default: 1]` : GBM과 XGB의 subsample과 같은 파라미터
- `colsample_bytree [default: 1]` : XGB의 colsample_bytree와 같은 파라미터로 개별 트리를 학습할 때마다 무작위로 선택하는 피처의 비율
- `reg_lambda [default: 0]` :  XGB의 reg_lambda와 같은 파라미터로 L2 regulation 제어를 위한 값이다. 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.
- `reg_alpha [default: 0]` : XGB의 reg_alpha와 같은 파라미터로 L1 regulation 제어를 위한 값이다. 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.
- 학습 태스크 파라미터 `objective` : 최소값을 가져야할 손실함수를 정의한다. XGB의 objective 파라미터와 동일

### 하이퍼 파라미터 튜닝 방안
num_leaves의 개수를 중심으로 `min_child_samples(min_data_in_leaf`, `max_depth`를 함께 조정하면서 모델의 복잡도를 줄이는 것이 기본 튜닝 방안이다.

`learning_rate`	를 작게 하면서 `n_estimators`를 크게 하는 것은 부스팅 계열 튜닝에서 가장 기본적인 튜닝 방안이므로 이를 적용하는 것도 좋다. 물론 `n_estimators`를 너무 크게 하는 것은 과적합으로 오히려 성능이 저하될 수도 있다는 것은 유념해야할 것

## 적용 - 위스콘신 유방암 예측
LightGBM 설치
```python
!pip install lightgbm

# 버젼확인
lightgbm.__version__
# '3.3.3'
```
데이터 가져오기
```python
# LightGBM에 파이썬 패키지인 lightgbm 에서 LGBMClassifier import
from lightgbm import LGBMClassifier

import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

dataset = load_breast_cancer()
dataset
```
데이터 변수에 할당 및 나누기
```python
cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)
cancer_df['target']=dataset.target

X_features = dataset.data
y_label = dataset.target

# 전체 데이터 중 80을 학습용으로 20을 데이터 용으로 추출
X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=11)

# 위에서 만든 X_train, y_train을 다시 쪼개서 90은 학습, 10은 검증용으로 분리
X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=11)
```
학습
```python
# 앞서 XGBoost와 동일하게 n_estimators는 400
lgbm_wrapper = LGBMClassifier(n_estimators=400, learning_rate=0.05)

# Liglightgbm도 XBoost와 동일하게 조기 중단(early_stopping_rounds) 수행 가능
evals = [(X_tr, y_tr), (X_val, y_val)]
lgbm_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric="logloss", eval_set=evals, verbose=True)
```
![](https://velog.velcdn.com/images/cyhse7/post/4f4413f7-0626-4206-bd67-a261214a613f/image.png)
출력을 보면 조기중단 설정으로 인해 150까지만 반복하고 학습을 종료하였다.

LightGBM 예측/평가
```python
# 사용할 사용자 정의 함수
from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix, f1_score, roc_auc_score

def get_clf_eval(y_test, pred = None, pred_proba = None) :
    confusion = confusion_matrix(y_test, pred)    #  오차행렬
    accuracy = accuracy_score(y_test, pred)       # 정확도
    precision = precision_score(y_test, pred)     # 정밀도
    recall = recall_score(y_test, pred)           # 재현율
    
    f1 = f1_score(y_test, pred)      # F1 스코어
    
    # ROC-AUC
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차행렬')
    print(confusion)
    
    print()
    # ROC-AUC 추가
    print(f'정확도:{accuracy:.4f}, 정밀도:{precision:.4f}, 재현율:{recall:.4f}, f1_score:{f1:.4f}, AUC:{roc_auc:.4f}')


# 예측
preds = lgbm_wrapper.predict(X_test)
pred_proba = lgbm_wrapper.predict_proba(X_test)[:,1]

# 평가
get_clf_eval(y_test, preds, pred_proba)
```
```
오차행렬
[[36  2]
 [ 0 76]]

정확도:0.9825, 정밀도:0.9744, 재현율:1.0000, f1_score:0.9870, AUC:1.0000
```
피처별 중요도 시각화하여 보기
```python
# plot_imporance()를 이용하여 feature중요도 시각화
from lightgbm import plot_importance
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 12))
plot_importance(lgbm_wrapper, ax=ax)
```
![](https://velog.velcdn.com/images/cyhse7/post/2f5f3009-11a8-498b-a353-5ee26e541070/image.png)

----

## 베이지안 최적화 기반의 하이퍼 파라미터 튜닝
지금까지는 튜닝을 위해 사이킷런에서 제공하는 Grid Search 방식을 적용했다. 이 방식의 주요한 단점 한가지는, 튜닝해야할 파라미터 개수가 많을 경우 최적화 수행시간이 오랜 걸린다는 것이다. 개별 하이퍼 파라미터 값의 범위가 넓거나 학습 데이터가 대용량일 경우에는 더욱 늘어나게 된다.

베이지안 최적화는 베이지안 확률에 기반을 두고 있는 최적화 기법이다. 구성하는 두가지 중요 요소는 `대체모델(Surrogate Model)`과 `획득 함수(Acquisition function)`이다. 
- 대체모델 : 획득 함수로부터 최적 함수를 예측할 수 있는 입력값을 추천 받은 뒤 이를 기반으로 최적 함수 모델을 개선
- 획득함수 : 개선된 대체 모델을 기반으로 최적 입력값을 계산

대체 모델은 획득 함수가 계산한 하이퍼 파라미터를 입력받으면서 점차적으로 개선되며, 개선된 대체 모델을 기반으로 획득 함수는 더 정확한 하이퍼 파라미터를 계산 할 수 있게 된다.


## HyperOpt 사용해보기
베이지안 최적화를 머신러닝 모델의 하이퍼 파라미터 튜닝에 적용할 수 있게 제공되는 패키지 중 하나이다. `pip install`을 통해 설치할 수 있으며 `HyperOpt`의 주요 로직은 아래와 같다.
```
첫번째는 입력변수명과 입력값의 공간설정
둘째는 목적함수의 설정
마지막으로 목적함수의 반환최솟값을 가지는 최적 입력값 유추
```
입력 변수명과 입력값 검색 공간은 딕셔너리 형태로 설정한다. key값으로 입력 변수명, value값으로 해당 입력 견수의 검색공간이 주어짐
```python
!pip install hyperopt

from hyperopt import hp

# 2개의 입력변수 x,y에 대해서 입력값 검색 공간을 지정한다.+

# -10 ~ 10까지의 1 간격 설정
# 입력변수 x와 -15 ~ 15까지 1 간격으로 입력 변수 y 설정
search_space = {'x' : hp.quniform('x', -10, 10, 1), 'y' : hp.quniform('y', -15, 15, 1)}
```
입력값의 검색 공간을 제공하는 대표적 함수들은 아래와 같다. 함수인자로 들어가는 label은 입력 변수명을 다시 적어주어야 하며 (low : 최소값 / high : 최댓값 / q : 간격)이다.
```
hp.quniform(label, low, hight, q) : label로 지정된 입력값 변수 검색 공간을 최솟값 low에서 최대값 high까지의 q 간격을 가지고 설정

hp.uniform(label, low, high) : 최소값 low에서 최대값 high까지 정규 분포 형태의 검색 공간 설정

hp.randint(label, upper) : 0부터 최대값 upper까지 random한 정수값으로 검색 공간 설정

hp.loguniform(label, low, hight) : exp(uniform(low, high) 값을 반환하며, 반환 값의 log 변환 된 값은 정규 분포 형태를 가지는 검색 공간 설정

hp.choice(label, options) : 검색 값이 문자열 또는 문자열과 숫자값이 섞여있을 경우 설정, options는 리스트나 튜플형태로 제공되며
							hp.choice('tree_criterion', ['gini', 'entropy'])와 같이 설정하면 입력변수 tree_criterion의 값을 gini와 entropy로 설정하여 입력한다.
```
목적함수는 반드시 변수값과 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환하는 구조로 만들어져야한다.
```python
from hyperopt import STATUS_OK

# 목적 함수를 생성, 변수값과 변수 검색 공간을 가지는 딕셔너리를 인자로 받고, 특정 값을 반환
def objective_func(search_space) :
    x = search_space['x']
    y = search_space['y']
    retval = x**2 - 20*y
    
    return retval
```
목적 함수의 반환값이 최소가 될 수 있는 최적의 입력값을 베이지안 최적화 기법에 기반하여 찾아줘야한다. `HyperOpt`는 이런 기능을 수행할 수 있는 함수가 있다.
`fmin(objective, space, algo, max_evals, trials`
```
fn : 위에서 생성한 objective_func와 같은 목적 함수
space : 위에서 생성한 search_space와 같은 검색공간 딕셔너리
algo : 베이지안 최적화 적용 알고리즘. 기본적으로 tpe.suggest이며 이는 HyperOpt의 기본 최적화 알고리즘인 TPE(Tree of Parzen Estimator)을 의미한다
max_evals : 최적 입력값을 찾기 위한 입력값 시도 횟수
trials : 최적 입력값을 찾기 위해 시도한 입력값 및 해당 입력값의 목적함수 반환값 결과를 저장하는데 사용된다. Trials클래스를 객체로 생성한 변수명을 입력
rstate : fmin()을 수행할 때마다 동일한 결과값을 가질 수 있도록 설정하는 랜덤 시드(seed)값
```

```python
from hyperopt import fmin, tpe, Trials

# 입력 결과값을 저장한 Trials 객체값 생성
trial_val = Trials()

# 목적 함수의 최소값을 반환하는 최적 입력 변수값을 5번 입력값 시도(max_evals = 5)로 찾아내보자
best_01 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=5, trials=trial_val
               # rstate는 결과값이 책과 동일하게 만들기 위해 적용한것으로 일반적으로는 잘 이용안함
               , rstate=np.random.default_rng(seed=0))

print('best : ', best_01)
```
```
100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 835.55trial/s, best loss: -224.0]
best :  {'x': -4.0, 'y': 12.0}
```
```python
# max_evals값을 20으로 수행하도록 하면 어떤 최적값을 반환하는지 살펴보기
best_02 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=20, trials=trial_val
               # rstate는 결과값이 책과 동일하게 만들기 위해 적용한것으로 일반적으로는 잘 이용안함
               , rstate=np.random.default_rng(seed=0))

print('best : ', best_02)
```
```
100%|███████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 578.48trial/s, best loss: -296.0]
best :  {'x': 2.0, 'y': 15.0}
```
목적함수의 반환값을 `x**2 - 20*y`로 설정했으므로 x는 0에 가까울수록 y는 15에 가까울수록 반환값이 최소로 근사될 수 있다. 20번으로 설정한 후 결과값이 x는 2 y는 15의 결과가 나왔으므로 꽤 주시할만 한 결과이다.

### HyperOpt를 이용한 XGB 하이퍼 파라미터 최적화
```python
# 아까 불러와줬던 위스콘신 유방암 데이터 셋 사용
dataset
X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=150)
# 위에서 만든 X_train, y_train을 다시 쪼개서 90은 학습, 10은 검증용으로 분리
X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=11)
```
max_dept는 5에서 20까지 1 간격으로, min_child_weight는 1에서 2까지 1간격으로 설정하기 위해 `hp.quniform()`을 사용하고,  colsample_bytree는 0.5에서 1사이, learning_rate는 0.01에서 0.2 사이 정규 분포된 값으로 검색하기 위해 `hp.uniform()`을 사용하겠다.
```python
from hyperopt import hp

# max_dept는 5에서 20까지 1 간격으로, min_child_weight는 1에서 2까지 1간격으로
# colsample_bytree는 0.5에서 1사이, learning_rate는 0.01에서 0.2 사이 정규 분포된 값으로 검색.
xgb_search_space = {'max_depth': hp.quniform('max_depth', 5, 20, 1), 
                    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),
                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),
                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),
                   } 
```
이제 목적함수를 설정한다. 유의할 사항은 두가지인데
- 첫째, 검색 공간에서 목적 함수로 입력되는 모든 인자들은 실수형 값이므로 이들을 XGBoostClassifier의 정수형 하이퍼 파라미터 값으로 설정 할 때는 정수형으로 형변환을 해야한다.
- 두번째, HyperOpt의 목적 함수는 최소값을 반환할 수 있도록 최적화해야하기 때문에 정확도와 같이 값이 클수록 좋은 성능지표를 뽑아낼 경우, -1을 곱한뒤 반환해야 한다.

이를 유의하면서 목적함수를 작성해보도록 하자. 반환값은 교차 검증 기반인 평균 정확도(accuracy)를 사용하겠다. 아쉬운 것은 `cross_val_score()`를 XGBoost나 LightGBM에 적용할 경우 조기 중단(early stopping)이 지원되지 않는다는 점이다. 조기 중단을 위해서는 KFold로 학습과 검증용 데이터 셋을 만들어 직접 교차 검증을 수행해야 함
```python
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
from hyperopt import STATUS_OK

# fmin()에서 입력된 search_space 값으로 입력된 모든 값은 실수형임.
# XGBClassifier의 정수형 하이퍼 파라미터는 정수형 변환을 해줘야 함.
# 정확도는 높을수록 더 좋은 수치임. -1 * 정확도를 곱해서 큰 정확도 값일수록 최소가 되도록 변환
def objective_func(search_space):
    # 수행 시간 절약을 위해 nestimators는 100으로 축소
    xgb_clf = XGBClassifier(n_estimators=100, max_depth=int(search_space['max_depth']),
                            min_child_weight=int(search_space['min_child_weight']),
                            learning_rate=search_space['learning_rate'],
                            colsample_bytree=search_space['colsample_bytree'],
                            eval_metric='logloss')
    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)
    
    # accuracy는 cv=3 개수만큼 roc-auc 결과를 리스트로 가짐. 이를 평균해서 반환하되 -1을 곱함.
    return {'loss':-1 * np.mean(accuracy), 'status': STATUS_OK} 
```
fmin()을 이용해 최적 하이퍼 파라미터를 도출해보자
```python
from hyperopt import fmin, tpe, Trials

trial_val = Trials()
best = fmin(fn=objective_func,
            space=xgb_search_space,
            algo=tpe.suggest,
            max_evals=50,     # 최대 반복 횟수 지정
            trials=trial_val, rstate=np.random.default_rng(seed=9))

print('best : ', best)
```
```
100%|███████████████████████████████████████████████| 50/50 [00:11<00:00,  4.46trial/s, best loss: -0.9517398629022887]
best :  {'colsample_bytree': 0.5036717216371022, 'learning_rate': 0.19808959234346474, 'max_depth': 12.0, 'min_child_weight': 1.0}
```
위 출력결과에서 보이듯 max_depth, min_child_weight가 실수형 값으로 도출되었음을 유의해야한다. 

도출된 최적 하이퍼 파라미터를 이용해 XGBClassifier를 재학습한 후 성능평가를 해보자
```python
xgb_wrapper = XGBClassifier(n_estimators=400,
                            learning_rate=round(best['learning_rate'], 5),
                            max_depth=int(best['max_depth']),
                            min_child_weight=int(best['min_child_weight']),
                            colsample_bytree=round(best['colsample_bytree'], 5)
                           )

evals = [(X_tr, y_tr), (X_val, y_val)]
# 재학습
# verbose=False = 진행되는거 안봐도 괜찮다
xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss', eval_set=evals, verbose=False)
```
![](https://velog.velcdn.com/images/cyhse7/post/c692a4a3-a00e-45dc-942f-d5f6e36d432f/image.png)
```python
# 예측
preds = xgb_wrapper.predict(X_test)
pred_proba = xgb_wrapper.predict_proba(X_test)[:,1]

get_clf_eval(y_test, preds, pred_proba)
```
```오차행렬
[[40  2]
 [ 0 72]]

정확도:0.9825, 정밀도:0.9730, 재현율:1.0000, f1_score:0.9863, AUC:0.9937
``` 
