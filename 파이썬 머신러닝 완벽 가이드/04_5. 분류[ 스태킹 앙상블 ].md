위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---


# 스태킹 앙상블
스태킹(Stacking)은 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서는 앞선 배깅이나 부스팅과 비슷한 점이 있지만, 가장 큰 차이점은 '개별 알고리즘으로 예측한 데이터'를 기반으로 다시 예측을 수행한다는 것이다.

개별 알고리즘으로 예측한 데이터를 메타 데이터 셋으로 만들어 그 데이터를 또 다른 별도의 알고리즘으로 최종 학습을 수행, 테스트를 수행하는 방법이다. 이런식으로 개별 모델의 예측 데이터를 기반으로 학습하고 예측하는 방식을 메타 모델이라고 한다.

현실에서는 잘 적용하지 않는 기법이지만 캐글과 같은 대회처럼 조금이라도 성능 수치를 높이고 싶을 때 사용한다.
![](https://velog.velcdn.com/images/cyhse7/post/b119c280-a171-4333-bc42-292dc0024137/image.png)

2-3개의 개별 모델만을 결합해서는 쉽게 예측 성능을 향상시킬 순 없으며, 스태킹을 적용했다고 하여 반드시 성능 향상이 되리라는 보장도 없다. 일반적으로는 성능이 비슷한 모델을 결합해 좀 더 나은 성능 향상을 도출하기 위해 적용된다.

## 적용 - 위스콘신 유방암 데이터
### 데이터 불러오기 및 나누기
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# 위스콘신 유방암 데이터
cancer_data = load_breast_cancer()

X_data = cancer_data.data
y_label = cancer_data.target

X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0)
```
### ML 모델 불러오기 및 학습
```python
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score 

cancer = load_breast_cancer()
type(cancer)

# 데이터 처리
X_data = cancer.data
y_label = cancer.target

X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0)

# 개별 ML 모델을 위한 Classifier 생성.
knn_clf  = KNeighborsClassifier(n_neighbors=4)
rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)
dt_clf = DecisionTreeClassifier()
ada_clf = AdaBoostClassifier(n_estimators=100)

# 개별 모델들을 학습. 
knn_clf.fit(X_train, y_train)
rf_clf.fit(X_train , y_train)
dt_clf.fit(X_train , y_train)
ada_clf.fit(X_train, y_train)
```
```
AdaBoostClassifier(n_estimators=100)
```
KNN, 랜덤 포레스트, 결정 트리, 에이다부스트를 개별 모델로 설정했다. 학습된 개별 모델들이 각자 반환하는 예측 데이터 셋을 생성하고 개별 모델의 정확도 측정. 
```python
knn_pred = knn_clf.predict(X_test)
rf_pred = rf_clf.predict(X_test)
dt_pred = dt_clf.predict(X_test)
ada_pred = ada_clf.predict(X_test)

print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))
print('결정 트리 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))
print('에이다부스트 정확도: {0:.4f} '.format(accuracy_score(y_test, ada_pred))) 
```
```
KNN 정확도: 0.9211
랜덤 포레스트 정확도: 0.9649
결정 트리 정확도: 0.9123
에이다부스트 정확도: 0.9561
```
이제 개별 예측 결과를 알았으니 이 것들을 하나로 붙여주어야 한다. 옆으로 길게 나와있는 예측값을 세로로 세워서 붙여준다. (위에 그림처럼)
```python
# 개별 예측 결과를 stacking
# 배열화하여 붙여준 결과
pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred]) 
print(pred.shape)
(4, 114)

# ---------
# ---------
# ---------
# --------- 의 모양으로 붙은것을
# | | |
# | | |
# | | |
# | | | 의 모양으로 전치 시켜준것이다.
pred = pred.T
print(pred.shape)
(114, 4)
```
### 최종 학습/예측/평가
```python
# 최종 Stacking 모델을 위한 Classifier 생성
# 최종 모델은 로지스틱으로 하기로 했다.
lr_final = LogisticRegression(C=10)
lr_final.fit(pred, y_test)
final = lr_final.predict(pred)

accuracy_score(y_test, final)
```
```
0.9736842105263158
```
LogisticRegression뒤에 C는 하이퍼 파라미터를 제어하는 옵션인데, 높은 값을 설정할 수록 낮은 강도의 제약조건이 설정되고 낮은 값을 설정할 수록 높은 강도의 제작조건이 설정된다.

### LogisticRegression(C조건)
차이가 궁금했기에 적용해보기로 한다.
```python
lr_final = LogisticRegression(C=0.01).fit(pred, y_test)
final_001 = lr_final.predict(pred)

lr_final = LogisticRegression(C=0.1).fit(pred, y_test)
final_01 = lr_final.predict(pred)

lr_final = LogisticRegression(C=1).fit(pred, y_test)
final_1 = lr_final.predict(pred)

lr_final = LogisticRegression(C=10).fit(pred, y_test)
final_10 = lr_final.predict(pred)

lr_final = LogisticRegression(C=100).fit(pred, y_test)
final_100 = lr_final.predict(pred)

print(f'(C=0.01) : {accuracy_score(y_test, final_001)}')
print(f'(C=0.1) : {accuracy_score(y_test, final_01)}')
print(f'(C=1) : {accuracy_score(y_test, final_1)}')
print(f'(C=10) : {accuracy_score(y_test, final_10)}')
print(f'(C=100) : {accuracy_score(y_test, final_100)}')
```
```
(C=0.01) : 0.9298245614035088
(C=0.1) : 0.9736842105263158
(C=1) : 0.9736842105263158
(C=10) : 0.9736842105263158
(C=100) : 0.9736842105263158
```
결과를 보면 0.01 보다는 0.01 로 설정했을 때 정확도가 높아진 것을 확인할 수 있었지만, 그 이상으로 높인다고 해서 눈에 띄는 변경이 없었다.

C는 Regularization(정규화)의 강도를 결정하는 parameter이다. 값이 낮을수록 계수를 0으로 근사하므로 regularization이 강화되는 데, 값이 클수록 overfitting의 가능성이 증가한다. 따라서 값을 조정하고 확인하면서 적당한 값을 찾는 것이 중요한 옵션으로 보인다.
