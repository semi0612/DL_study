위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

----


## 로지스틱 회귀
로지스틱 회귀는 선형 회귀 방식을 분류에 적용한 알고리즘이다. 일반적인 선형회귀와는 조금 다르다. 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라, 시그모이드(Sigmoid)함수의 최적선을 찾고 이 시그모이드 함수의 반환값을 확률로 간주하며 그 확률에 따라 분류를 결정한다는 점이다.

단순 선형 회귀에서는 목표가 실수값 예측이다. 따라서 선형함수인 `wx + b`를 이용하여 예측할 수 있지만 로지스틱 회귀분석에서는 종속변수가 0또는 1이기 때문에 `y = wx+b`를 통해 예측하는 것은 의미가 없다. 그래서 Odds(오드, 오즈)값을 이용한다.

만약 독립변수가 하나라면 공식은 다음과 같다.
![](https://velog.velcdn.com/images/cyhse7/post/c6ac1a23-cbee-4ce6-9023-8aaf4693ebca/image.png)


이 식을 정리하여 확률에 대해 나타내면 다음과 같다.
![](https://velog.velcdn.com/images/cyhse7/post/6dbc2e77-45b5-4631-b94f-389781d653d5/image.png)

위 식에서 1/1+e(¯x)형태의 함수식을 시그모이드 함수라 한다.

### 구현하며 이해해보기
사이킷런에서는 `LogisticRegression`클래스를 이용하여 로지스틱 회귀를 해볼 수 있다. 앞에서 사용하던 위스콘신 유방암 데이터 셋을 기반으로 실습을 진행해보자

데이터 불러오기
```python
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

# 데이터 불러오기
cancer = load_breast_cancer()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# StaStandardScaler()로 평균이 0, 분산이 1이 나오도록 데이터 분포도 변환
scaler = StandardScaler()
data_scaled = scaler.fit_transform(cancer.data)

# train, test
X_train, X_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)
```
선형회귀 계열인 로지스틱 회귀는 데이터의 정규 분포도에 따라 예측 성능이 영향을 받을 수 있으므로, 데이터에 먼저 정규 분포 형태의 표준 스케일링을 적용한 뒤 데이터 셋을 나누었다.

이제 로지스틱 회귀를 이용해 학습 및 예측을 수행하고, 정확도와 ROC-AUC값을 구해보자. solver의 값을 `lbfgs`로 설정하고 성능을 확인할 것인데, defalt값이 lbfgs이다.
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score

lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)

lr_pred = lr_clf.predict(X_test)
lr_proba = lr_clf.predict_proba(X_test)[:,1].reshape(-1,1)

accuracy = accuracy_score(y_test, lr_pred)
auc = roc_auc_score(y_test, lr_proba)

print(f"accuracy: {accuracy:.4f}")
print(f"auc: {auc:.4f}")'
```
```
accuracy: 0.9766
auc: 0.9947
```

Solver와 max_iter외에 주요 하이퍼 파라미터로는 `penaly`와 `C`가 있다.
- penalty : L1규제와 L2규제를 설정하며, 디폴트는 `l2`이다.
- C : 규제 강도를 조절하는 alpha값의 역수로 C값이 작을수록 alpha는 크고, 규제강도도 커진다.

L1과 L2 규제의 경우 solver설정에 따라 영향을 받는다. Liblinear, saga의 경우 L1, L2 규제가 모두 가능하지만 lbfgs, newton-cg, sag의 경우에는 L2규제만 가능하다.
```python
# 실행시 에러가 나야하는 부분인데도 나지 않아서 보니
# ignore해놓은 상태였다.. 풀어주기
warnings.filterwarnings('default')

from sklearn.model_selection import GridSearchCV

params = {
    "solver" : ['liblinear', 'lbfgs'],
    "penalty": ["l2", "l1"], 
    "C": [0.01, 0.1, 1, 5, 10]
}

lr_clf = LogisticRegression()
grid_cv = GridSearchCV(lr_clf, param_grid = params, scoring="accuracy", cv=3)
grid_cv.fit(cancer.data, cancer.target)

print("최적 하이퍼 파라미터:", grid_cv.best_params_)
print("최적 평균 정확도:", round(grid_cv.best_score_, 4))
```
수많은 경고와 함께 아래가 출력되었다.
```
최적 하이퍼 파라미터: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}
최적 평균 정확도: 0.9561
```
이는 solver가 lbfgs일 때 L1 규제를 지원하지 않음에도 해당 규제값을 입력했기 때문에 나오는 메시지이다.

-----

## 회귀 트리
결정 트리와 크게 다르지 않다. 회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측을 하는 것인데, 리프 노드에서 예측 결정 값을 만드는 과정에 있어 차이가 있다.

결정 트리는 특정 클래스의 레이블을 결정하지만, 회귀 트리는 리프 노드에 속한 데이터 값의 평균값을 구해 회귀 예측 값을 계산한다.

또한 결정트리는 각 영역의 지니계수가 낮은 피처를 기준으로 분할하며, 회귀 트리는 RSS를 가장 잘 줄일수 있는 피처를 기준으로 분할한다.
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score

# 보스턴 데이터
boston = load_boston()
boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)

boston_df['PRICE'] = boston.target
y_target = boston_df['PRICE']
X_data = boston_df.drop(['PRICE'], axis=1,inplace=False)

# RandomForestRegressor
rf = RandomForestRegressor(random_state=0, n_estimators=1000)

neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
rmse_scores  = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

print(f'5 교차 검증의 개별 Negative MSE scores: {np.round(neg_mse_scores, 2)}')
print(f'5 교차 검증의 개별 RMSE scores: {np.round(rmse_scores, 2)}')
print(f'5 교차 검증의 평균 RMSE : {avg_rmse:.3f}')
```
```
5 교차 검증의 개별 Negative MSE scores: [ -7.88 -13.14 -20.57 -46.23 -18.88]
5 교차 검증의 개별 RMSE scores: [2.81 3.63 4.54 6.8  4.34]
5 교차 검증의 평균 RMSE : 4.423
```
랜덤포레스트회귀를 이용해 앞의 선형회귀에서 다룬 보스턴 주택 가격 예측을 수행, RMSE를 구해봤다. 랜덤 포레스트 외에도 XGB, LGBM 등 분류에서 사용한 알고리즘은 회귀 트리로 사용 가능하다.

### 선형회귀와 회귀 트리
```python
warnings.filterwarnings('ignore')

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# boston sample: 100, feature: RM
boston_df_sample = boston_df[['RM','PRICE']].sample(n=100, random_state=0)
X_feature = np.array(boston_df_sample.RM).reshape(-1,1)
y_target = np.array(boston_df_sample.PRICE).reshape(-1,1)

# test data: 4.5 ~ 8.5 , 100개의 데이터
X_test = np.arange(4.5, 8.5, 0.04).reshape(-1,1)

# 모델 객체 생성
lr_reg = LinearRegression()
rf_reg2 = DecisionTreeRegressor(max_depth=2)
rf_reg7 = DecisionTreeRegressor(max_depth=7)

# 학습/예측
lr_reg.fit(X_feature, y_target)
rf_reg2.fit(X_feature, y_target)
rf_reg7.fit(X_feature, y_target)

lr_pred = lr_reg.predict(X_test)
rf_reg2_pred = rf_reg2.predict(X_test)
rf_reg7_pred = rf_reg7.predict(X_test)


# 시각화
```
![](https://velog.velcdn.com/images/cyhse7/post/edf32552-d42c-439d-833e-13a430e96b48/image.png)
선형회귀는 직선으로 예측 회귀선을 표현하지만 회귀 트리의 경우 분할되는 데이터 지점에 따라 꺾이면서 계단 형태로 회귀선을 만든다. 차원이 높아진 경우 학습 데이터의 이상치 까지 학습하면서 복잡한 회귀선을 만든 모습을 확인할 수 있었고, 이경우 과적합이 되기 쉬운 모델이 되었음을 알 수 있다.
