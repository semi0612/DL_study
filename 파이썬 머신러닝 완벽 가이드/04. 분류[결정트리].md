위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---

## 결정트리(Decision Tree)
ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘이다. 분류와 회귀가 모두 가능하며(범주형 연속형 수치를 모두 예측할 수 있다는 말) 범주예측, 즉 분류과정은 학습을 통해 데이터에 있는 규칙을 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것이며, 규칙을 가장 쉽게 표현하는 방법은 `if/else`를 기반으로 나타내는 것이다.
<img src="https://velog.velcdn.com/images/cyhse7/post/60965815-78ec-41e3-af45-a87a9c83b557/image.png" width="600px">
규칙 노드(Decision Node)는 `규칙 조건`이 되고 리프 노드(Leaf Node)는 결정된 클래스 값이며, 새로운 규칙 조건마다 서브 트리(Sub Tree) 가 생성된다. Data Set에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 새로운 규칙 노드가 생성된다.

하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 복잡해진다는 얘기이고 이는 곧 과적합으로 이어질 가능성이 높아진다.
→ 트리의 깊이(Depth)가 깊어질 수록 예측 성능이 저하될 가능성이 있다는 말이다.
<img src="https://velog.velcdn.com/images/cyhse7/post/716d3e62-96da-4453-8716-478194ceed6d/image.png" width="600px">
결정트리를 분할 할때는 최대한 균일하게 데이터 셋을 구성할 수 있도록 분할하는 노력이 필요함. 위 그림에서 균일한 데이터의 순서를 매기자면 정보의 균일도가 높은 C>B>A 순이 될것이다.
```
데이터 C에서는 하나의 공을 뽑았을 때 별다른 예측 정보가 없어도 검은색 공이라고 쉽게 예측해 볼수 있으나
데이터 A에서는 데이터를 판단하기 위해 더 많은 정보가 필요하다
```
이렇게 정보의 균일도는 예측에 영향을 끼치게 되는데, 결정 노드는 정보의 균일도가 높은 데이터 셋을 먼저 선택할 수 있도록 규칙 조건을 만든다. 이러한 '정보의 균일도'를 측정하는 대표적인 방법으로는 `정보 이득` 과 `지니계수`가 있다.

결정트리는 정보의 균일도라는 룰을 기반으로 하기 때문에 알고리즘이 쉽고 직관적이며, 정보의 균일도만 신경쓰면 되기 때문에 (스케일링과 같은) 데이터 전처리 과정에 얽매일 필요가 앖다. 하지만 과적합으로 인해 정확도가 떨어질 확률이 높아 유의 해야한다.
<table>
  <tr style="border-bottom: 1px solid #444444">
    <th>결정 트리의 장점</th>
    <th>결정 트리의 단점</th>
  </tr>
  <tr>
    <td>쉽다. 직관적이다. <br> 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않다.</td>
    <td>과적합으로 알고리즘 성능이 떨어진다. <br> 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요</td>
  </tr>
</table>

<img src = "https://velog.velcdn.com/images/cyhse7/post/02d5126a-e7e0-4025-a6ea-67d669162350/image.png" width="500px">
위와 같이 순도 100%가 될 때까지 분할하며 찾을 수 도 있지만, 이렇게 Train Data 기반 모델의 정확도를 높이기 위해 계속하여 조건을 추가하다보면 결국 트리 깊이가 커지고(깊어지고), 결과적으로는 복잡한 학습 모델에 이르게 된다. 이는 실제 상황(Test Data Set)에 유연하게 대처할 수 없어서 오히려 예측 성능이 떨어지는 결과를 초래할 수 있다.
→ 즉, 한없이 훈련시키지 말고 트리의 크기를 사전에 제한하는 것이 오히려 성능 튜닝에 더 도움을 주는 것이다.

### +) 정보이득과 지니계수
- `정보 이득(Information Gain)`
	- 1에서 엔트로피 지수를 뺀 값으로 `1 - 엔트로피 지수`
	- 엔트로피라는 개념을 기반으로 하며, 주어진 데이터 집합의 혼잡도를 의미한다.
	- 서로 다른 값들이 섞여있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮은 것이다.
    결정 트리는 이 정보 이득 지수로 분할 기준을 정하는데, 정보 이득이 높은 속성을 기준으로 분할한다고 생각하면 되는 것
    
- `지니 계수`
	- 원래 경제학에서 불평등 지수를 나타낼 때 사용
    - 0이 가장 평등하고 1로 갈 수록 불평등함을 나타내는 것
    - 머신러닝에 적용할 때에는 지니 계수가 낮을수록 데이터의 균일도가 높은 것으로 해석해, 낮은 속성을 기준으로 분할한다.

<br><br>

`엔트로피`란 확률변수의 불확실성을 수치로 나타낸 것으로 엔트로피가 높을수록 불확실성이 높다고 볼 수 있다. 아래와 같은 식으로 정의된다.
(Pk = A 영역에 속하는 레코드 가운데에 k 범주에 속하는 레코드 비율)
![](https://velog.velcdn.com/images/cyhse7/post/843638f6-dd4d-412c-b32c-cebfee5e5cf7/image.png)
동전을 던졌을 때 앞/뒷면이 나올 확률이 동일하게 1/2이라면
![](https://velog.velcdn.com/images/cyhse7/post/b9412916-dc3c-406e-9a21-6fa135a7c86c/image.png)
으로 계산해볼 수 있다. 앞면이 나올 확률이 0.25라면 뒷면이 나올 확률이 0.75일거고 이때 엔트로피는
![](https://velog.velcdn.com/images/cyhse7/post/f7c18ac3-9072-4586-9a81-ac67f53e13c6/image.png)

위 아래 같은 크기의 영역으로 분할한 후 엔트로피를 구하면 이전보다 0.2만큼 감소한 것을 확인할 수 있는데, 이는 불확실성의 감소가 데이터의 순도를 증가시키고 이는 정보를 획득하는데 유의미한 결과를 가져온 것이라 볼 수 있다.
<img src="https://velog.velcdn.com/images/cyhse7/post/a3192266-9d98-49d9-86af-11ebd8febc93/image.png" width="500px">
(따라서 모델은 분할하는 것이 분할하기 전보다 더 좋다는 판단을 내리고 데이터를 두 개의 부분집합으로 분할하게 된다.)



→ `한 줄 정리`
엔트로피 지수 : 주어진 데이터 집합의 혼합(혼잡)도를 의미하며 서로 다른 값들이 많이 섞여있을 수록 값이 커지고, 같은 값이 섞여있으면 값이 낮아진다
지니 계수 : 불평등 지수로 값이 0일때 가장 평등하고 1에 가까워질수록 불평등해진다.

모든 데이터가 같은 값을 가지면 엔트로피 지수는 0이 되며, 머신러닝에서는 각 영역의 지니계수가 낮은 속성을 기준으로 분할 한다.

### 분류 결정트리 파라미터
분류를 위한 DecisionTreeClassifer 와 회귀를 위한 DecisionTreeRegressor 모두 파라미터는 동일하다.
<table>
  <tr style="border-bottom: 2px solid #444444">
    <th>파라미터 명</th>
    <th>설명</th>
  </tr>
  <tr style="border-bottom: 0.5px solid lightgray">
    <td>min_samples_split</td>
    <td>✔ 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용됨<br>✔ 디폴트는 2이고 작게 설정할 수록 분할되는 노드가 많아져서 과적합 가능성 증가</td>
  </tr>
  <tr style="border-bottom: 0.5px solid lightgray">
    <td>min_samples_leaf</td>
    <td>✔ 분할이 될 경우 왼쪽과 오른쪽의 브랜치 노드에서 가져야 할 최소한의 샘플 데이터 수<br>✔ 큰 값으로 설정될수록, 분할될 경우가 왼쪽과 오른쪽의 브랜치 노드에서 가져야 할 최소한의 샘플 데이터 수 조건을 만족시키기 어려우므로 노드 분할을 상대적으로 덜 수행함<br>✔ min_samples_split와 유사하게 과적합 제어용도이다. 그러나 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요</td>
  </tr>
  <tr style="border-bottom: 0.5px solid lightgray">
    <td>max_features</td>
    <td>✔ 최적의 분할을 위해 고려할 최대 피처 갯수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행<br>✔ int 형으로 지정하면 대상 피처의 개수, float형으로 지정하면 전체 피처 중 대상 피처의 퍼센트<br>✔ sqrt 는 전체 피처 중 sqrt(전체 피처 갯수) 만큼 선정<br>✔ auto 로 지정하면 sqrt와 동일 <br>✔ 'log'는 전체 피처 중 log2(전체 피처 개수) 선정<br>✔ None은 전체 피처 선정</td>
  </tr>
  <tr>
    <td>max_depth</td>
    <td>✔ 트리의 최대 깊이를 규정<br>✔ 디폴트는 None. 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_samples_splt보다 작아질 때까지 계속 깊이를 증가시킨다<br>✔ 깊이가 깊어지면 min_samples_splt설정대로 최대 분할하여 과적합 할 수 있으므로 적절한 값으로 제어 필요</td>
  </tr>
  <tr>
    <td>max_leaf_nodes</td>
    <td>말단 노드(Leaf)의 최대 개수</td>
  </tr>
</table>


## 결정 트리 모델 실습
실습 데이터는 [UCI 머신러닝 리포지토리(Machine Learning Repository)](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)에서 제공하는 '사용자 행동 인식(Human Activity Recognition)' 데이터 셋으로 한다. 해당 데이터는 30명에게 스마트 폰 센서를 작착한 뒤 사람의 동작과 관련한 여러가지 feature를 수집한 데이터이다.

### 데이터 불러오기
```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.
feature_name_df = pd.read_csv('./UCI HAR Dataset/UCI HAR Dataset/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
feature_name_df[:3]
```
![](https://velog.velcdn.com/images/cyhse7/post/dc9c9145-6d18-4952-81d2-fb31739a6bdc/image.png)
```python
feature_name_df.info()

>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 561 entries, 0 to 560
Data columns (total 2 columns):
 #   Column        Non-Null Count  Dtype 
---  ------        --------------  ----- 
 0   column_index  561 non-null    int64 
 1   column_name   561 non-null    object
dtypes: int64(1), object(1)
memory usage: 8.9+ KB
```
features.txt에는 561개의 index와 name이 저장되어 있음을 확인할 수 있다. 하지만 여기에는 중복된 피처명이 존재하기 때문에 이를 확인하고 변경해주는 작업이 필요함을 느낄 수 있다.
```python
# 중복확인
# column_name으로 그룹지어서 수를 센 후 column_index로 정렬시킨다
temp = feature_name_df.groupby("column_name").count().sort_values(by="column_index", ascending=False)

# 만약 컬럼 인덱스가 1 이상이라면 그 값을 반환해준다
feature_dup = temp[temp["column_index"] > 1]

print("중복된 피처 수:", feature_dup.count()[0])
feature_dup.head()
```
![](https://velog.velcdn.com/images/cyhse7/post/08937f70-011e-4990-9f30-aeaf0d8327fe/image.png)
```python
# 중복된 피처명 정리
def get_new_feature_name_df(old_feature_name_df):
    # column_name으로 그룹지어서 cumcount()로 피처별 중복 존재시 숫자를 부여
    # reset_index()로 column_index를 생성한다
    feature_dup = pd.DataFrame(old_feature_name_df.groupby("column_name").cumcount()).reset_index()

    # features.txt의 column_index는 1부터 시작이기 때문에
    feature_dup.columns = ["column_index", "dup_cnt"]
    feature_dup["column_index"] = feature_dup["column_index"] + 1
    
    
    # column_index를 기준으로 머지 후 중복컬럼명 변경
    new_feature_name_df = pd.merge(old_feature_name_df, feature_dup, how='outer')
    # 만약 x라는 컬럼명이 있다면 x_1, x_2와 같이 변경되도록 한다
    new_feature_name_df['column_name'] = new_feature_name_df.apply(\
                                                                   lambda x: x.column_name + "_" + str(x.dup_cnt)
                                                                   if x.dup_cnt > 0 
                                                                   else x.column_name, axis=1)
    return new_feature_name_df 
```
데이터 불러오는 함수 작성
```python
import pandas as pd

def get_human_dataset( ):
    
    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.
    feature_name_df = pd.read_csv('./UCI HAR Dataset/UCI HAR Dataset/features.txt',sep='\s+',
                        header=None,names=['column_index','column_name'])
    
    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성. 
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용
    X_train = pd.read_csv('./UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt',sep='\s+', names=feature_name )
    X_test = pd.read_csv('./UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt',sep='\s+', names=feature_name)
    
    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여
    y_train = pd.read_csv('./UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt',sep='\s+',header=None,names=['action'])
    y_test = pd.read_csv('./UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt',sep='\s+',header=None,names=['action'])
    
    # 로드된 학습/테스트용 DataFrame을 모두 반환 
    return X_train, X_test, y_train, y_test

# 데이터 불러오기
X_train, X_test, y_train, y_test = get_human_dataset() 
```
그래프 그리기
```python

import seaborn as sns
plt.figure(figsize=(10,5))

frequency = y_train['action'].value_counts()

label = []
for key, value in frequency.to_dict().items():
    label.append(f"{key}: {value}")

plt.pie(frequency,
    startangle = 180,
    counterclock = False,
    explode = [0.03] * len(label),
    autopct = '%1.1f%%',
    labels = label,
    colors = sns.color_palette('pastel', len(label)),
    wedgeprops = dict(width=0.7)
  )


plt.axis('equal')
plt.show()
```
![](https://velog.velcdn.com/images/cyhse7/post/028256a3-fc1e-444b-ab18-b234d0cf01a8/image.png)
feature 레이블 값 확인. 1, 2, 3, 4, 5, 6 총 6개의 값을 가졌고, 특정한 값으로 쏠리기보다는 비교적 고르게 분포되어있음을 확인할 수 있었다.

### 성능평가
하이퍼 파라미터 별 디폴트 성능 평가. 사이킷 런의 DecisionTreeClassifier를 이용해 동작 예측 분류를 수행해보자. 하이퍼 파라미터는 모두 디폴트 값으로 설정해 수행하고, 이때의 하이퍼 파라미터 값을 추출해보겠다.
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 예제 반복 시마다 동일한 예측 결과 도출을 위해 random_state 설정
dt_clf = DecisionTreeClassifier(random_state = 156)
dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred)

print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))
```
`결정 트리 예측 정확도: 0.8548` 로 약 58.48% 의 정확도가 나왔다.
```python
# DecisionTreeClassifier의 하이퍼 파라미터 추출
print('DecisionTreeClassifier 기본 하이퍼 파라미터:\n', dt_clf.get_params())
```

```
DecisionTreeClassifier 기본 하이퍼 파라미터:
 {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 156, 'splitter': 'best'}
```

다음은 GridSearchCV 를 사용하여 성능 평가를 했는데. n_jobs에 따른 차이가 궁금했다. (n_jobs = 디폴트 값은 1이며, 수를 늘릴수록 그만큼 CPU 코어를 사용하여 멀티 프로세스를 진행시킨다. 따라서 속도가 증가한다.. 고 되어있었기에 `%%time`를 사용하여 체크해보기로)
```python
%%time
from sklearn.model_selection import GridSearchCV

params = {
    'max_depth' : [6, 8, 10, 12, 16, 20, 24],
    'min_samples_split' : [16]
    }

# n_jobs = -1 = 있는 cpu 모두 쓰겠다.
grid_cv = GridSearchCV(dt_clf, param_grid = params, scoring='accuracy', cv = 5, verbose = 3, n_jobs = -1)
grid_cv.fit(X_train, y_train)

print(f'GridSearchCV 최고 평균 정확도 수치 : {grid_cv.best_score_:.4f}')
```

```
Fitting 5 folds for each of 7 candidates, totalling 35 fits
GridSearchCV 최고 평균 정확도 수치 : 0.8549
CPU times: total: 3.16 s
Wall time: 33.7 s
```
time이 [3.16 s , 33.7s]로 잡혔다. 다른건 동일한 상황에서 n_jobs만 변경해보자
```python
%%time
from sklearn.model_selection import GridSearchCV

params = {
    'max_depth' : [6, 8, 10, 12, 16, 20, 24],
    'min_samples_split' : [16]
    }

grid_cv = GridSearchCV(dt_clf, param_grid = params, scoring='accuracy', cv = 5, verbose = 3, n_jobs = 1)
grid_cv.fit(X_train, y_train)

print(f'GridSearchCV 최고 평균 정확도 수치 : {grid_cv.best_score_:.4f}')
```
![](https://velog.velcdn.com/images/cyhse7/post/db61178b-14cc-466e-bc22-ea210af064a8/image.png)
어.... 굉장히 뭐가 많이 출력되었다. 마지막에 있는 시간만 확인해보면 [1min 43s, 1min 44s]로 확연히 차이가 나는 걸 확인할 수 있었다!

`GridSearchCV 최고 평균 정확도 수치 : 0.8549` 의 결과값을 통해 약 85.55이 정확도를 보였음을 알 수 있었다.

```python
print("GridSearchCV 최적 하이퍼 파라미터:", grid_cv.best_params_)

>
GridSearchCV 최적 하이퍼 파라미터: {'max_depth': 8, 'min_samples_split': 16}
```

표로 비교, 확인해볼 수도 있었다.
```python
cv_results_df = pd.DataFrame(grid_cv.cv_results_)
cv_results_df[["param_max_depth", "mean_test_score"]]
```
![](https://velog.velcdn.com/images/cyhse7/post/3af85306-1141-467e-92c4-a7161fae8be0/image.png)
