위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---

## 보스턴 주택 가격 예측
사이킷 런의 `LinearRegression`을 이용하여 보스턴 주택 가격을 예측해보자.

`LinearRegression` 클래스는 예측 값과 실제 값의 RSS를 최소화해 OLS(Ordinary Least Squares) 추정 방식으로 구현한 클래스이다. fit() 메서드로 X, y 배열을 입력받으면 회귀계수인 W 를 `coef_` 속성에 저장한다.

### 데이터 로드
```python
import warnings
warnings.filterwarnings('ignore')

from scipy import stats
from sklearn.datasets import load_boston

# 데이터 셋 로드
boston = load_boston()

# boston 데이터 셋 DataFrame 변환
boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)

boston_df
```
![](https://velog.velcdn.com/images/cyhse7/post/efe7877d-5970-4ce5-8423-6222b2b6b3e1/image.png)

### 주택 가격 컬럼에 추가
```python
# boston 데이터 셋의 target 은 주택 가격이다. 이를 price 칼럼으로 DataFrame에 추가
boston_df['PRICE'] = boston.target
print(f'Boston Dataset 크기 : {boston_df.shape}')
boston_df[:3]
```
![](https://velog.velcdn.com/images/cyhse7/post/ba326bb0-fe69-4388-a097-76f203d56c95/image.png)

### 데이터 살펴보기
```python
boston_df.describe()
```
![](https://velog.velcdn.com/images/cyhse7/post/3db5cdfe-9747-4f65-954a-17052d5d1d7f/image.png)
'price' 컬럼이 생긴것을 확인하였고, 크게 튀는 값은 안보인다. 각 컬럼이 회귀 결과에 미치는 영향이 어느정도인지 그래프로 시각화 하여 본다.
```python
import seaborn as sns

# 2개의 행과 4개의 열을 가진 subplots를 이용
# axs는 4x2의 ax를 가짐
fig, axs = plt.subplots(figsize=(16, 8), ncols=4, nrows=2)
lm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRATIO', 'LSTAT', 'RAD']
for i, feature in enumerate(lm_features) :
    row = int(i/4)
    col = i%4
    # seaborn의 regplot를 이용하여 산점도와 선형회귀 직선을 함께 표현
    sns.regplot(x=feature, y='PRICE', data=boston_df, ax=axs[row][col])
```
![](https://velog.velcdn.com/images/cyhse7/post/a70f3c17-7b78-4736-8f27-8d2b13a1fc77/image.png)
`matplotlib.subplots()`은 여러개의 그래프를 한 번에 표현하기 위해 자주 사용된다. 인자로 입력되는 `ncols`는 열 방향으로 위치할 그래프의 개수이며 `nrows`는 행 방향으로 위치할 그래프의 갯수이다.

시각화한 결과 다른 column들 보다 'RM'과 'LSTAT'의 'PRICE'영향도가 가장 두드러지게 나타나고 있음을 확인 할수 있다(선형회귀 직선과 모양이 비슷하게 분포 되어있는 모습)
→ 방의 크기(RM)가 클수록 가격도 증가, LSTAT(하위 계층의 비율)은 적을수록 PRICE가 증가하니, 음의 방향으로 선형성이 보이는 모습이다.

### LinearRegression 클래스를 이용해 회귀 모델 만들기
`metrics` 모듈의 `mean_squared_error()`와 `r2_score()` API를 이용하여 MSE와 R2 SCORE를 측정해보자

```python
# LinearRegression 클래스를 이용해 회귀 모델 만들기
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 x, y로 분리하기
y_target = boston_df['PRICE']
X_data = boston_df.drop(['PRICE'], axis=1, inplace=False)

# train_test_split으로 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)

# 선형회귀 OLS로 학습
lr = LinearRegression()
lr.fit(X_train, y_train)

# 예측
y_preds = lr.predict(X_test)

# mean_squared_error : 평균 오차 계산
mse = mean_squared_error(y_test, y_preds)
# sqrt에 넣어줌 = 평균 제곱근 오차 계산(rmse)
rmse = np.sqrt(mse)

print(f'MSE : {mse:.3f}, RMSE : {rmse:.3f}')
print(f'Variance score:{r2_score(y_test, y_pred=y_preds)}')
```
```
MSE : 17.297, RMSE : 4.159
Variance score:0.757226332313895
```

`LinearRegression`으로 생성한 주택 가격 모델의 `intercept(절편)`의 `coefficients(회귀 계수)` 값을 살펴보자
```python
print(f'절편 값: {lr.intercept_}')
print(f'회귀 계수 값 : {np.round(lr.coef_,1)}')
```
```
절편 값: 40.995595172164435
회귀 계수 값 : [ -0.1   0.1   0.    3.  -19.8   3.4   0.   -1.7   0.4  -0.   -0.9   0.   -0.6]
```
회귀 계수
```python
coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns)
coeff.sort_values(ascending=False)
```
```
RM          3.4
CHAS        3.0
RAD         0.4
ZN          0.1
INDUS       0.0
AGE         0.0
TAX        -0.0
B           0.0
CRIM       -0.1
LSTAT      -0.6
PTRATIO    -0.9
DIS        -1.7
NOX       -19.8
dtype: float64
```

### +) RMSE - mean_squared_error
- RMSE(Root Mean Squared Error)
- 표준편차와 동일하다. 특정 수치에 대한 예측의 정확도를 표현할 때, Accuracy로 판단하기에는 정확도를 올바르게 표기할 수 없어 RMSE 수치로 정확도 판단을 하고는 한다. 일반적으로 해당 수치가 '낮을수록' 정확도가 높다고 판단한다.
- 예측한 값과 실제 환경에서 관찰되는 값의 차이를 다룰 때 많이 사용하는 측도이다.

```python
from sklearn.metrics import mean_squared_error

RMSE = mean_squared_error(y, y_pred)**0.5
#혹은
mse = mean_squared_error(y, y_pred)
RMSE = np.sqrt(mse)
```

### +) 결정계수 - r2_score
'결정계수'는 상관계수를 제곱한 값으로 보면된다. 변수간 영향을 주는 정도 또는 인과 관계의 정도를 정량화 하여 나타낸 수치이며 회귀 분석에서 사용한다. 주어진 데이터들에 기반해 만들어진 선형(linear)이 주어진 데이터들을 얼마나 잘 설명하는가에 대한 수치

`r2_score`은 결정계수를 나타내며 회귀 모델 성능에 대한 평가 지표이다. 회귀모델이 얼마나 '설명력'있느냐를 의미
→ 설명력은 SSR/SST 식으로 확인할 수 있는데 요약하자면 '실제 값의 분산 대비 예측값의 분산 비율'이고
→ 이는 예측모델과 실제 모델이 얼마나 강한 상관관계(Correlated)를 가지는 가 로 생각해볼 수 있다.

----

## 다항 회귀와 과(대)적합/과소적합
다항회귀는 회귀가 독립변수의 단항식이 아니라 2차, 3차 방정식과 같은 다항식으로 표현되는 것을 말한다. 비선형 회귀로 혼동하기 쉽지만, 분명히 선형회귀라는 점을 주의해야한다.

피처의 직선적관계가 아닌 복잡한 다항 관계를 모델링 할 수 있다. 다항식의 차수가 높아질 수록 더욱 복잡한 관계로 모델링이 가능하다는 것인데, 차수(degree)가 높아질 수록 Train데이터에만 잘 맞는 학습이 이루어져 정작 Test Data 환경에서는 오히려 예측 정확도가 떨어지는 현상이 발생할 수 있다(=과적합)

반면 차수가 너무 낮으면 Train 데이터에도 잘 맞지 않는 학습이 이루어 질 수 있는데 이를 과소적합이라 한다.

### 다항 회귀
사이킷런은 `PolynomialFeatures` 클래스를 통해 피처를 Polynomial(다항식) 피처로 변환한다. 해당 클래스는 입력받은 단항식 피처를 입력받은 degree 파라이터에 해당하는 다항식 피처로 변환한다.

단항식을 2차 다항값으로 변환해본다. 변환 형태는 아래와 같다 [단항식→2차 다항식]
![](https://velog.velcdn.com/images/cyhse7/post/53c33a8f-8235-471a-8367-b09df605fd32/image.png)
```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# 다항식으로 변활할 단항식 만들기
X = np.arange(4).reshape(2,2)
print('일차 단항식 계수 피처\n', X)

# (degree=2) : 2차 다항식으로 변환하겠다.
poly = PolynomialFeatures(degree=2)
poly_ftr = poly.fit_transform(X)
print('2차 다항식으로 변환된 계수 피처\n', poly_ftr)
```
```
일차 단항식 계수 피처
 [[0 1]
 [2 3]]
2차 다항식으로 변환된 계수 피처
 [[1. 0. 1. 0. 0. 1.]
 [1. 2. 3. 4. 6. 9.]]
```
3차 다항식으로 변환할 때에도 `degree`부분을 변경해주면 된다. [단항식→3차 다항식]의 변환형태는 아래와 같다.
![](https://velog.velcdn.com/images/cyhse7/post/6ed0c800-e1fb-4c24-96d6-3c428e574ec4/image.png)
```python
poly3 = PolynomialFeatures(degree=3)
poly_ftr_3 = poly3.fit_transform(X)
print('3차 다항식으로 변환된 계수 피처\n', poly_ftr_3)
```
```
3차 다항식으로 변환된 계수 피처
 [[ 1.  0.  1.  0.  0.  1.  0.  0.  0.  1.]
 [ 1.  2.  3.  4.  6.  9.  8. 12. 18. 27.]]
```



피처 변환과 선형 회귀 적용을 각각 별도로 하는 것 보다는 사이킷런의 `Pipeline`객체를 이용해 한 번에 다항 회귀를 구현하는 것이 코드를 더 명료하게 작성하는 방법이다.
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np

def polynomial_func(X) :
    y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3
    return y

# pipeline 객체로 Streamline하게 Polynomial Feature 변환과 Linear Ression연결
model = Pipeline([('poly', PolynomialFeatures(degree=3)),
                 ('linear', LinearRegression())])
x = np.arange(4).reshape(2, 2)
y = polynomial_func(x)

model = model.fit(x, y)

print('Polynomial 절편 \n', np.round(model.named_steps.linear.intercept_, 2))
print('Polynomial 회귀 계수 \n', np.round(model.named_steps['linear'].coef_,2))
```
```
Polynomial 절편 
 1.76
Polynomial 회귀 계수 
 [0.   0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34]
```

### 과적합/과소적합
다항 회귀는 피처의 직선적 관계가 아닌 복잡한 다항 관계를 모델링 할 수 있다.

데이터 만들기
```python
def true_fun(X) :
    # 임의의 값으로 구성된 X값에 대해 코사인 변환 값을 반환
    return np.cos(1.5*np.pi*X)

# X는 0부터 1까지 30개의 임의 값을 샘플링한 데이터
np.random.seed(0)
n_samples = 30
X = np.sort(np.random.rand(n_samples))
```
책에서 약간의 노이즈 변동값을 y값 생성시 더하라고 되어있는데 하지 않으면 어떻게 되는 지 모습을 보려한다.
```python
y = true_fun(X)
plt.scatter(X, y)
```
![](https://velog.velcdn.com/images/cyhse7/post/7c84cbf9-4655-447a-8552-496cd6010aa7/image.png)

그래프가 정말 깔끔하다.
```python
y = true_fun(X)+ np.random.randn(n_samples)*0.1
plt.scatter(X, y)
```
![](https://velog.velcdn.com/images/cyhse7/post/6b624bc7-d49a-4dba-a25f-b9351c25569a/image.png)

이제 생성된 데이터를 가지고 다항식으로 변환 후 예측 결과를 비교하겠다. 0부터 1까지 균일하게 구성된 100개의 테스트용 데이터 셋을 이용해 차수별 회귀 예측 곡선을 그려보자
```python
plt.figure(figsize=(14, 5))
degrees = [1, 4, 15]


# 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교한다.
for i in range(len(degrees)) :
    ax = plt.subplot(1, len(degrees), i+1)
    plt.setp(ax, xticks=(), yticks=())
    
    # 개별 degree 별로 Polynomial 변환
    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
    linear_regression = LinearRegression()
    pipeline = Pipeline([('polynomial_features', polynomial_features),
                        ('linear_regression', linear_regression)])
    pipeline.fit(X.reshape(-1, 1), y)
    
    # 교차 검증으로 다항회귀를 평가
    scores = cross_val_score(pipeline, X.reshape(-1, 1), y, scoring='neg_mean_squared_error', cv=10)
    
    # Pipeline을 구성하는 세부 객체를 접근하는 named_steps['객체명']을 이용해 회귀계수 추출
    coefficients = pipeline.named_steps['linear_regression'].coef_
    
    print(f'\nDegree {degrees[i]} 회귀 계수는 {np.round(coefficients, 2)}')
    print(f'Degree {degrees[i]}, MSE는 {-1*np.mean(scores)}')
    
    # 테스트 데이터 셋에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교한다.
    # 0부터 1까지 테스트 데이터 셋을 100개로 나누어 예측을 수행한다.
    X_test = np.linspace(0, 1, 100)
    
    # 예측값 곡선
    plt.plot(X_test, pipeline.predict(X_test[:,np.newaxis]), label='Model')
    # 실제값 곡선
    plt.plot(X_test, true_fun(X_test), '-', label='True function')
    plt.scatter(X, y, edgecolor='b', s=20, label='Samples')
    
    plt.xlabel('x');
    plt.ylabel('y');
    plt.ylim((-2, 2));
    plt.legend(loc='best')
    plt.title(f'Degree {degrees[i]}\nMSE={-scores.mean():.2e}(+/- {scores.std():.2e})')
    
plt.show()
```
```
Degree 1 회귀 계수는 [-1.49]
Degree 1, MSE는 0.40749914882743843

Degree 4 회귀 계수는 [  2.7  -27.43  38.45 -14.59]
Degree 4, MSE는 0.00782197333136253

Degree 15 회귀 계수는 [-1.46900000e+02  3.70899000e+03 -4.16930500e+04  2.03111560e+05
  1.17495890e+05 -7.23987037e+06  4.55980728e+07 -1.61271287e+08
  3.76483176e+08 -6.09306953e+08  6.90821140e+08 -5.40216956e+08
  2.78152767e+08 -8.49854729e+07  1.16829064e+07]
Degree 15, MSE는 341762296.3729119
```
![](https://velog.velcdn.com/images/cyhse7/post/91905a78-929e-484b-b7bb-d37b743d5c97/image.png)

다항식 차수를 각각 [1, 4, 15]로 변경하면서 예측 결과를 비교했다.
- Degree 1 : 단순 선형 회귀로 실제 코사인 데이터를 예측하기에는 단순하여(직선) 패턴을 잘 파악하지 못한 과소적합 모델이다. MSE는 약 0.407이다.
- Degree 4 : 예측 곡선과 매우 유사하며 MSE는 약 0.007이다. 셋 중 가장 작은 값이 나왔다.
- Degree 15 : 학습 데이터에 과적합 하여 학습 데이터는 잘 예측 했지만 새로운 데이터는 잘 예측하지 못한다. MSE가 압도적으로 크게 나왔다.

---

## 편향-분산 트레이드 오프(Bias-Variance Trade Off)
편향-분산 트레이드 오프는 머신러닝이 극복해야 할 가장 중요한 이슈 중의 하나이다. 차수가 1인 모델은 매우 단순화된 모델로 지나치게 한 방향성으로 치우치는 경향이 있다. 이러한 모양의 모델은 고편향(High Variance)성을 가졌다고 표현한다.

반면 차수가 15와 같은 모델은 학습 데이터 하나하나의 특성을 반영하면서 매우 복잡한 모델이 생성되고, 지나치게 높은 변동성을 가지게 되는데 이러한 모델은 고분산(High Variance)성을 가진다고 표현한다.

![](https://velog.velcdn.com/images/cyhse7/post/4662e1aa-3017-4f55-a157-eed189a54922/image.png)

위의 양궁과녁을 보면 편향과 분산의 의미를 직관적으로 잘 표현하고 있다. Bias가 낮을 수록 정답에 가까운 결과를 내고 Variance가 낮을 수록 예측 값들의 변동성이 적어 한데 모여있는 결과를 보여준다는 것을 알 수 있다.

일반적으로 편향과 분산은 한쪽이 높으면 한쪽이 낮아지는 경향이 있다 → 편향이 높으면 분산은 낮아지고(과소적합) / 분산이 높으면 편향이 낮아진다(과적합)
∴ 무조건 한쪽을 줄인다고 해결되는 문제는 아니다. 오류를 최소화하려면 편향과 분산의 합이 최소가 되는 적당한 지점을 찾아야 한다.

----

## 규제 선형 모델
회귀 모델은 적절히 데이터에 적합하면서도 회귀 계수가 기하급수적으로 커지는 것을 제어할 수 있다면 좋다. 이전까지 선형 모델의 비용함수를 최소화하는 것(예측값과 실제값의 차이를 최소화 하는 것)만 고려했다.
→ 그 결과 학습 데이터에만 지나치게 맞고, 회귀계수가 쉽게 커지는 경향이 발생했다. 이럴경우 변동성이 오히려 심해져서 테스트 데이터 세트에서는 예측 성능이 저하되기가 쉽다.

이를 반영하기 위해 비용함수는 학습 데이터의 잔차 오류값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 서로 균형을 이뤄야 한다. 

규제 선형 모델은 선형회귀 계수에 대한 제약 조건을 추가함으로 과최적화를 막는 방법이다. 모형이 너무 과도하게 최적화되면 모형 계수의 크기도 과도하게 증가하는 경향이 나타나기 때문에, 규제 선형 모델에서 추가하는 제약 조건은 일반적으로 계수의 크기를 제한하는 방법이다.

![](https://velog.velcdn.com/images/cyhse7/post/a9a9f934-27be-4ab0-89d6-211d5e7cd53c/image.png)
alpha는 여기서 학습데이터 적합 정도와 회귀 계수 값의 크기를 제어하는 파라미터로 적절한 값을 설정해야 한다. alpha가 너무 작으면 결귝 `(Error(W)+0)`과 같아지며 없는 것과 다를게 없고, 너무 크면 `a*||W||2/2` 값이 너무 커지게 되기 때문에 이때는 W를 0 또는 매우 작은 값으로 수렴하게 만들어야 한다.

위와 같이 α값을 이용해 회귀 계수 값의 크기를 적절히 조절하며 과적합을 개선하는 방식을 규제(Regularization)이라 부르며 크게 L1방식과 L2방식이 있다.

```
- L1
W의 절대값에 대해 페널티를 부여한다.
L2규제를 적용하면 영향력이 크지 않은 회귀 계수 값을 0으로 변환
라쏘(Lasso)회귀에 적용되어 있다.

- L2
W의 제곱에 대해 페널티를 부여하는 방식
릿지(Ridge) 회귀에 적용되어 있다.
```

### 릿지(Ridge) 회귀
![](https://velog.velcdn.com/images/cyhse7/post/c456b55f-d828-4e67-801e-477f2432e284/image.png)
Ridge 회귀 모형에서는 가중치(회귀 계수)들의 제곱합을 최소화하는 것을 추가적인 제약 조건(L2 규제)로 한다.

사이킷런은 Ridge 클래스를 통해 구혀할 수 있으며 주요 생성 파라미터는 alpha이며, 이는 릿지 회귀의 alpha L2 규제 계수에 해당한다.
```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# alpha=10으로 설정해 릿지 회귀 수행
ridge = Ridge(alpha=10)
neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-1*neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

# (cv = 5) = 5 folds
print(f'5 folds의 개별 Negative MSE scores : {np.round(neg_mse_scores, 3)}')
print(f'5 folds의 개별 RMSE scores : {np.round(rmse_scores, 3)}')
print(f'5 folds의 평균 RMSE : {avg_rmse}')
```
```
5 folds의 개별 Negative MSE scores : [-11.422 -24.294 -28.144 -74.599 -28.517]
5 folds의 개별 RMSE scores : [3.38  4.929 5.305 8.637 5.34 ]
5 folds의 평균 RMSE : 5.518166280868977
```
이번에는 릿지의 알파 값을 [ 0, 0.1, 1, 10, 100 ] 으로 변화시키면서 RMSE와 회귀 계수 값의 변화를 살펴보겠다.
```python
# 릿지에 사용될 alpha 파라미터의 값을 정의
alphas = [0, 0.1, 1, 10, 100]

# 알파 리스트 값을 반복하면서 알파에 따른 평균 RMSE를 구한다
for alpha in alphas :
    ridge = Ridge(alpha = alpha)
    
    # cross_val_score를 이용해 5 folds의 평균 RMSE를 계산
    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
    avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))
    print(f'알파값이 {alpha}일 때, 5 folds의 평균 RMSE : {avg_rmse:.3f}')
```
```
알파값이 0일 때, 5 folds의 평균 RMSE : 5.829
알파값이 0.1일 때, 5 folds의 평균 RMSE : 5.788
알파값이 1일 때, 5 folds의 평균 RMSE : 5.653
알파값이 10일 때, 5 folds의 평균 RMSE : 5.518
알파값이 100일 때, 5 folds의 평균 RMSE : 5.330
```
릿지 회귀는 알파 값이 커질수록 회귀 계수 값을 작게 만든다. 결과적으로 값이 100일때 평균 RMSE가 5.330으로 가장 좋게 나온것을 보면 알수 있다.

```python
# 각 알파에 따른 회귀 계수 값을 시각화 하기 위해 5개의 열로 된 matplot 축 생성
fig, axs = plt.subplots(1,5, figsize=(18,6))
# 각 알파에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성
coeff_df = pd.DataFrame()

# 위에서 설정한 알파 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장을 진행한다
for i, alpha in enumerate(alphas):
    ridge = Ridge(alpha = alpha)
    ridge.fit(X_data, y_target)
    
    # 회귀 계수 데이터프레임
    # 알파에 따른 피처별로 회귀 계수를 Series 변환 후 dataframe 칼럼으로 추가하는 것
    coef_series = pd.Series(data = ridge.coef_, index = X_data.columns)
    colname='alpha:'+str(alpha)
    coeff_df[colname] = coeff
    
    # 회귀 계수 plot
    coef_series = coef_series.sort_values(ascending=False)
    sns.barplot(x = coef_series.values, y = coef_series.index, ax = axs[i])
    
    axs[i].set_title(f"alpha: {alpha}")
    axs[i].set_xlim(-3,6)
    
plt.show()
```
![](https://velog.velcdn.com/images/cyhse7/post/2f7971c7-8915-4d24-ae8f-ac12ceec5ee3/image.png)
alpha 값을 계속 증가시킬수록 회귀 계수 값은 지속적으로 작아지는 모습을 볼 수 있다. 특히 NOX피처의 경우 알파 값을 계속 증가시킴에 따라 회귀 계수가 크게 작아지고 있는 모습이 보인다.

### 라쏘 회귀
![](https://velog.velcdn.com/images/cyhse7/post/49dd2b0b-59bb-4ead-990f-57cb458fead0/image.png)

W의 절대값에 페널티를 부여하는 L1 규제를 선형 회귀에 적용한 회귀이다. L2규제가 회귀 계수의 크기를 감소시키는 데 반해, L1규제는 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거한다. 이러한 측면에서 L1 규제는 적절한 피처만 회귀에 포함시키는 피처 선택의 특성을 가지고 있다.

사이킷 런은 Lasso 클래스를 통해 구현할 수 있으며 주요 생성 파라미터는 alpha이다. 이는 라쏘 회귀의 alpha L1 규제 계수에 해당한다.

```python
from sklearn.linear_model import Lasso, ElasticNet
from sklearn.model_selection import cross_val_score

# 인자로 회귀 모델의 이름, alpha값들의 리스트, 피처 데이터 세트와 타깃 데이터 세트를 받아서
# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수 값들을 DataFrame으로 반환하는 사용자 함수
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True) :
    coeff_df = pd.DataFrame()
    
    # 모델이름 출력
    if verbose :
        print('####', model_name, '####')
    
    # 모델에 따라 설정
    for param in params : 
        if model_name == 'Ridge' : model = Ridge(alpha=param)
        elif model_name == 'Lasso' : model = Lasso(alpha=param)
        elif model_name == 'ElasticNet' : model = ElasticNet(alpha=param, l1_ratio=0.7)
        
        # 교차 검증
        # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출
        neg_mes_scores = cross_val_score(model, X_data_n, y_target_n, scoring='neg_mean_squared_error', cv=5)
        avg_rmse = np.mean(np.sqrt(-1*neg_mes_scores))
        print(f'alpah {param}일 때 5 fold set의 평균 RMSE : {avg_rmse:.3f}')
        
        # 회귀 계수
        model.fit(X_data_n, y_target_n)
        
        if return_coeff :
            # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 칼럼으로 추가
            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)
            colname='alpha'+str(param)
            coeff_df[colname] = coeff    
    return coeff_df


X_data = boston_df.iloc[:,:-1]
y_target = boston_df.iloc[:,-1]

lasso_alphas = [0.07, 0.1, 0.5, 1, 3]
lasso_coef = get_linear_reg_eval("Lasso", params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)
```
```
#### Lasso ####
alpah 0.07일 때 5 fold set의 평균 RMSE : 5.612
alpah 0.1일 때 5 fold set의 평균 RMSE : 5.615
alpah 0.5일 때 5 fold set의 평균 RMSE : 5.669
alpah 1일 때 5 fold set의 평균 RMSE : 5.776
alpah 3일 때 5 fold set의 평균 RMSE : 6.189
```
알파가 0.07일 때 RMSE의 평균이 약 5.612로 가장 작은(좋은)것을 확인 할 수있다.
```
- 릿지 출력값
알파값이 0일 때, 5 folds의 평균 RMSE : 5.829
알파값이 0.1일 때, 5 folds의 평균 RMSE : 5.788
알파값이 1일 때, 5 folds의 평균 RMSE : 5.53
알파값이 10일 때, 5 folds의 평균 RMSE : 5.518
알파값이 100일 때, 5 folds의 평균 RMSE : 5.330
```
어제 봐줬던 릿지보다는 약간 높은 결과인 것 같다.
```python
lasso_coef
```
![](https://velog.velcdn.com/images/cyhse7/post/3382d519-e803-449c-b2a8-d4fa36269728/image.png)
alpha(하이퍼 파라미터)의 크기가 증가함에따라 일부 피처의 회귀 계수는 아예 0으로 바뀌는 모습을 확인할 수 있었다. 회귀계수가 0인 피처는 회귀 식에서 제외되며 결과적으로 피처 선택의 효과를 얻을 수 있다.

---

### Elastic Net
엘라스틱넷 회귀모형은 가중치(회귀계수) 절대값의 합과 제곱합을 동시에 제약조건(L1, L2 규제의 결합)을 가지는 모형이다.
![](https://velog.velcdn.com/images/cyhse7/post/b35a4c53-ac85-406d-85e8-301fb528b743/image.png)
```
λ1, λ2 라는 두 개의 하이퍼 모수를 가진다.
```

alpha값에 따라 회귀 계수의 값이 급격히 변동할 수 있는 것을 완화가기 위하여 L2규제를 라쏘회귀에 추가한 것인데, 단점은 두 규제가 결합되다보니 수행시간이 상대적으로 오래 걸린다는 점이다.
```python
# l1_ratio는 0.7로 고정
elasitic_alphas = [0.07, 0.1, 0.5, 1, 3]
elasitic_coef = get_linear_reg_eval("ElasticNet", params=elasitic_alphas, X_data_n=X_data, y_target_n=y_target)
```
```
#### ElasticNet ####
alpah 0.07일 때 5 fold set의 평균 RMSE : 5.542
alpah 0.1일 때 5 fold set의 평균 RMSE : 5.526
alpah 0.5일 때 5 fold set의 평균 RMSE : 5.467
alpah 1일 때 5 fold set의 평균 RMSE : 5.597
alpah 3일 때 5 fold set의 평균 RMSE : 6.068
```
alpah가 0.5일때 RMSE가 가장 작다(좋다)
```python
elasitic_coef
```
![](https://velog.velcdn.com/images/cyhse7/post/e573e00c-5b9b-434d-ac77-3ccabc5a860f/image.png)
앞서 봤던 Lasso에 비해 alpha값이 커진다고 해서 0이 되는 값이 많지 않다.

----

### 선형 회귀 모델을 위한 데이터 변환
선형회귀 모델과 같은 선형 모델은 일반적으로 피처와 타깃값 간에 선형의 관계가 있다고 가정하고, 이러한 최적의 선형함수를 찾아내 결과값을 예측한다. 또한 피처값과 타깃값의 분포가 정규분포(=평균을 중심으로 종 모양으로 데이터 값이 분포된 형태)를 매우 선호한다.

사이킷런을 이용해 피처 데이터 세트에 적용하는 변환작업은 다음과 같은 방법이 있을 수 있다.

1. StandardScaler 클래스를 이용해 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터 셋으로 변환하거나, MinMaxScaler 클래스를 이용해 최소값이 0이고 최대값이 1인 값으로 정규화를 수행
→ 예측 성능 향상을 크게 기대하기 어려운 경우가 많다
2. 스케일링/정규화를 수행한 데이터 셋에 다시 다항 특성을 적용하여 변환하는 방법. 보통 1번 방법을 통해 예측 성능에 향상이 없을 경우 이와 같은 방법을 적용
→ 피처의 개수가 매우 많을 겨우 다항 변환으로 생성되는 피처의 개수가 기하급수로 늘어나 과적합의 이슈가 발생할 수 있다.
3. 원래 값에 log 함수를 적용하면 보다 정규 분포에 가까운 형태로 값이 분포된다. 이러한 변환을 로그변환이라고 부르는데, 매우 유용한 변환으로 실제 선형 회귀에서는 앞에서 소개한 방법보다는 로그 변환이 훨씬 많이 사용된다.
