위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---

# 정확도(Accuracy)
실제 데이터에서 예측 데이터가 얼마나 같은지를 판단하는 지표
```
	 예측 결과가 동일한 데이터의 건수
Accuracy = --------------------------------
		전체 예측 데이터 건수
```

직관적으로 모델 예측 성능을 나타내는 평가 지표이다. 하지만 이진 분류의 경우 데이터 구성에 따라 성능을 왜곡하는 지표가 될수도 있음

전체 데이터에서 0이 98%를 차지하는 훈련데이터로 모델의 성능을 99%까지 끌어올린다고 하더라도, 이 모델이 1을 제대로 구분할 수 있을지 신뢰할 수 없다는 말이다.

이렇듯 편향적이고 불균형한 label data set으로 구한 정확도는 성능 수치로 사용할 수가 없다. 이러한 한계점을 극복하기 위해 여러가지 분류 지표를 함께 적용하여 머신러닝의 모델 성능을 평가해야한다.

# 오차행렬(혼동 행렬, Confusion Matrix)
이진분류에서는 주로 오차행렬을 통해 성능을 평가하게 된다. 오차행렬은 모델의 예측이 얼마나 헷갈리고 있는지 보여주는 유용한 지표가 된다.

`TN` : True Negative, 예측 결과가 정답과 일치(True)하고, 실제 레이블이 negative(0)인 경우
`TP` : True Positive, 예측 결과가 정답과 일치(True)하고, 실제 레이블이 positive(1)인 경우
`FP` : False Positive, 예측 결과가 정답과 불일치(False)하고, 실제 레이블이 negative(0)인 경우
`FN` : False Negtive, 예측 결과가 정답과 불일치(False)하고, 실제 레이블이 positive(1)인 경우

`※ T/F 값은 예측값과 실제값이 같다/틀리다`
`※ N/P 는 예측 결과값`
![](https://velog.velcdn.com/images/cyhse7/post/75de3a87-0a41-4272-81fe-dd11439203c5/image.png)


위는 분류기의 성능을 여러 방면으로 판단할 수 있는 기반 정보를 제공한다. 이 값들을 조합해 정확도(Accuracy), 정밀도(Precision), 재현도(Recall) 값을 알 수 있기 때문이다.

정확도는 예측값과 실제 값이 얼마나 동일한가에 대한 비율만으로 계산되기 때문에 오차 행렬에서 True에 해당하는 TN과 TP에 매우 많이 좌우된다.

불균형한 데이터 세트에서 정확도보다 더 선호되는 평가 지표는 정밀도(Precision)와 재현율(Recall)이다.
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

print(len(y_test), len(rf_pred))
# 179 179

# 혼동행렬 만들어보기
confusion_matrix(y_test, rf_pred)
# array([[107,  11],
#        [ 15,  46]], dtype=int64)


# 정확도 비교
# accuracy_score() 사용
accuracy_score(y_test, rf_pred)
# 0.8547486033519553

# 혼동행렬에서 나온 값으로 계산
(107+46) / (107+11+15+46)
# 0.8547486033519553
```

----

## 정밀도(Precision) / 재현율(Recall)
`정밀도` : 모델이 Positive로 예측(FP+TP) 중에 실제 값이 Positive인 경우(TP) 의 비율로 Positive 예측의 성능을 더욱 정밀 측정하기 위한 평가 지표
```
					   TP
Precision = 정밀도 = --------
					 FP + TP
```
`재현율` : 실제 값이 Positive인 경우(FN+TP) 중에 모델의 예측이 Positive로 일치한 경우(TP)의 비율
```
				   TP
Recall = 재현율 = --------
				 FN + TP
```

`재현율` 과 `정밀도`는 모두 동일하게 TP를 높이는데 초점을 두고 있지만 재현율은 FN(실제 Positive, 예측 Negative)를 낮추는 곳에, 정밀도는 FP(예측 불일치False하고, 실제 negative)를 낮추는 데에 초점을 맞추고 있다. 
→ 이 같은 특성때문에 서로 보완적인 지표로서 분류의 성능을 평가하는 데 적용된다. 가장 좋은 성능 평가는 재현율과 정밀도 모두 높은 수치를 얻는 것.


### 정밀도/재현율 Trade-off
정밀도와 재현율은 상호 보완적인 평가 지표이기 때문에 어느 한쪽을 강제로 높인다면 다른 한쪽의 수치는 떨어지기 쉬운데 이를 트레이드오프(Trade-off)라 한다.

임계값의 변화는 정밀도와 재현율에 영향을 끼치는데, 낮아질수록 positive로 분류할 확률이 높아지기 때문에 재현율 값이 높아진다. Positive 예측을 많이 하다보니 실제 Positive를 Negative로 예측(FN)하는 횟수가 줄어들기 때문, 반면 실제 Negative를 Positive라고 판단(FP)하는 횟수가 늘어나기 때문에 정밀도는 떨어지게 된다.

이처럼 정밀도와 재현율 성능수치를 한쪽만 참조한다면 극단적으로 수치 조작이 가능하다. 수치가 적절하게 조합되어 분류의 종합적인 성능 평가에 사용될 수 있는 평가 지표가 필요

위에서 사용하던 타이타닉 데이터를 이용하여 예시를 보겠다.
```python
# 오차행렬, 정확도, 정밀도, 재현율을 한꺼번에 계산하는 함수 생성
from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix
def get_clf_eval(y_test , pred):
    # 오차행렬
    confusion = confusion_matrix( y_test, pred)
    # 정확도
    accuracy = accuracy_score(y_test , pred)
    # 정밀도
    precision = precision_score(y_test , pred)
    # 재현율
    recall = recall_score(y_test , pred)
    print('오차 행렬')
    print(confusion)
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(accuracy , precision ,recall))
    print()




# 임계값을 0.4에서 0.6까지 0.05씩 증가시키며 평가 지표 조사
# 테스트를 수행할 모든 임곗값을 리스트 객체로 저장. 
thresholds = [0.4, 0.45, 0.50, 0.55, 0.60]

from sklearn.preprocessing import Binarizer
def get_eval_by_threshold(y_test , pred_proba_c1, thresholds):
    for custom_threshold in thresholds:
        # Binarizer()는 threshold보다 작거나 같으면 0, 크면 1을 반환한다.
        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) 
        
        # binarizerf를 이용하여 transform(fit은 해주지 않는다.)
        custom_predict = binarizer.transform(pred_proba_c1)
        print('임곗값:', custom_threshold)
        get_clf_eval(y_test , custom_predict)        


# predict_proba( ) 를 통하여 개별 레이블별 예측확률을 반환받을 수 있다
# 반환되는 ndarraysms 첫번째 칼럼이 클래스 값 0에 대한 예측확률
# 두번째 칼럼이 클래스 값 1에 대한 예측확률이다
pred_proba = lr_clf.predict_proba(X_test)
get_eval_by_threshold(y_test ,pred_proba[:,1].reshape(-1,1), thresholds )
```
```
임곗값: 0.4
오차 행렬
[[99 19]
 [13 48]]
정확도: 0.8212, 정밀도: 0.7164, 재현율: 0.7869

임곗값: 0.45
오차 행렬
[[108  10]
 [ 14  47]]
정확도: 0.8659, 정밀도: 0.8246, 재현율: 0.7705

임곗값: 0.5
오차 행렬
[[109   9]
 [ 14  47]]
정확도: 0.8715, 정밀도: 0.8393, 재현율: 0.7705

임곗값: 0.55
오차 행렬
[[111   7]
 [ 16  45]]
정확도: 0.8715, 정밀도: 0.8654, 재현율: 0.7377

임곗값: 0.6
오차 행렬
[[112   6]
 [ 17  44]]
정확도: 0.8715, 정밀도: 0.8800, 재현율: 0.7213
```
책과는 다르게 임계값이 0.5인 경우가 적당해보인다. (정확도와 정밀도가 올랐고 재현율은 0.55보다 낮고 0.45와 같다)

## F1 Score
정밀도와 재현율을 결합한 지표이다. 둘 중 어느 한쪽으로 치우치지 않는 수치를 나타낼때 상대적으로 높은 값을 가지며, 정밀도와 재현율의 조화 평균으로 계산된다.
![](https://velog.velcdn.com/images/cyhse7/post/7d8d093f-7454-40b0-a3fb-9b490916dfdc/image.png)

타이타닉 생존자 예측에서 인계값을 변화시키며 F1 스코어를 포함한 평가 지표를 구해보자. 앞서 작성한 `get_clf_eval()` 함수에 F1 스코어를 구하는 로직을 추가한 후 `get_eval_by_threshold()` 함수를 통해 임계값 0.4~0.6 별로 정확도와 정밀도, 재현율, F1스코어를 본다.
```python
# 오차행렬, 정확도, 정밀도, 재현율을 한꺼번에 계산하는 함수 생성
from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix
def get_clf_eval(y_test, pred) :
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    
    # F1 스코어 추가
    f1 = f1_score(y_test, pred)
    print('오차행렬')
    print(confusion)
    
    # f1 score print
    print('정확도 : {0:.4f}, 정밀도:{1:.4f}, 재현율 : {2:.4f}, F1:{3:.4f}'.format(accuracy, precision, recall, f1))
    print()
    
thresholds = [0.4, 0.45, 0.50, 0.55, 0.60]
pred_proba = lr_clf.predict_proba(X_test)
get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1, 1), thresholds)
```
```
임곗값: 0.4
오차행렬
[[99 19]
 [13 48]]
정확도 : 0.8212, 정밀도:0.7164, 재현율 : 0.7869, F1:0.7500

임곗값: 0.45
오차행렬
[[108  10]
 [ 14  47]]
정확도 : 0.8659, 정밀도:0.8246, 재현율 : 0.7705, F1:0.7966

임곗값: 0.5
오차행렬
[[109   9]
 [ 14  47]]
정확도 : 0.8715, 정밀도:0.8393, 재현율 : 0.7705, F1:0.8034

임곗값: 0.55
오차행렬
[[111   7]
 [ 16  45]]
정확도 : 0.8715, 정밀도:0.8654, 재현율 : 0.7377, F1:0.7965

임곗값: 0.6
오차행렬
[[112   6]
 [ 17  44]]
정확도 : 0.8715, 정밀도:0.8800, 재현율 : 0.7213, F1:0.7928
```
위에서 했던 예상과 같이 임계값이 0.5일때 F1 스코어가 가장 좋은 값을 보여주고 있다. ~~기분좋다~~

-----

## ROC 곡선과 AUC
`ROC 곡선`
- Receiver Operation Characteristic Curve
- 우리말로 '수신자 판단 곡선' ~~이라는 이상한 이름~~
- 의학분야에서 많이 사용하며 머신러닝에서는 지표로 많이 사용된다.
- FPR(False Positive Rate)가 변할 때 TPR(True Positive Rate)가 어떻게 변하는지를 나타내는 지표이다.
→ `FPR`값이 x축 / `TPR`값이 y축이 된다.

`TPR`
- True Positive Rate
- 재현율, 민감도
- 실제 Positive TP+FN 중 모델이 올바르게 분류한 TP의 비율을 의미한다 (TP/(FN+TP))
- 이에 대응하는 지표로 TNR(Ture Negative Rate)라고 불리는 특이성(Specificity)이 있다. 이 특이성은 실제 Negative FP+TN 중 모델이 올바르게 Negative로 분류한 TN의 비율을 의미한다.
→ `FPR` = `FP / (FP+TN)` = `1 - TNR`
∴ `1 - 특이성` 으로도 표현 가능하다.


![](https://velog.velcdn.com/images/cyhse7/post/96e61b56-27e4-44fe-a6e1-735b2a8e5a26/image.png)

ROC곡선을 살펴보면 (가운데 직선인) 0.5 값은 동전을 무작위로 던져 앞/뒤를 맞추는 랜덤수준의 이진 분류 ROC 직선으로 최저값이다. 이처럼 ROC곡선은 가운데 직선에 가까울 수록 성능이 떨어지는 것으로 보고, 멀어질 수록 성능이 좋은 것이라고 본다.

FPR을 0부터 1까지 점진적으로 증가하면서 TPR의 변화 값을 구해 그리는데, 임계값(Threshold)값을 변화시키며 수행할 수 있다.
→ 0으로 지정하면 TN 값이 0이 되기 때문에 FPR값은 1이 된다. 이렇게 하면 분류기의 Positive 확률 기준이 너무 낮아져 모두 Positive 로 예측한다.
→ 1로 지정하면 TN값이 1이 되기 때문에 분류기의 기준이 너무 높아져 데이터를 Postive로 예측할 수 없다. 이에 따라 FPR값은 0이 된다.

일반적으로 `ROC 곡선` 자체는 `FPR`과 `TPR`의 변화값을 보는데만 이용하고 사실상 분류 성능의 지표로 사용되는 것은 ROC 곡선 면적에 기반한 `AUC(Area Under Curve)` 값이다. `AUC` 값은 ROC 곡선 밑의 면적을 계산한 것으로 일반적으로는 1에 가까워지면 좋은 성능수치를 얻게 된다.
→ AUC 값이 높으려면 FPR이 얼마나 작은 상태에서 큰 TPR을 얻을 수 있는지가 관건

<br><br>

예시로 `roc_curve()`를 이용해 타이타닉 생존자 예측 모델의 FPR, TPR, 임계값을 구해보자
```python
from sklearn.metrics import roc_curve

# Positive 확률
lr_pred_proba_po = lr_clf.predict_proba(X_test)[:,1].reshape(-1,1)

# FPR(1-특이도), TPR(재현율=민감도), Threshold
# 임계값이 큰 값부터 작은 값 순서이다.
FPRs, TPRs, thresholds = roc_curve(y_test, lr_pred_proba_po)
print("임계값 배열:", thresholds.shape)

# 일부 threshold만 사용
# roc_curve에서 thresholds[0]은 max(예측확률)+1로 임의 설정된다. 이를 제외하기 위해 np.arange는 1부터 시작
thr_idx = np.arange(1, thresholds.shape[0], 10)

print("샘플 추출을 위한 배열의 index : ", thr_idx)
print("샘플 index로 추출한 임계값:", np.round(thresholds[thr_idx],2))
print("샘플 임계값별 FPR:", np.round(FPRs[thr_idx],3))
print("샘플 임계값별 TPR:", np.round(TPRs[thr_idx],3))


>
임계값 배열: (51,)
샘플 추출을 위한 배열의 index :  [ 1 11 21 31 41]
샘플 index로 추출한 임계값: [0.95 0.64 0.4  0.22 0.13]
샘플 임계값별 FPR: [0.    0.042 0.161 0.322 0.669]
샘플 임계값별 TPR: [0.016 0.656 0.787 0.902 0.967]
```
`roc_curve()`를 이용하여 임계값별 FPR과 TPR을 구했다.
```python
def roc_curve_plot(y_test, pred_proba_po):
    # FPR(1-특이도), TPR(재현율=민감도), Threshold
    # precision_recall_curve와는 다르게 임계값이 큰 값부터 작은 값 순서이다.
    FPRs, TPRs, thresholds = roc_curve(y_test, pred_proba_po)
    
    # ROC 곡선을 그래프 곡선으로 그린다
    plt.plot(FPRs, TPRs, label ="ROC")
        
    # 기준값(?인 가운데 대각선 직선을 그린다
    plt.plot( [0,1], [0,1], "k--", label="Random")
    
    # x축 스케일 0.1 단위로 조정
    start, end = plt.xlim()
    plt.xticks( np.round( np.arange(start, end, 0.1), 2))
    
    # 기타 옵션
    plt.xlim(0,1), plt.ylim(0,1)
    plt.xlabel("FPR(1-Specificity)"), plt.ylabel("TPR(Sensitivity)")
    plt.legend()
    
    # 그래프 보기
    plt.show()
    
# Positive 확률
lr_pred_proba_po = lr_clf.predict_proba(X_test)[:,1].reshape(-1,1)
roc_curve_plot(y_test, lr_pred_proba_po)
```
![](https://velog.velcdn.com/images/cyhse7/post/32cec647-05d7-4cfd-9c32-88ea5e435040/image.png)


## 피마 인디언 당뇨병 예측
먼저 이전에 사용한 `get_clf_eval()`함수에 AOC AUC 값을 측정하는 로직을 추가하면서, 함수의 인자를 늘려준다.
```python
# 평가 지표 함수
# 오차행렬, 정확도, 정밀도, 재현율을 한꺼번에 계산한다
from sklearn.metrics import accuracy_score, precision_score , recall_score , confusion_matrix
def get_clf_eval(y_test, pred = None, pred_proba = None) :
    confusion = confusion_matrix(y_test, pred)    #  오차행렬
    accuracy = accuracy_score(y_test, pred)       # 정확도
    precision = precision_score(y_test, pred)     # 정밀도
    recall = recall_score(y_test, pred)           # 재현율
    
    f1 = f1_score(y_test, pred)      # F1 스코어
    
    # ROC-AUC
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차행렬')
    print(confusion)
    
    # ROC-AUC 추가
    print(f'정확도:{accuracy:.4f}, 정밀도:{precision:.4f}, 재현율:{recall:.4f}, f1_score:{f1:.4f}, AUC:{roc_auc:.4f}')
```

사용할 라이브러리들 로드
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
```
데이터를 읽어와서 0/1 값과 대략적인 데이터 모양을 확인한다.
```python
diabetes_data = pd.read_csv('pima_diabetes.csv')
print(diabetes_data['Outcome'].value_counts())
diabetes_data[:3]
```
![](https://velog.velcdn.com/images/cyhse7/post/bccfe610-2844-43c3-a23f-91ca9852ad5f/image.png)

### 데이터 살펴보기
`Pregnancies` : 임신횟수
`Glucose` : 포도당 부하 검사 수치
`BloodPressure` : 혈압
`SkinThickness` : 팔 삼두근 뒤쪽의 피하지방 측정값(mm)
`Insulin` : 혈청 인슐린(mu U/ml)
`BMI` : 체질량지수(체중(kg)/키(m))^2)
`DiabetesPedigreeFunction` : 당뇨 내력 가중치 값
`Age` : 나이
`Outcome` : 클래스 결정 값(0 or 1)

```
0    500
1    268
```
Negative 값(0)이 500개, Positive 값(1)이 268개로 0의 값이 상대적으로 많다. ~~음... 이정도 불균형은 괜찮은 걸까?~~

```python
diabetes_data.info()
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
 #   Column                    Non-Null Count  Dtype  
---  ------                    --------------  -----  
 0   Pregnancies               768 non-null    int64  
 1   Glucose                   768 non-null    int64  
 2   BloodPressure             768 non-null    int64  
 3   SkinThickness             768 non-null    int64  
 4   Insulin                   768 non-null    int64  
 5   BMI                       768 non-null    float64
 6   DiabetesPedigreeFunction  768 non-null    float64
 7   Age                       768 non-null    int64  
 8   Outcome                   768 non-null    int64  
dtypes: float64(2), int64(7)
memory usage: 54.1 KB
```
### 데이터 나누기
```python
# feature data set : X
# label data set : Y
# 가장 마지막은 Outcome 칼럼으로 레이블 값이기 때문에 X, Y에 적절히 담기
X = diabetes_data.iloc[:, :-1]
y = diabetes_data.iloc[: ,-1]

# train-test set 나누기
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=156)

# 확인
X_train
print(len(X_train), len(X_test))
```
614 154

```python
# 로지스틱 회귀로 학습/예측/평가 를 수행하겠다
lr_clf = LogisticRegression(solver = 'liblinear')
lr_clf.fit(X_train, y_train)
pred = lr_clf.predict(X_test)

# output이 두개가 나오는데 뒤( =1값 =positive값)만 취하겠다.
pred_proba = lr_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test, pred, pred_proba)
```

```
오차행렬
[[87 10]
 [26 31]]
정확도:0.7662, 정밀도:0.7561, 재현율:0.5439, f1_score:0.6327, AUC:0.8343
```
이때 책에서는 `lr_clf = LogisticRegression(solver = 'linear')`라고 되어있는데 `(solver = 'liblinear')` 의 모양으로 사용해주어야 제대로 나온다. (작은 데이터에 적합한 알고리즘 설정값이라 하며, L1 L2 제약조건을 모두 지원한다고)

`diabetes_data.describe()` 데이터 값을 보면 min() 값이 0으로 되어 있는 feature가 생각보다 많다. 예를들어 포도당 수치인 Glucose 칼럼에도 0값이 들어있는데 이는 말이 되지를 않기 때문에 이 값들을 모두 평균으로 대체하기로 한다.
```python
# 0값 검사해보기
zero_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# 전체 데이터 수
total_count = diabetes_data['Glucose'].count()

# feature별로 반복하면서 데이터 값이 0인 건수를 추출하고 퍼센트로 확인해본다.
for feature in zero_features :
    zero_cnt = diabetes_data[diabetes_data[feature] == 0][feature].count()
    print(f'{feature}\n0 건수는 {zero_cnt:.4f}, 퍼센트는 {100*zero_cnt/total_count:.4f}')
    print()
```

```
Glucose
0 건수는 5.0000, 퍼센트는 0.6510

BloodPressure
0 건수는 35.0000, 퍼센트는 4.5573

SkinThickness
0 건수는 227.0000, 퍼센트는 29.5573

Insulin
0 건수는 374.0000, 퍼센트는 48.6979

BMI
0 건수는 11.0000, 퍼센트는 1.4323
```

```python
# 0을 각 칼럼값의 평균으로 대체
zero_mean = diabetes_data[zero_features].mean()
diabetes_data[zero_features] = diabetes_data[zero_features].replace(0, zero_mean)

# 확인
diabetes_data[zero_features]
```
![](https://velog.velcdn.com/images/cyhse7/post/c29d0458-18eb-4466-930b-d59249e6f27e/image.png)
```python
X = diabetes_data.iloc[: ,:8]
y = diabetes_data.iloc[: ,8]

# 표준화
# 일괄적으로 스케일링 적용
scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=156)

# 로지스틱 회귀로 학습/예측/평가 를 수행
lr_clf = LogisticRegression(solver = 'liblinear')
lr_clf.fit(X_train, y_train)
pred = lr_clf.predict(X_test)

# output이 두개가 나오는데 뒤( =1값 =positive값)만 취하겠다.
pred_proba = lr_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test, pred, pred_proba)
```

```
오차행렬
[[83 14]
 [25 32]]
정확도:0.7468, 정밀도:0.6957, 재현율:0.5614, f1_score:0.6214, AUC:0.8318
```
음... 이전 
`정확도:0.7662, 정밀도:0.7561, 재현율:0.5439, f1_score:0.6327, AUC:0.8343`
평균값으로 대체한 후
`정확도:0.7468, 정밀도:0.6957, 재현율:0.5614, f1_score:0.6214, AUC:0.8318`
로 결과가 나왔다. 이외에도 전체적으로 책과는 다르게 수치가 떨어진 모습을 확인할 수 있었다. 그렇다면 앞에서 임계값을 낮춘상태에서 예측을 하면 결과가 어떻게 나올까. 사이킷런의 `predict()` 메서드는 임계값을 마음대로 변환할 수 없으므로 별도의 로직으로 이를 구연해야 한다. 앞에서 살펴본 `Binarizer` 클래스를 이용해 `predict_proba()`로 추출한 예측 결과 확률 값을 변환하여 변경된 임계값에 따른 예측 클래스 값을 구해보자(결과가 궁금하다)

```python
from sklearn.preprocessing import Binarizer

# 임계값을 0.48로 설정
binarizer = Binarizer(threshold = 0.48)

pred_proba = lr_clf.predict_proba(X_test)[:,1]
pred_th_048 = binarizer.fit_transform(pred_proba.reshape(-1, 1))
get_clf_eval(y_test, pred_th_048, pred_proba)
```

```
오차행렬
[[83 14]
 [25 32]]
정확도:0.7468, 정밀도:0.6957, 재현율:0.5614, f1_score:0.6214, AUC:0.8318
```
결과적으로
그냥 했을 때
`정확도:0.7662, 정밀도:0.7561, 재현율:0.5439, f1_score:0.6327, AUC:0.8343`
평균값으로 대체한 후
`정확도:0.7468, 정밀도:0.6957, 재현율:0.5614, f1_score:0.6214, AUC:0.8318`
임계값 변화를 주었을때
`정확도:0.7468, 정밀도:0.6957, 재현율:0.5614, f1_score:0.6214, AUC:0.8318`

~~음............~~
