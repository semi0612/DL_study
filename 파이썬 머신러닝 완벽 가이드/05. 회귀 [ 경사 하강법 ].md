위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---

# 회귀(regression)
데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계적 기법이며, 여러 개의 독립변수와 한 개의 종속변수 간 상관관계를 모델링하는 기법을 통칭한다.
![](https://velog.velcdn.com/images/cyhse7/post/3992ffe7-b0ef-47ce-8cc0-8f2410533fd6/image.png)
```
예를 들어 위와 같은 선형회귀 식을 예로 들면 Y는 종속변수 즉, 아파트 가격을 뜻한다.
그리고 X1, X2, X3 ... Xn은 방 개수, 방 크기, 주변 학군등의 독립변수를 의미
W1, W2 ... Wn은 이 독립변수 값에 영향을 미치는 회귀계수
```

독립변수가 한개라면 단일회귀, 여러개라면 다중회귀이며 독립 변수의 결합방식이 선형이나 비선형이냐에 따라 선형회귀와 비선형 회귀로 분류됩니다.
<table style="width:500%">
  <tr style = "border-bottom:1px solid">
    <th>독립변수 개수</th>
    <th>회귀 계수의 결합</th>
  </tr>
  <tr>
    <td>1개 : 단일 회귀</td>
    <td>선형 : 선형 회귀</td>
  </tr>
  <tr>
    <td>여러 개 : 다중회귀</td>
    <td>비선형 : 비선형 회귀</td>
  </tr>
</table>

<br><br>

지도학습의 양대 산맥인 `분류`와 `회귀`의 가장 큰 차이는 예측값의 결과인데, 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고 회귀는 연속되는 숫자값으로 나온다는 점이다.

[ 대표적인 선형회귀 모델 ]
- 일반 선형 회귀 : 예측값과 실제값의 RSS(Residual Sum of Suqares)를 최소화할 수 있도록 회귀 계수를 최소화하며, 규제(Regularization)을 사용하지 않는 모델
- 릿지(Ridge) : 선형회귀에 L2 규제를 추가한 회귀 모델, 규제를 추가함으로 상대적으로 큰 회귀 계수 값의 예측 영향도 감소
- 라쏘(Lasso) : 선형회귀에 L1 규제를 추가한 모델. 예측 영향력이 작은 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 한다 → 피처 선택(selection)기능으로도 불린다.
- 엘라스틱넷 : 선형회귀에 L1과 L2 규제를 결합한 모델. 주로 피처가 많은 데이터 셋에 적용되며 L1규제로 피처의 개수를 줄이면서 L2 규제로 계수 값의 크기를 조정한다
- 로지스틱 회귀 : 회귀라는 이름이지만 사실 분류에 사용되는 선형 모델. 매우 강력한 분류 알고리즘으로 일반적으로 이진 분류 뿐 아니라 희소 영역의 분류(ex. 텍스트 분류)와 같은 영역에서 뛰어난 예측 성능을 보인다.

## 회귀에 대한 이해
단순 선형 회귀는 독립변수도 하나, 종속변수도 하나인 경우로 주택가격이 주택의 크기로만 결정 된다고 할 때를 예로 들 수 있다.
<p style="width:60%;"><img src = "https://velog.velcdn.com/images/cyhse7/post/fc3ee28e-8a9e-4092-a423-0f0037425bdb/image.png"></p>
X축이 주택의 크기 축(평당 크기), Y축이 주택 가격 축인 2차원 평면에서 주택 가격은 '특정 기울기와 절편'을 가진 '1차 함수식'으로 모델링 할 수 있다. 즉 예측값 Ŷ는 w0+w1*X로 계산할 수 있는 것이다.

그리고 회귀 모델을 이러한 `Ŷ = w0+w1*X` 와 같은 1차 함수로 모델링했다면 실제 주택 가격은 이러한 1차 함수 값에서 실제 값 만큼의 오류값을 뺀(혹은 더한)값이 된다(w0+w1*X+오류값)

이렇게 실제 값과 회귀 모델의 차이에 따른 오류값을 남은 오류, 즉 잔차라 부르며, 최적의 회귀 모델을 만든다는 것은 바로 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만든다는 의미이기도 하다. 동시에 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는 다는 의미도 된다.

잔차는 + 혹은 - 값을 모두 가질 수 있기 때문에 단순히 전체 데이터에 대한 오류를 계산하기 위하여 단순히 합을 한다면 오류의 값이 크게 줄어들 수 있다. 따라서 보통 절대값을 취하여 더하는 방식인 MAE(Mean Absolute Error), 오류 값의 제곱을 구해서 더하는 방식(Error² = RSS)인 RSS(Residual Sum of Square) 방식을 사용한다.
![](https://velog.velcdn.com/images/cyhse7/post/632cdce7-49b5-46a3-9004-834b76a221b5/image.PNG)

위처럼 RSS는 변수가 W0 w1인 식으로 표현할 수 있으며, 이 RSS를 최소로 하는 회귀계수(W0 W1) 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항이다.

회귀에서 RSS는 비용(Cost)이며 w변수(회귀 계수)로 구성되는 RSS를 비용함수라 한다. 머신러닝에서 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용함수(=손실함수=loss function)가 반환하는 값(=오류값)을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류값을 구하는 것이다.

## 비용 최소화 - 경사하강법
어떻게 하면 오류가 작아지는 방향으로 회귀계수를 보정할 수 있을까 하며 나온 방법이다. 고차원 방정식에 대한 문제를 해결해 주면서 비용함수를 최소화 할 수 있는 방안을 직관적으로 제공하는 방식. 마치 어두운 밤 산 정상에서 아래로 내려갈때, 현 위치보다 낮은 위치로 이동하다보면 지상에 도착하는 것과 마찬가지로 회귀계수를 보정한다.

결론만 보자면, 오차함수의 1차 미분값의 기울기가 최소인 점을 찾으면 된다. 아래에서 자세히 알아보겠다.

![](https://velog.velcdn.com/images/cyhse7/post/641927a6-eb69-461b-a85f-de317a4a5b3c/image.png)
RSS는 회귀계수(위 그림에서는 β0, β1)의 2차 함수로서 포물선 형태를 가질 것이며, 이 형태의 2차함수 최저점은 2차 함수의 미분값인 1차 함수의 기울기가 가장 최소일 때다. 경사하강법은 최초의 β에서부터 미분을 적용한 뒤 이 미분값이 감소하는 방향으로 β를 업데이트 한다.
- 만약 최초의 β에서 기울기(미분값)이 양수라면 포물선을 고려하였을 때 최초의 β는 2차함수 최저점의 우측에 있을 것이다
→ 따라서 최초 β에서 왼쪽으로 이동하면 2차함수 최저점에 조금 더 가까워질것이다
→ 즉 미분값의 부호 반대방향으로 이동하면서 점차 미분값을 감소시킨다
→ 더이상 감소하지 않는 지점을 비용이 최소가 되는 지점으로 간주하고 그때 β를 반환한다.

-----

## 경사 하강법 종류
### 배치 경사 하강법(Batch Gradient Descent:BGD)
- 전체 학습 데이터를 하나의 배치로 묶어 학습시키는 방법으로 수행 시간이 오래 걸리고 많은 메모리가 필요하다.
- Global Optima로 수렴이 안정적이지만 Local Optima에 수렴될 경우 탈출하기 어려울 수도 있다.
```
local optimum
아직 최종목표(비용함수 최소값(=Global Optima)에 도달하지 않았음에도 불구하고,
특정 지점에 들어가니 '진행시 loss가 증가하기 때문에' w를 멈추는 현상을
보이는 지점(Local Local 이라고도 한다)

Optimization(최적화)을 잘못한 경우 로컬옵티마에 빠지기 쉽다고 함
```
- GPU를 활용한 병렬처리에 유리하다

### 확률적 경사 하강법(Stochastic Gradient Descent:SGD)
- 전체 학습 데이터 중 랜덤하게 선택된 하나의 데이터를 하나의 배치로 학습한다.
- 하나의 데이터만 사용하므로 Shooting이 발생하여 Local Optima에 빠질 위험은 적어지지만 Global Optima로 수렴이 어려울 수 있다.
- 하나의 데이터만 사용하므로 GPU 전부 활용이 불가하다

### 미니 배치 확률적 경사 하강법(Mini-Batch Stochastic Gradient Descent:MSGD)
- BGD와 SGD의 절충안으로 전체 데이터를 Batch_size만큼 나누어 Mini-Batch를 구성 후 학습한다.
- BGD보다는 local optima에 빠질 리스크가 적고 메모리도 적게 사용한다
- SGD보다는 GPU 성능을 활용한 병렬 처리가 가능하여 속도면에서 유리

-----

## 구현하며 이해하기
![](https://velog.velcdn.com/images/cyhse7/post/2f89095d-342c-456a-a82c-f3d3800a22f8/image.png)
```
yi = 실제값 i
ŷi = 예측값i

위 수식은 R(w)를 W1, W0로 편미분한 결과이다.
앞에서 언급한 비용함수 RSS(W0, W1)을 편의상 R(w)로 지징한다
```
- 먼저 임의의 초기 W0, W1 값을 설정한 후 그 값에 대한 편미분이 구해지면 다음 W0, W1는 해당 편미분 값에 `-`를 곱하여 업데이트 한다.
※ 다만 이 편미분 값이 너무 클 수도 있기 때문에 보정계수 n을 곱해주는데 이를 학습률이라 함

즉, 새로운 W0, W1 값은 이전 `W0W1 값 - 학습률 * 편미분` 값이 된다. 새로운 값으로 다시 비용을 계산하다가 비용이 감소하면 계속 업데이트를 진행하고, 더이상 진전이 없으면(감소하지 않으면) 반복을 중지하고 그 때의 W0, W1를 반환한다.



### 미니배치 구현해보기
간단한 회귀 식인 `y=4X+6`을 근사하기 위해 100개의 임의(랜덤)데이터 셋을 만들고, 여기에 경사 하강법을 이용하여 회귀계수(W0, W1)을 도출 할 것이다.

데이터 셋 만들기
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.random.seed(0)
x = 2*np.random.rand(100,1)

# y = 4x + 6을 근사
# w1=4, w0은 6
y = 6 + 4*x + np.random.rand(100,1)
```
산점도로 시각화
```python
plt.scatter(x, y)
```
![](https://velog.velcdn.com/images/cyhse7/post/93a6b163-6b72-4995-88c1-97968d82954b/image.png)

데이터가 `y = 4x + 6` 을 중심으로 무작위로 퍼저 있는 모습이다.

선형모델을 학습시켜 기울기 파라미터를 확인해보자. 기울기 파라미터는 가중치(weight) 또는 계수(coefficient)라고 하며 객체의 `coef_` 속성에 저장되어 있다.
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept=True)

lr = model.fit(x, y)
```
```
array([[3.96827553]])
```
편향(offset)또는 절편(intercept) 파라미터 값은 `intercept_` 속성에 저장되어 있다. 
```python
lr.intercept_
```
```
array([6.55808002])
```

비용을 구하는 사용자 함수 정의
```python
def get_cost(y, y_hat):
    n = len(y) 
    cost = np.sum( np.square(y - y_hat) ) / n
    return cost
```
미니 배치를 사용하는 사용자 함수 정의
```python
# Mini-Batch Stochastic Gradient Descent: MSGD
def mini_stochastic_gradient_descent(w1, w0, X, y, learning_rate = 0.01, iters = 10000, batch_size=10):
    n = batch_size
    
    for i in range(iters):
        # batch_size만큼 랜덤으로 데이터 추출
        np.random.seed(i)
        stochastic_random_index = np.random.permutation(X.shape[0]) # permutation은 정수를 arange 후 셔플
        sample_X = X[ stochastic_random_index[0:batch_size] ]
        sample_y = y[ stochastic_random_index[0:batch_size] ]
                
        # 예측값 nx1
        y_hat = w0 + w1*sample_X

        # 학습률 * 편미분
        w1_update = learning_rate * -(2/n) * sample_X.T @ (sample_y - y_hat)
        w0_update = learning_rate * -(2/n) * np.sum(sample_y - y_hat)

        # 파라미터 업데이트
        w1 = w1 - w1_update
        w0 = w0 - w0_update
        
        
    return w1, w0
```
배치사이즈를 10으로 잡고 실행해보기
```python
w1, w0 = mini_stochastic_gradient_descent(0, 0, X, y, iters=1000, batch_size=10)

y_hat = w0 + w1[0,0]*X
cost = get_cost(y, y_hat)

print(f"w1: {w1[0,0]:.3f}, w0: {b0:.3f}")
print(f"Mini Stochastic Gradient Descent Total Cost: {cost:.4f}")
```
```
w1: 4.031, w0: 6.486
Mini Stochastic Gradient Descent Total Cost: 0.0777
```

----

## 회귀 평가 지표
회귀의 평가를 위한 지표는 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심이다. 실제 값과 예측값의 차이를 그냥 더한다면 음수와 양수가 섞여있기 때문에 오류가 상쇄되고 정확한 지표가 될 수 없다. 이 때문에 절대값으로 변환한 뒤 평균을 구하거나 제곱 혹은 제곱 후 다시 루트를 씌운 평균값을 구한다.
- MAE : 실제값과 예측값의 차이를 절대값으로 변환해 평균 값을 구한다
- MSE : 실제값과 예측값이 차이를 제곱한 뒤 평균한 값
- RMSE : MSE 값은 오차를 제곱하기 때문에 실제 오차모다 더 커지는 특성이 있다. 그렇기 때문에 MSE에 루트를 씌운 것이 RMSE이다.
- R² : 분산 기반으로 예측 성능을 평가한다. 실제 값의 분산 대비 예측 값의 분산 비율을 지표로 하여 1에 가까운 값일 수록 예측 정확도가 높다고 본다.

<br>

[MSE - MSE]
- `MSE`는 오차의 제곱을 더하기 때문에 오차값이 커지는 이상치 데이터에 `MAE`보다 크게 반응을 하게 된다
- `MAE`를 사용하는 것이 `MSE`보다 이상치 데이터에 좀 더 좋은 결과를 만들 수는 있지만 작은 loss value의 변화에도 큰 기울기 값을 가지게 된다.
