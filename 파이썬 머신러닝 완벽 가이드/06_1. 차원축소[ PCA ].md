위키북스의 '파이썬 머신러닝 완벅 가이드' 개정 2판으로 공부하고 있습니다. 실습 과정과 이해에 따라 내용의 누락 및 코드의 변형이 있을 수 있습니다.

---

# 차원축소
매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 셋을 생성하는 것이다. 일반적으로 차원이 증가할 수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고, 데이터들 사이의 공간이 많이 비어있는 모습을 갖게 된다.

![](https://velog.velcdn.com/images/cyhse7/post/14b2d99e-c6d4-4ae6-815f-ba0ae88dcb91/image.png)
[그림출처](https://datapedia.tistory.com/15)

많은 다차원의 피처를 차원 축소해 피처수를 줄이면 더 직관적으로 데이터를 해석할 수 있다. 또한 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력도 줄일 수 있다.

## PCA(Principal Component Analysis)
대표적인 차원 축소 기법 중 하나이다. 주성분 분석이라고도 함.

여러 변수간에 존재하는 상관관계를 이용해 이를 대표하는 주성분(Pricipal Component)를 추출해 차원을 축소한다. 축소시 기존 데이터 정보의 손실이 유실되는 것을 최소화히기 위해 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소하는데 이것이 PCA의 주성분이 된다. 일반적으로 차원 축소는 아래 두가지로 나눌 수 있다.
- `피처 선택(feature selection)`
피처 선택, 즉 특성 선택은 말 그대로 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것이다.
- `피처 추출(feature extraction)`
기존 피처를 저차원의 중요 피처로 압축하여 추출하는 것이다. 이렇게 새롭게 추출된 중요 특성은 기존의 치퍼가 압축된 것이므로.. 기존 피처와는 완전히 다른 값이 된다.

차원 축소는 단순히 데이터의 압축을 의미하는 것이 아니다. 더 중요한 것은 차원 축소를 통해 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는데에 있다. PCA, SVD, NMF는 이처럼 잠재적인 요소를 찾는 대표적인 차원 축소 알고리즘이다. 이 차원 축소 알고리즘은 매우 많은 픽셀로 이뤄진 이미지 데이터에서 잠재된 특성을 피처로 도출해 함축적 형태의 이미지 변환과 압축을 수행할 수 있다. 이렇게 변환된 이미지는 원본 이미지보다 훨씬 적은 차원이기 때문에 이미지 분류등의 분류 수행시 과적합 영향력이 작아져서 오히려 원본 데이터로 예측하는 것보다 예측 성능을 더 끌어올릴 수도 있다.


### 선형대수에서의 PCA
선형대수로 PCA를 해석하면 입력 데이터의 공분산 행렬을 고유값 분해하고, 구한 고유벡터에 입력 데이터를 선형 변환하는 것이다.
→ 고유벡터는 PCA의 주성분 벡터로서 입력 데이터의 분산이 큰 방향을 나타낸다
→ 고유값은 고유벡터의 크기를 나타내며 동시에 입력데이터의 분산을 나타낸다.
![](https://velog.velcdn.com/images/cyhse7/post/3d43691b-438c-4520-87c6-8a941a54ee6e/image.png)
- 공분산 행렬 C는 (교유벡터 직교 행렬 * 고유값 정방행렬 * 고유벡터 직교행렬의 전치행렬)로 분해된다.
- ei , λi는 각각 i번째 고유벡터, 해당 고유벡터의 크기를 의미한다.
- e₁는 가장 분산이 큰 방향을 가진 고유벡터이며 e₂는 e₁에 수직이면서 다음으로 가장 분산이 큰 방향을 가진 고유벡터이다.

### PCA 수행과정
1. 입력 데이터 세트의 공분산 행렬을 생성한다
2. 공분산 행렬의 고유벡터와 고유값을 계산한다
3. 고유값이 가장 큰 순으로 K개(PCA변환 차수)만큼 고유벡터를 추출한다.
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다.

### 구현하며 이해하기 - IRIS데이터
```python
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np

# 데이터 가져오기
iris = load_iris()
X_features = iris.data
y_labels = iris.target
```
iris(붓꽃) 데이터는 `[ 0:sepal lenght, 1:sepal width, 2:petal length, 3:petal width ]` 의 4가지 속성으로 이루어져있다. 원본 데이터가 어떻게 분포되어있는지 2차원으로 시각화해보자
```python
iris_df['target'] = iris.target
markers = ["^", "s", "o"]

for i, marker in enumerate(markers) :
    x_axis_data = iris_df[iris_df['target']==i][0]
    y_axis_data = iris_df[iris_df['target']==i][1]
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])
plt.legend()
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.show()
```
![](https://velog.velcdn.com/images/cyhse7/post/1d9a7139-39a5-4815-a7d1-334d01fa5ed4/image.png)
setosa의 경우 sepal width가 3이상, sepal length가 6이하인 곳에 대부분 분포되어 있고 versicolor, virginica는 sepal width, sepal length만으로는 분류하기 어려워 보인다.

PCA는 여러 피처 값을 연산하기 때문에 스케일에 영향을 받으므로 pca로 압축하기 전에 피처 스케일링 작업이 필요하다. 여기서는 표준정규분포(StandardScaler)로 변환하였다.
```python
# Target 값을 제외하고 모든 속성값을 StandardScaler를 이용해 표준 정규 분포를 가지는 값들로 변환하자
from sklearn.preprocessing import StandardScaler
iris_scaled = StandardScaler().fit_transform(X_features)
iris_scaled
```
```
array([[-9.00681170e-01,  1.01900435e+00, -1.34022653e+00,
        -1.31544430e+00],
       [-1.14301691e+00, -1.31979479e-01, -1.34022653e+00,
        -1.31544430e+00],
       [-1.38535265e+00,  3.28414053e-01, -1.39706395e+00,
        -1.31544430e+00],
       [-1.50652052e+00,  9.82172869e-02, -1.28338910e+00,
        -1.31544430e+00],
       [-1.02184904e+00,  1.24920112e+00, -1.34022653e+00,
        -1.31544430e+00],
..이하생략
```
PCA를 이용해 기존 차원을 2차원으로 변환하자
```python
from sklearn.decomposition import PCA
# 2차원 PCA데이터로 변환
pca = PCA(n_components=2)
pca.fit_transform(iris_scaled)
iris_pca = pca.transform(iris_scaled)
print(iris_pca)
```
```
[[-2.26470281  0.4800266 ]
 [-2.08096115 -0.67413356]
 [-2.36422905 -0.34190802]
 [-2.29938422 -0.59739451]
 [-2.38984217  0.64683538]
 [-2.07563095  1.48917752]
 [-2.44402884  0.0476442 ]
 [-2.23284716  0.22314807]
 [-2.33464048 -1.11532768]
...이하생략
```
```python
print(iris_scaled.shape,'\n',iris_pca.shape)
```
```
(150, 4) 
 (150, 2)
```
원본 붓꽃 데이터 세트와 PCA로 변환한 데이터 세트의 정확도를 확인해보자
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rf_clf = RandomForestClassifier(random_state=156)
orignal_data = cross_val_score(rf_clf, iris_scaled, y_labels, scoring='accuracy', cv=3)
pca_data = cross_val_score(rf_clf, iris_pca, y_labels, scoring='accuracy', cv=3)

print(f'원본 데이터 교차검증 개별 정확도 : {orignal_data}\n원본 데이터 평균 정확도 : {np.mean(orignal_data)}')
print()
print(f'PCA 변환 데이터 교차검증 개별 정확도 : {pca_data}\nPCA 변환 데이터 평균 정확도 : {np.mean(pca_data)}')
```
```
원본 데이터 교차검증 개별 정확도 : [0.98 0.94 0.96]
원본 데이터 평균 정확도 : 0.96

PCA 변환 데이터 교차검증 개별 정확도 : [0.88 0.88 0.88]
PCA 변환 데이터 평균 정확도 : 0.88
```
원본 데이터 세트 대비 예측 정확도는 PCA 변환 차원 개수에 따라 예측성능이 떨어질 수 밖에 없다. 실습결과 원본 데이터의 평균 정확도에 비해 PCA변환 데이터의 평균 정확도가 약 8% 하락한 것을 확인할 수 있었다. 속성개수가 절반 감소한 것을 고려한다면 변환후에도 원본 데이터의 특성을 상당부분 유지하고 있음을 알 수 있는 부분

### 실습 - UCI 신용카드 고객 데이터
```python
!pip install xlrd
# credit card clients
import pandas as pd

# 1번 인덱스를 header로 삼고, 데이터가 있는 덱셀 시트명은 'Data'이다
df = pd.read_excel('./default of credit card clients.xls', header=1, sheet_name='Data').iloc[0:, 1:]
print(df.shape)
df[:3]
```
![](https://velog.velcdn.com/images/cyhse7/post/13276b0e-6fa6-48e7-a1e4-82b0c6c82dcc/image.png)
target으로 잡아야하는 컬럼은 default payment next month으로 다음달 연체 여부를 의미한다(0은 정상납부, 1은 연체)

PAY_0 다음에 PAY_2가 나와 있어 PAY_0를 PAY_1으로 변경하고 target의 컬럼명도 default로 변경한 후 상관관계를 보도록 하자
```python
# 컬럼명 변경
df.rename(columns={'PAY_0':'PAY_1','default payment next month':'default'}, inplace=True)

y_target = df['default']
X_features = df.drop('default', axis=1)

# 상관관계 보기
corr = X_features.corr()
corr
```
![](https://velog.velcdn.com/images/cyhse7/post/f56a069f-a17b-4505-aaf7-8d6873f76458/image.png)
어...... 한눈에 안보인다. 예상은 했지만 더 안보인다. 이래서 책에서는 heatmap으로 시각화하여 보라고 했나보다
```python
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 14))
sns.heatmap(corr, annot=True, fmt='.1g')
```
![](https://velog.velcdn.com/images/cyhse7/post/5ef78ab1-5127-432a-a22b-ae9e386973e6/image.png)
BILL_AMT1 ~ BILL_AMT6 간의 상관계수가 높은 것으로 보인다. 이렇게 높은 상관돌르 가진 속성들은 소수의 PXA만으로도 자연스럽게 이 속성들의 변동성을 수용할 수 있으니 솎아내자
```python
# BILL_AMT1~6만 솎아내기
df.info()
# 이름도 변경할 겸 데이터 안전을 위해 df를 카피하여주자
card_df = df.copy()
# BILL_AMT1  ~ 6까지의 6개 속성만 뽑아주기
card_df = card_df.iloc[:, 11:17]
card_df
```
![](https://velog.velcdn.com/images/cyhse7/post/c09afc56-6b5b-42c8-afcc-beac3bf629ef/image.png)
이제 순서대로 변환을 진행하면 된다.
1. scaling 변환
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
card_bill_scaler = scaler.fit_transform(X_features)
card_bill_scaler
```
```
array([[-1.13672015,  0.81016074,  0.18582826, ..., -0.30806256,
        -0.31413612, -0.29338206],
       [-0.3659805 ,  0.81016074,  0.18582826, ..., -0.24422965,
        -0.31413612, -0.18087821],
       [-0.59720239,  0.81016074,  0.18582826, ..., -0.24422965,
        -0.24868274, -0.01212243],
       ...,
       [-1.05964618, -1.23432296,  0.18582826, ..., -0.03996431,
        -0.18322937, -0.11900109],
       [-0.67427636, -1.23432296,  1.45111372, ..., -0.18512036,
         3.15253642, -0.19190359],
       [-0.90549825, -1.23432296,  0.18582826, ..., -0.24422965,
        -0.24868274, -0.23713013]])
```
2. pca 변환
```python
from sklearn.decomposition import PCA
# 2차원 PCA데이터로 변환
pca = PCA(n_components=2)
card_bill_pca = pca.fit_transform(card_bill_scaler)
print(card_bill_pca)
```
```
[[-1.88796247 -0.9061075 ]
 [-0.76469577 -2.10928777]
 [-0.84740789 -1.0721793 ]
 ...
 [ 0.35745734 -3.31275505]
 [ 0.65055187  0.72290108]
 [-0.14556442 -0.80975087]]
```
3. pca 변환 이전에 대하여 logisticregression으로 분류
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

lr_clf = LogisticRegression(random_state=156)
cross_val_score(lr_clf, card_bill_scaler, y_target, scoring='accuracy', cv=3)
# cv에 따른 차이도 궁금하여 돌려봄
# (cv=5) array([0.803     , 0.806     , 0.8125    , 0.81616667, 0.812     ])
# (cv=3) array([0.806 , 0.8138, 0.8086])
```
4. pca 변환한 logisticRegression으로 분류
```python
cross_val_score(lr_clf, card_bill_pca, y_target, scoring='accuracy', cv=3)
# cv에 따른 차이도 궁금하여 돌려봄
# (cv=5) array([0.792     , 0.79316667, 0.79466667, 0.79966667, 0.797     ])
# (cv=3) array([0.7936, 0.7969, 0.7959])
```
결과적으로 
```python
print(card_bill_scaler.shape,'\n',card_bill_pca.shape)
```
```
(30000, 23) 
 (30000, 2)
```
전체 23개 속성의 약 25%수준인 6개의 PCA컴포넌트 만으로도 원본 데이터를 기반으로 한 분류예측 결과에 비해 약간의 예측 성능 저하를 보였다. 작은 수치는 아니지만 전체 속성의 25%만으로도 이 정도의 예측 성능을 유지할 수 있다는 것은 PCA의 뛰어난 압축 능력을 잘 보여주는 것이라 생각해볼 수 있다.
