
## 학습 파이프라인
모델 학습의 전체 파이프라인을 소개한다. 이는 이 책에서 소개하는 5가지 과제에만 적용되는 것이 아니라 학습에 공통으로 적용된다.

이 책에서 진행하는 모든 실습은 ratsnlp라는 오픈소스 패키지를 사용할 것이며, 구글 코랩 환경에서 실습을 진행 할 수 있도록 책의 필자가 직접 개발.
https://github.com/ratsgo/ratsnlp/tree/master/ratsnlp/nlpbook

### 각종 설정 값 정하기
모델을 만들기 위해서는 설정값들을 먼저 정해줘야 하는데, 어떤 프리트레인 모델을 사용할지, 학습에 사용할 데이터는 어떤것인지, 결과는 어디에 저장할 지 등을 말한다.

모델 구조와 학습등에 직접 관계된 설정값을 가리키는 하이퍼파라미터(hyperparameter) 역시 미리 정해둬야 하는 중요한 정보이다. 예를 들어 러닝레이트(learning rate), 배치크기(batch size)등을 말할 수 있다.

<sub>설정값 예시</sub>
```python
from ratsnlp.nlpbook.classification import ClassificationTrainArguments
args = nlpbook.TrainArguments(
    pretrained_model_name="beomi/kcbert-base",
    downstream_corpus_name="nsmc",
    downstream_corpus_root_dir="/content/Korpora",
    downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-cls",
    learning_rate=5e-5,
    batch_size=32,
)
```

### 데이터 내려받기
이 책에서는 프리트레인을 마친 모델을 다운스트림 데이터로 파인튜닝하는 실습을 진행한다. 이를 위해서는 다운스트림 데이터를 미리 내려받아 둬야 하는데, 이 책에서는 상업적으로도 사용가능한 데이터를 사용할 것이다. '박은정'님이 공개하진 네이버 영화평 말뭉치인 NAVER Sentiment Movie Corpus(NSMC)가 대표적

<sub>데이터 다운로드 예시</sub>
```python
from Korpora import Korpora
Korpora.fetch(
    corpus_name=args.downstream_corpus_name,
    root_dir=args.downstream_corpus_root_dir,
    force_download=True,
)
```

위 예시에서 다운로드 툴킷으로 ratsnlp뿐 아니라 코포라(Korpora)라는 패키지를 사용하는 걸 볼 수 있는데, 이 패키지는 다양한 한국어 말뭉치를 쉽게 내려받고 전처리 할 수 있도록 도와준다.
[코포라 깃허브](https://github.com/ko-nlp/Korpora)

### 프리트레인을 마친 모델 준비하기

대규모 말뭉치를 활용한 프리트레인에는 많은 리소스가 필요한데, 최근 많은 기업과 개인이 자유롭게 사용할 수 있도록 공개하는 것이 많아 그 혜택을 이용해볼 수 있다. 특히 미국 자연어처리 기업 허깅페이스(hugginface)에서 만든 [트랜스포머](https://github.com/huggingface/transformers) 라는 오픈소스 파이썬 패키지는 단 몇 줄만으로도 모델을 사용할 수 있게끔 한다.

<sub>모델 준비하는 예시</sub>
```python
# 이준범 님이 허깅페이스 트랜스포머에 등록해 주신 kcbert-base 모델을 준비하는 코드
# 앞선 설정값 예시에서 args.pretrained_model_name에 beomi/kcbert-base라고 선언해놨기 때문에
# 설정값 예시와 아래 예시를 순차적으로 실행하면 바로 kcbert-base 모델을 사용할 수 있는 상태가 된다.

from transformers import BertConfig, BertForSequenceClassification
pretrained_model_config = BertConfig.from_pretrained(
    args.pretrained_model_name,
    num_labels=2,
)
model = BertForSequenceClassification.from_pretrained(
        args.pretrained_model_name,
        config=pretrained_model_config,
)
```

### 토크나이저 준비하기
자연어 처리 모델의 입력은 토큰(token)이다. 토큰이란 문장(sentence)보다 작은 단위를 뜻하며, 한 문장은 여러 개의 토큰으로 구성된다. 토큰의 분리 기준은 그때그때 가를 수 있는데, 문장을 띄어쓰기만으로 나눌 수도 있고, 의미의 최소단위인 형태소(morpheme) 단위로 분리할 수도 있다

문장을 토큰 시퀀스(token sequence)로 분석하는 과정을 토큰화(tokenization), 토큰화를 수행하는 프로그램을 토크나이저(tokenizer)라고한다. 이 책에서는 BPE(Byte Pair Encoding)이나 워드피스(wordpiece)알고리즘을 채택한 토크나이저를 실습에 사용할 것이다

<sub>토크나이저 예시</sub>
```python
#K CBERT-BASE 토크나이저 준비

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(
    args.pretrained_model_name,
    do_lower_case=False,
)
```

### 데이터 로더 준비하기
파이토치는 딥러닝 모델 학습을 지원하는 파이썬 라이브러리 이기 때문에 데이터 로더(DataLoader)라는 것이 포함되어 있다. 파이토치로 딥러닝 모델을 만들고자 한다면 이 데이터로더를 반드시 정의해야한다.

로더는 학습때 데이터를 배치 단위로 모델에 밀어넣어주는 역할을 한다. 전체 데이터 가운데 일부 인스턴스를 뽑아 (sample) 배치를 구성. 데이터셋은 데이터 로더의 구성 요소 중 하나이다. 데이터 셋은 여러 인스턴스(문서+레이블)을 보유하고 있으며, 데이터 로더가 배치를 만들 때 인스턴스를 뽑는 방식은 사용자가 자유롭게 정할 수 있다.

배치는 그 모양이 고정적어야 할 때가 많습니다. 다시 말해 동일한 배치에 있는 문장들의 토큰(input_ids) 개수가 같아야 한다는 말이다.

<sub> 문서 분류 데이터 로더 선언 예시</sub>
```python
from torch.utils.data import DataLoader, RandomSampler
from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset corpus = NsmcCorpus()
train_dataset = ClassificationDataset(
    args=args, 
    corpus=corpus, 
    tokenizer=tokenizer, 
    mode="train",
)
train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size, 
    sampler=RandomSampler(train_dataset, replacement=False), 
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers,
)
```

### 태스크 정의하기

이 책에서는 모델 학습을 할 때 파이토치 라이트닝(pytorch lightning)이라는 라이브러리를 사용한다. 이것은 딥러닝 모델을 학습할 때 반복적인 내용을 대신 수행해주면서 사용자가 모델 구축에만 신경쓸 수 있도록 돕는다. 이 책에서는 파이토치 라이트닝이 제공하는 라이트닝(lightning) 모듈을 상속받아 태스크(task)를 정의하는데, 이 태스크에는 앞서 준비한 모델과 최적화 방법, 학습 과정 등이 정의되어 있다.

최적화(optimization)란 특정 조건에서 어떤 값이 최대나 최소가 되도록 하는 과정을 가리킨다. 앞서 설명했던 것처럼 딥러닝에서는 모델의 출력과 정답 사이의 차이를 작게 만드는 데 관심이 있는데, 이를 위해 옵티마이저(optimizer), 러닝 레이트 스케줄러(learning rate scheduler) 등을 정의해두는 것이다.

모델 학습은 배치단위로 이루어지며, 배치를 모델에 입력한 뒤 모델 출력을 정답과 비교해 차이를 계산한 후 그 차이를 최소화하는 방향으로 모델을 업데이트 하게 된다. 이 일련의 순환 과정을 스텝(step)이라 하며 task의 학습 과정에는 1회 스텝에서 벌어지는 일들을 정의해두게 된다.

### 모델 학습하기

트레이너(Trainer)는 파이토치 라이트닝에서 제공하는 객체로 실제 학습을 수행한다. 이는 GPU¹등 하드웨어 설정, 학습 기록 로깅, 모델 체크포인트 저장 등 복잡한 설정들을 알아서 처리해준다.

<sub>문서 분류 모델 학습 예시</sub>
```python
# 태스크와 트레이너를 정의한 다음
# 앞서 준비한 데이터 로더를 가지고 fit() 함수를 호출하면 학습을 시작

from ratsnlp.nlpbook.classification import ClassificationTask

task = ClassificationTask(model, args)
trainer = nlpbook.get_trainer(args)
trainer.fit(
    task,
    train_dataloader=train_dataloader,
)
```

<sub>¹ GPU(Graphic Processing Unit)는 그래픽 연산을 빠르게 처리하는 장치. 병렬 연산을 잘하는 덕분에 딥러닝 모델 학습에 널리 쓰이고 있다.</sub>
