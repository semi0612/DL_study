## 딥러닝 기반 자연어 처리 모델
- 기계는 어떠한 입력을 받아 처리를 수행하는 함수(모델)을 통해 출력(확률)을 얻어낸다는 점에 주목해야한다. 확률(Probability)란 말그대로 어떠한 사건이 일어날 가능성을 의미하는 수치이며 0~1사이의 값으로 나타나게 된다.<br>
즉, 모델은 어떠한 입력을 받아 해당 입력이 특정 범주일 확률을 반환하는 확률함수인 것인데. 자연어 처리모델은 자연어(사람말)을 입력으로 받아서 해당 입력이 특정 범주일 확률을 반환하는 확률함수인 것이다.

- 모델의 종류는 매우 다양한데, 가장 인기있는 종류는 (많은 은닉층을 사용하는)딥러닝이고, 이중에서도 BERT(Bidirectional Encoder Representations from Transformers)나 GPT(Generative Pre-Training)등의 딥러닝 기반 자연어 처리모델이 주목을 받고있다.<br>
딥러닝 기반 자연어 처리 모델의 출력 역시 확률이다. 따라서 자연어를 입력받아 해당 입력이 특정 범주일 확률을 출력하고, 이 출력된 확률값을 후처리를 통해 자연어 형태로 바꿔보여주게 되는데, 이 책에서 다룰 문서분류, 문장 쌍 분류, 개체명 인식, 질의응답, 문장 생성 등의 과제가 모두 위와 같다.

  - 문서분류 : 자연어(문서나 문장)를 입력받아 해당 입력이 어떠한 범주(긍정, 중립, 부정 등)에 속하는 지 그 확률값을 반환한다.
  - 개체명 인식 : 자연어(문서나 문장)을 입력받아 단어별로 기관명, 인명, 지명 등 어떤 개체명 범주에 속하는지 그 확률값을 반환한다.
  - 질의응답 : 자연어(질문+지문)를 입력받아 각 단어가 정답의 시작일 확률값과 끝일 확률값을 반환한다.
  - 문장생성 : GPT 계열 언어 모델이 널리 쓰이는데, 자연어(문장)를 입력받아 어휘 전체에 대한 확률값을 반환한다. 이 값은 입력된 문장 다음에 올 단어로 얼마나 적절한지를 나타내는 점수가 될 수 있다.
  - 자연어 추론 : 문장 2개를 입력받아 두 문장 사이의 관계가 참, 거짓, 중립 등 어떤 범주인지 그 확률값을 반환한다.

### 딥러닝 모델의 학습
모델을 만들기 위해서는 먼저 데이터를 준비해야한다. 학습데이터라고 부르는, 각 문장에 레이블을 달아놓은 자료가 있어야 한다.

그 다음으로는 모델이 데이터의 패턴(pattern)을 스스로 익히게끔 반복하여 학습을 해야한다. 그 과정에서 모델은 정답 레이블에 가까운 답을 낼 수 있도록 업데이트를 반복할 것이다.

### 트랜스퍼 러닝(transfer learning)
특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법을 가리킨다. 따라서 트랜스퍼 러닝을 적용하면 처음 아무것도 없는 것에서부터 학습을 하던 기존의 모델보다 학습 속도가 빨라지고 새로운 태스크를 더 잘 수행하는 경향을 보이기 때문에 최근 널리 쓰이고 있다 한다.

![image](https://user-images.githubusercontent.com/51469989/210687587-afcc47c0-3fa9-49e2-869b-0919347649ad.png)

위 이미지와 같이 태스크 2를 수행한느 모델을 만든다고 가정할 시 태스크 1을 수행해봤던 경험을 재활용하며 도움을 받는 것이다. 이때 태스크1은 업스트림(upstream) 태스크라고 부르고, 태스크2는 이와 대비된 개념으로 다운스트림(downstream) 태스크라고 부른다. 1은 주로 다음 단어 맞히기, 빈칸 채우기 등 대규모 말뭉치의 문맥을 이해하는 것이 과제이며, 2는 문서분류, 개체명 인식 등 자연어 처리의 구체적인 문제들을 다룬다.

업스트림 태스크를 학습하는 과정을 프리트레인(pretrain)이라 부르는데 이는 다운스트림 태스크를 본격적으로 수행하기에 앞서(pre) 학습한다는 의미에서 붙은것으로 추정

### 업스트림 태스크
트랜스퍼 러닝이 주목받게 된 것은 업스트림 태스크와 프리트레인 덕분이다. 자연어의 풍부한 문맥(context)을 모델에 내재화하고 이 모델을 다양한 다운스트림 태스크에 적용하여 성능을 대폭 끌어올리게 된 것이다.

업스트림 태스크의 대표과제로는 다음단어 맞추기와 빈칸 채우기가 있고, 다음 단어 맞히기로 업스트림 태스크를 수행한 모델은 **언어모델(language model)**, 빈칸 채우기로 업스트림 태스크를 수행한 모델을 **마스크 언어 모델(masked language model)** 이라고 한다.

일일히 정답(레이블)을 만들어줘야하는 지도학습에 반하여 다음 단어 맞히기, 빈칸 채우기와 같은 업스트림 태스크는 뉴스와 웹문서와 같은 글만 있으면 수작업 없이도 다량의 학습 데이터를 값싸게 만들어 낼 수 있다. 덕분에 업스트림 태스크를 수행한 모델은 성능이 기존보다 월등히 좋아질 수 있었던 것이다.

이처럼 데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 자기지도 학습¹(Self-supervised Learning)이라고 부른다

<sub> ¹자기지도 학습 : 데이터 자체에서 스스로 레이블을 생성하여 학습에 이용하는 방법을 의미. Label이 없는 다량의 데이터로부터 Label을 자동으로 생성하여 지도학습에 이용하는 비지도 학습기법</sub>

### 다운스트림 태스크
이 책에서 소개하는 다운스크림 태스크의 본질은 분류(classfication)이다. 다시 말해 자연어를 입력받아 해당 입력이 어떤 범주에 해당하는지 확률 형태로 반환받는 것이다. 문장 생성을 제외한 대부분의 과제에서는 프리트레인을 마친 마스크 언어 모델(BERT 계열)을 사용하게 된다.

다운스트림 태스크를 학습하는 방식은 크게 아래와 같은 3가지 방법이 있다.
1. 파인튜닝 : 다운스트림 태스크 데이터 전체를 사용한다. 다운스트림 데이터에 맞게 모델 전체를 업데이트해야함
2. 프롬프트 튜닝 : 다운스트림 태스크 데이터 전체를 사용한다. 다운스트림 태스크 데이터에 맞게 모델 일부만 업데이트
3. 인컨텍스트 러닝 : 다운스트림 태스크 데이터의 일부만 사용한다. 모델을 업데이트 하지는 않는다. 여기에는 3가지 방식을 더 확인해볼 수 있는데
  - 제로샷 러닝 : 다운스트림 태스크 데이터를 전혀 사용하지 않는다. 모델이 바로 다운스트림 태스크를 수행한다
  - 원샷 러닝 : 다운스트림 태스크 데이터를 몇 건만 사용한다. 모델은 몇 건의 데이터가 어떻게 수행되는지 참고한 뒤 다운스트림 태스크를 수행한다.

이 책에서 설명하는 다운스트림 태스크의 학습 방식은 모두 파인튜닝(fine-tuning)이며, 이는 프리트레인을 마친 모델을 다운 태스크에 맞게 업데이트하는 기법이다.
