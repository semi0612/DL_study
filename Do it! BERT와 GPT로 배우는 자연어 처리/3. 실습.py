# -*- coding: utf-8 -*-
"""실습______1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_LSez-Rt7UOkq-1onP3F8jCU3H_awVY
"""

# 패키지 설치
!pip install ratsnlp

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

"""### 말뭉치 다운로드 및 전처리

오픈소스 파이썬 패키지 코포라(Korpora)를 활용해 BPE 수행 대상 말뭉치를 내려받고 전처리한다. 실습용 말뭉치는 박은정 님이 공개하신 Naver Sentiment Movie Corpus(NSMC)를 사용할 것.

"""

from Korpora import Korpora

# 데이터를 내려받아 `nsmc`라는 변수로 읽어들이기
nsmc = Korpora.load("nsmc", force_download=True)

import os

# 읽어들인 것 저장
# NSMC에 포함된 영화 리뷰들을 순수 텍스트 형태로 저장해두게 된다.
def write_lines(path, lines) :
  with open(path, 'w', encoding='utf-8') as f :
    for line in lines :
      f.write(f'{line}\n')

write_lines("./train.txt", nsmc.train.get_all_texts())
write_lines("./test.txt", nsmc.test.get_all_texts())

!head train.txt

!head test.txt

"""### GPT 토크나이저 구축

GPT 계열 모델이 사용하는 토크나이저는 Byte-level Byte Pair Encoding(BBPE). 우선 어휘집합 구축 결과를 저장해둘 디렉토리를 자신의 구글 드라이브 계정 내 `내 드라이브/nlpbook/bbpe`로 만들어 두고 저장.


"""

import os

os.makedirs("/content/drive/MyDrive/중앙수업/nlpbook/bbpe", exist_ok=True)

# 아래 수행시 nsmc 데이터를 가지고 BBPE 어휘집합을 구축한다
from tokenizers import ByteLevelBPETokenizer

bytebpe_tokenizer = ByteLevelBPETokenizer()
bytebpe_tokenizer.train(
    
    # 학습 말뭉치를 리스트 형태로 넣기
    files=["/content/train.txt", "/content/test.txt"],

    # 어휘집합 크기 조절
    vocab_size = 10000,

    # 특수 토큰 추가
    special_tokens=["[PAD]"]
)
bytebpe_tokenizer.save_model("/content/drive/MyDrive/중앙수업/nlpbook/bbpe")

# >>
# ['/content/drive/MyDrive/중앙수업/nlpbook/bbpe/vocab.json',
#  '/content/drive/MyDrive/중앙수업/nlpbook/bbpe/merges.txt']

# 바이트 레벨 BPE의 어휘 집합
!cat /content/drive/MyDrive/중앙수업/nlpbook/bbpe/vocab.json

# 바이그램 쌍의 병합 우선순위
!cat /content/drive/MyDrive/중앙수업/nlpbook/bbpe/merges.txt

"""### BERT 토크나이저 구축

BERT는 워드피스(wordpiece) 토크나이저를 사용한다. 우선 어휘집합 구축 결과를 저장해둘 디렉토리를 자신의 구글 드라이브 계정 내 `내 드라이브/nlpbook/bbpe`로 만들어 저장.
"""

import os

os.makedirs("/content/drive/MyDrive/중앙수업/nlpbook/wordpiece", exist_ok=True)

# 아래 수행시 BERT 모델이 사용하는 워드피스 어휘집합을 구축가능
from tokenizers import BertWordPieceTokenizer

wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)
wordpiece_tokenizer.train(
    files=["/content/train.txt", "/content/test.txt"],
    vocab_size=10000,
)

wordpiece_tokenizer.save_model("/content/drive/MyDrive/중앙수업/nlpbook/wordpiece")

# 워드피스 어휘집합인 vocab 생성된 것 확인
!head /content/drive/MyDrive/중앙수업/nlpbook/wordpiece/vocab.txt

"""### GPT 입력값 만들기

GPT 모델 입력값을 만들려면 Byte-level Byte Pair Encoding 어휘집합 구축 결과(`vocab.json`, `merges.txt`)가 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/wordpiece`)에 있어야한다. 다음을 수행해 이미 만들어 놓은 BBPE 어휘집합을 포함한 GPT 토크나이저를 `tokenizer_gpt`라는 변수로 쓰겠다 선언.
"""

from transformers import GPT2Tokenizer

# 해당 위치에 위 실습에서 만들어놓은 바이트 기준 BPE 어휘집합(vocab.json)과
# 바이그램 쌍의 병합 우선순위(merge.txt)가 있어야 실행 가능
tokenizer_gpt = GPT2Tokenizer.from_pretrained("/content/drive/MyDrive/중앙수업/nlpbook/bbpe")
tokenizer_gpt.pad_token = "[PAD]"

# 예시 문장(sentences)를 바이트 수준 BPE 토크나이저로 토큰화한 모습
sentences = [
    "아 더빙.. 진짜 짜증나네요 목소리",
    "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나",
    "별루 였다..",
]

tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]
tokenized_sentences

# 모델에 실제 입력값은 아래와 같이 만들게 된다
batch_inputs = tokenizer_gpt(
    sentences,

    # 문장의 최대 길이에 맞춰 패딩하겠다.
    padding="max_length",

    # 문장의 토큰 기준 최대 길이
    max_length=12,

    # 문장 잘림 허용 옵션
    truncation=True,
)

batch_inputs.keys()

"""위 코드 실행시 두 가지의 입력값(input_ids)와 일반 토큰이 자리한 곳(1)과 패딩토큰이 자리한 곳(0)을 구분해 알려주는 mask(attention_mask)가 만들어진다."""

# input_ids는 토큰화된 결과를 가지고 각 토큰들을 인덱스로 바꾼것이다.
# 어휘집합(vocab.json)을 확인해보면 각 어휘가 순서대로 나열된 모습을 확인할 수 있는데, 그 순서가 인덱스가 된다
# 이와같이 각 토큰을 인덱스로 변환하는 과정은 '인덱싱(indexing)'이라 불린다.
batch_inputs['input_ids']

# 값을 자세히보면 예시로 넣었던 세 문장의 길이가 모두 12로 맞춰진 것을 볼 수 있다
# 이는 max_length = 12 를 해줬기 때문
batch_inputs['attention_mask']

batch_inputs

"""### BERT 입력값 만들기
이번엔 BERT 모델의 입력값을 만들어보자. 바로 아래 코드를 실행하면 BERT 모델이 사용하는 토크나이저를 초기화할 수 있는데. 이때 당연히 자신의 구글 드라이브 경로에 BERT용 워드피스 어휘 집합(vocab.txt)이 있어야 한다.
"""

from transformers import BertTokenizer
tokenizer_bert = BertTokenizer.from_pretrained("/content/drive/MyDrive/중앙수업/nlpbook/wordpiece", do_lower_case=False)

# 예시 문장(sentences)를 바이트 수준 BPE 토크나이저로 토큰화한 모습
sentences = [
    "아 더빙.. 진짜 짜증나네요 목소리",
    "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나",
    "별루 였다..",
]

tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]
tokenized_sentences

batch_inputs = tokenizer_bert(
    sentences,

    # 문장의 최대 길이에 맞춰 패딩하겠다.
    padding="max_length",

    # 문장의 토큰 기준 최대 길이
    max_length=12,

    # 문장 잘림 허용 옵션
    truncation=True,
)

batch_inputs.keys()

batch_inputs['input_ids']

batch_inputs['token_type_ids']

batch_inputs

"""`token_type_ids`라는 입력값은 세그먼트(segment)에 해당하는 것으로 현재 실습에서는 문장을 하나씩 넣었기 때문에 모두 0으로 처리된 것이라 함.

token_type_ids는 전체 입력(input_ids)의 어느부분이 첫 번째 문장이고, 어느것이 두 번째 문장인지를 모델에 알려주는 값이라 한다.

일반적으로, 토큰화 완료된 입력에 token_type_ids가 있는지 여부에 대해 걱정할 필요는.. 없다고함. 토크나이저와 모델 모두에 동일한 체크포인트(checkpoint)를 사용하는 한, 토크나이저는 모델에 무엇을 제공해야 하는지 알고 있기 때문에

<sub>예시</sub> <br>
```python
# 데이터셋 전처리를 위해 우선적으로 텍스트를 모델이 이해할 수 있는 숫자로 변환하는 과정 = 전처리
# 이는 토크나이저가 담당
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])

# 하지만 두 개의 시퀸스를 모델에 바로 전달(각각의 문장을 모델에 별도로 매개변수로 전달)
# 하게 된다면 두 문장이 의역인지 아닌지에 대한 예측을 할 수 없다
# 따라서 두 시퀸스를 쌍(pair)으로 처리해야한다.
# 아래와 같이 두 문장을 집어넣어주면
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```
```python
{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```
위와 같은 결과를 얻어볼 수 있는데, 이때 token_type_ids에서 문장 1에 해당하는 부분은 0으로 문장2에 해당하는 부분은 1로 출력되는 모습을 확인할 수 있다. 이를 통해 전체 입력(input_ids)의 어느 부분이 첫 번째 문장이고 어느 것이 두 번째 문장인지 모델에 알려주게 되는 것이다. 
"""

